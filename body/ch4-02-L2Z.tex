Consider the case where the penalty function $ f_j $ for each group $ j $ is linearly decreasing before its desired allocation amount $ m_j $ and 0 afterwards. Namely, we have:
\begin{align*}
    f_j(x) = \begin{cases}
                 f_j^L(x) = -\beta_j x + \beta_j m_j, x\in [0, m_j], \\
                 f_j^R(x) = 0, x\in [m_j, \infty),
             \end{cases}
\end{align*}


The offline optimization problem can then be expressed as follows:
\begin{subequations}\label{equation-linear2zero-soft-quota-primal}
    \begin{align}
        \max_{0 \leq x_t \leq r_t}\quad
         & \sum_{t \in [T]} v_tx_t -
        \sum_{j \in [K]}
        \beta_j
        \Bigl[\,m_j - \sum_{t=1}^{T} x_t \cdot\mathbf 1_{\{j_t = j\}}\Bigr]^{+} \\[6pt]
        \text{s.t.}\quad
         & \sum_{t=1}^{T} x_t \leq 1
    \end{align}
\end{subequations}

\paragraph{Remark} We may take an alternative view of the optimization problem by considering it as a subsidy of $ \beta_j $ for the first $ m_j $ units of allocation to group $ j $. In this case, the optimization problem can be reformulated as follows,
\begin{subequations}\label{equation-linear2zero-subsidy}
    \begin{align}
        \max_{0 \leq x_t \leq r_t}\quad
         & \sum_{t \in [T]} (v_t + \beta_{j_t}) x_t -
        \sum_{j \in [K]}
        \beta_j
        \Bigl[\sum_{t \in [T]} x_t \cdot \mathbf{1}_{\{j_t = j\}} - m_j\Bigr]^{+} \\[6pt]
        \text{s.t.}\quad
         & \sum_{t=1}^{T} x_t \leq 1
    \end{align}
\end{subequations}.


\subsection{Sufficient Conditions}

Given the penalty function, we introduce auxiliary variables $ y_j \geq 0 $ for each group $ j\in [K] $ to represent the shortfall from its desired allocation $ m_j $. Thus, we can reformulate the offline optimization problem \eqref{equation-linear2zero-soft-quota-primal} as follows:
\begin{subequations}\label{equation-linear2zero-soft-quota-alternative-primal}
    \begin{align}
        \max_{x_t \geq 0, y_j \geq 0} & \sum_{i\in[K]}\left(\sum_{t\in[T]}v_t \cdot x_t \cdot \mathbf{1}_{\{j_t = j\}} - \beta_j \cdot y_j\right) \\
        \text{s.t. }                  & y_j \geq m_j - \sum_{t\in[T]}x_t \cdot \mathbf{1}_{\{j_t = j\}}\label{linear2zero-y-constraint}           \\
                                      & \sum_{t\in[T]} x_t \leq B \label{linear2zero-budget-constraint}
    \end{align}
\end{subequations}

Introducing dual variables $ \eta_j \geq 0 $ for constraints \eqref{linear2zero-y-constraint} and $ \lambda \geq 0 $ for the capacity constraint \eqref{linear2zero-budget-constraint}, we can derive its corresponding dual problem:
\begin{subequations}
    \begin{align}
        \min~        & (B-M)\cdot\lambda + \sum_{j\in[K]}m_j \cdot \eta_j              \\
        \text{s.t. } & \eta_{j_t} \geq v_t, \quad \forall t\in [T]                     \\
                     & \lambda \geq \eta_j \geq \lambda-\beta_j, \quad \forall j\in[K]
    \end{align}
\end{subequations}

Based on the online primal dual framework, now we present an algorithm that updates the dual variables at each time step and based on the updates of the dual variables, makes the allocation decisions of the primal variables. The updates of the dual variables directly follows from the \OPT in the offline setting. From solving the offline optimization problem \eqref{equation-linear2zero-soft-quota-primal}, we observe that
\begin{align*}
    \OPT = (B-M)\cdot v^{\max} + \sum_{j\in[K]} m_j\cdot \max \{v^{\max}_j, v^{\max}-\beta_j\},
\end{align*}
where $ v^{\max}_j $ is the maximum value among all arrivals from group $ j $, and $ v^{\max} = \max_{j\in[K]} v^{\max}_j $.
Intuitively,we need to select $ \lambda $ in a way that it updates according to $ v^{\max} $, and  $\eta_{j_t} $ according to $ \max \{v^{\max}_{j_t}, v^{\max}-\beta_{j_t}\} $ at each time step $ t $. Based on this intuition, we present the algorithm Linear-To-Zero in Algorithm \ref{alg_LinearToZero}.
\begin{algorithm}
    \caption{Linear-To-Zero}
    \label{alg_LinearToZero}
    \begin{algorithmic}[1]

        \Require $B, \theta$; $\{m_j, \beta_j, A_j\}_{j\in[K]}$ and $\alpha$
        \State Initialize $\eta_j^0 = 1$, $z_j^0 = 0$ for all $j \in [K]$, and $\lambda^0 = 1$

        \While{agent $t$ arrives}
        \State Obtain the agent's value $ v_t $ and class $j_t$

        \If{$z_{j_t}^{t-1} = 0$}
        \State $y_{j_t} = A_{j_t}$
        \Else
        \State $y_{j_t} = 0$
        \EndIf

        \State $x^G_t = \argmax_a \left\{ \frac{v_t + \beta^t_{j_t}}{\lambda^{t-1} + \beta^t_{j_t}} \cdot a - \int_0^{a} \exp\left(\frac{\alpha}{B-M} u\right)du \right\}$

        \State $x^{j_t}_t = \argmax_a \left\{ \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_{j_t} + \beta^t_{j_t}} \cdot a - \int_0^{a} \exp\left(\frac{\alpha}{m_{j_t}} u\right)du \right\}$

        \For{each $i \in [K]\setminus\{j_t\}$}
        \State $x^i_t = \argmax_a \left\{ \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_{j_t} + \beta^t_{j_t} + \beta_i} \cdot a - \int_0^{a} \exp\left(\frac{\alpha}{m_i} u\right)du \right\}$
        \EndFor

        \State $x_t = y_{j_t} + x^G_t + \sum_{j\in[K]} x^j_t$ \Comment{Total allocation}

        \State Update $z_{j_t}^t = z_{j_t}^{t-1} + x_t$

        \State Update:
        \[
            \eta^t_{j_t} = \max\{\eta^{t-1}_{j_t}, v_t\}, \quad
            \lambda^t = \max\{\lambda^{t-1}, v_t\}, \quad
            \eta^t_i = \max\{\eta^{t-1}_i, v_t - \beta_i\} \ \forall i \neq j_t
        \]

        \EndWhile

    \end{algorithmic}
\end{algorithm}

In Algorithm \ref{alg_LinearToZero}, $ z_j^t $ denotes the utilization level of class $ j $ by time $ t $, and $ \beta_j^t  = \beta_j \cdot \mathbf{1}_{\{z_j^{t-1} < m_j\}} $ indicates whether the subsidy for class $ j $ is still active at time $ t $.

Depending on the value $ v_t $, there are four sources of allocation for agent $ t $:
\begin{itemize}
    \item {\bf Initial Reservation $ y_{j_t} $:} if class $ j_t $ has not yet reached its reserved allocation amount $ A_{j_t} $, allocate $ y_{j_t} = A_{j_t} $ to agent $ t $.
    \item {\bf Global Allocation $ x^G_t $:} allocate an additional amount $ x^G_t $ based on the global dual variable $ \lambda^{t-1} $ which is shared across all classes. $ x^G_t $ is only non-zero when the agent's value $ v_t $ exceeds the current global dual variable $ \lambda^{t-1} $, meaning $ v_t $ is the highest value seen so far.
    \item {\bf Class-specific Allocation $ x^{j_t}_t $:} allocate an additional amount $ x^{j_t}_t $ based on the class-specific dual variable $ \eta^{t-1}_{j_t} $. This allocation is only non-zero when the agent's value $ v_t $ exceeds the current class-specific dual variable $ \eta^{t-1}_{j_t} $.
    \item {\bf Cross-class Allocation $ x^i_t $:} For every other class $ i \in [K]\setminus\{j_t\} $, allocate an additional amount $ x^i_t $ based on the adjusted class-specific dual variable $ \eta^{t-1}_i + \beta_i $. Cross-class allocation is only non-zero when the agent's value $ v_t $ is high enough to compensate the under allocation penalty for class $ i $.
\end{itemize}


In addition to the problem parameters, Algorithm \ref{alg_LinearToZero} additionally takes as input $ \{A_j\}_{\forall j\in [K]} $ as reservations for each class and $ \alpha $. In the following theorem, we provide sufficient conditions on designing $ \{A_j\}_{\forall j\in [K]} $ and $ \alpha $ such that Algorithm \ref{alg_LinearToZero} achieves the competitive ratio of $ \alpha $.

\begin{theorem}\label{theorem-linear2zero-soft-gfq-upper-bound}
    For any $ \alpha \geq \alpha^* $, Algorithm \ref{alg_LinearToZero} is $ \alpha $-competitive and produces a feasible solution, where $ \alpha^* $ is the optimal value of the following optimization problem:
    \begin{align*}
        \min_{\{A_j\}_{\forall j \in[K]}} & \alpha                                                                                                                                                                                                                                                                                            \\
        \text{s.t. }                      & \sum_{j\in[K]}A_j \geq \frac{B}{\alpha}                                                                                                                                                                                                                                                           \\
                                          & \begin{cases}A_{j} + \frac{B-M+m_{j}}{\alpha}\ln\left(\frac{v^*_{j,j}+\beta_{j}}{1+\beta_{j}}\right)=m_{j}                           \\
                                                A_{i} + \frac{m_{i}}{\alpha}\ln\left(\frac{v^*_{i,j}+\beta_{i}}{1+\beta_{i}}\right)=m_{i}, \quad \forall i \in[K]/\{j\} \\
                                                M+\frac{B-M+m_{j}}{\alpha}\ln\left(\frac{\theta}{v^*_{j,j}}\right)+\sum_{i \in[K]/\{j\}}\frac{m_{i}}{\alpha}\ln\left(\frac{\theta}{v^*_{i,j}}\right) \leq B
                                            \end{cases} \\
                                          & \hspace{5.5cm} ,\forall j \in [K]
    \end{align*}
\end{theorem}

\begin{proof}
    The proof to Theorem \ref{theorem-linear2zero-soft-gfq-upper-bound} follows the primal-dual analysis framework. First, we need to show that the dual variables are feasible at each step. second, we need to show that at each time step $ t $, the incremental inequality holds for the change in the primal and dual objectives. Lastly, we need to show that the obtained primal solution is feasible by showing that the total allocation does not exceed the capacity $ B $, regardless of the arrival sequence.

    \paragraph{Dual Feasibility.} Based on the update rules in Algorithm \ref{alg_LinearToZero}, it is straightforward to verify that the dual variables $ \eta_j^t $ and $ \lambda^t $ satisfy the dual feasibility conditions at each time step $ t $.

        {\color{red} TODO: Further consolidate the proofs by showing that it holds for any source of allocation, and then apply to different cases.}
    \paragraph{Incremental Inequality.} Let $ \Delta P_t= P_t - P_{t-1} $ and $ \Delta D_t = D_t - D_{t-1} $ denote the changes in the primal and dual objectives at time step $ t $, respectively. First, note that the primal change at each time step $ t $ can be expressed as the sum of the value obtained and the subsidy provided:
    \begin{align*}
        \Delta P_t = v_t\cdot x_t + \beta^t_{j_t} \cdot x_t = (v_t + \beta^t_{j_t}) \cdot x_t.
    \end{align*}
    Based on the arriving agent's value $ v_t $, we consider the following cases for the change in the dual objective:
    \paragraph{Case 1.} If $ v_t < \eta^{t-1}_{j_t} $ and $ v_t < \lambda^{t-1} $, then no dual variable is updated at time $ t $, and thus $ \Delta D_t = 0 $. It is also easy to see that the primal allocation $ x_t = 0 $ in this case, leading to $ \Delta P_t = 0 $. Therefore, we have $ \Delta P_t = 0 \geq \Delta D_t/\alpha $.

    \paragraph{Case 2.} If $ v_t \geq \eta^{t-1}_{j_t} $ but $ v_t < \lambda^{t-1} $, the only source of allocation is the class-specific allocation $ x^{j_t}_t $. The dual change can be expressed as:
    \begin{align*}
        \Delta D_t = m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t})
    \end{align*}
    By the design of $ x^{j_t}_t $ in Algorithm \ref{alg_LinearToZero} being the maximizer of the stated expression, we have the following from the first-order condition:
    \begin{align*}
        \exp(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t) = \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_{j_t} + \beta^t_{j_t}}.
    \end{align*}
    Rearranging the terms gives
    \begin{align*}
        v_t = (\eta^{t-1}_{j_t} + \beta^t_{j_t})\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) - \beta^t_{j_t}.
    \end{align*}
    Given the update rule $ \eta^t_{j_t} = \max\{\eta^{t-1}_{j_t}, v_t\} = v_t $ in this case, we have
    \begin{align*}
        \Delta D_t & = m_{j_t}\cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t})                                                                                                     \\
                   & = m_{j_t} \cdot \left((\eta^{t-1}_{j_t} + \beta^t_{j_t})\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) - \beta^t_{j_t} - \eta^{t-1}_{j_t}\right) \\
                   & = m_{j_t} \cdot (\eta^{t-1}_{j_t} + \beta^t_{j_t})\left(\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) - 1\right),
    \end{align*}
    and similarly,
    \begin{align*}
        \Delta P_t & = (v_t + \beta^t_{j_t}) x^{j_t}_t                                                                 \\
                   & = (\eta^{t-1}_{j_t} + \beta^t_{j_t})\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) x^{j_t}_t.
    \end{align*}
    To show $ \Delta P_t \geq \Delta D_t/\alpha $, we need to verify the following inequality:
    \begin{align*}
        (\eta^{t-1}_{j_t} + \beta^t_{j_t})\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) x^{j_t}_t \geq \frac{1}{\alpha} m_{j_t} \cdot (\eta^{t-1}_{j_t} + \beta^t_{j_t})\left(\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) - 1\right).
    \end{align*}
    Given that $ \eta^{t-1}_{j_t} + \beta^t_{j_t} > 0 $, we can divide both sides by this positive factor. Furthermore, let $ y := \frac{\alpha}{m_{j_t}} x^{j_t}_t \geq 0 $, then $ x^{j_t}_t = (m_{j_t}/\alpha) y $, and the inequality to prove becomes
    \begin{align*}
        e^{y} \cdot \frac{m_{j_t}}{\alpha} y & \geq \frac{m_{j_t}}{\alpha} \bigl(e^{y} - 1\bigr) \\
        e^{y} y                              & \geq e^{y} - 1
    \end{align*}
    Define $ g(y) := e^{y} y - e^{y} + 1 = e^{y}(y-1) + 1 $. Given that $ g(0) = 0, g'(y) = e^{y}(y-1) + e^{y} = e^{y} y \geq 0 $ for all $ y \geq 0 $, $ g $ is non-decreasing on $ [0, \infty) $ and $ g(y) \geq g(0) = 0 $ for all $ y \geq 0 $, which is equivalent to $ e^{y} y \geq e^{y} - 1 $ for all $ y \geq 0 $. Therefore, we obtain $ \Delta P_t \geq \Delta D_t/\alpha $.

    \paragraph{Case 3.} If $ v_t \geq \eta^{t-1}_{j_t} $ and $ v_t \geq \lambda^{t-1} $, but $ v_t - \beta_i \leq \eta^{t-1}_i $ for all $ i \ne j_t $, the only sources of allocation are the global allocation $ x^G_t $ and the class-specific allocation $ x^{j_t}_t $. The dual change can be expressed as:
    \begin{align*}
        \Delta D_t = (B-M) \cdot (\lambda^t - \lambda^{t-1}) + m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t})
    \end{align*}
    By the design of $ x^G_t $ and $ x^{j_t}_t $ in Algorithm \ref{alg_LinearToZero} being the maximizers of the stated expressions, we have the following from the first-order conditions:
    \begin{align*}
        \exp\left(\tfrac{\alpha}{B-M} x^G_t\right)         & = \frac{v_t + \beta^t_{j_t}}{\lambda^{t-1} + \beta^t_{j_t}},    \\
        \exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) & = \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_{j_t} + \beta^t_{j_t}}.
    \end{align*}
    Applying the same change-of-variable and rearrangement arguments as in Case 2 to both $ x^G_t $ and $ x^{j_t}_t $, we can show that
    \begin{align*}
        \Delta P_t^G     & \geq \frac{1}{\alpha} (B-M) \cdot (\lambda^t - \lambda^{t-1}),         \\
        \Delta P_t^{j_t} & \geq \frac{1}{\alpha} m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t}),
    \end{align*}
    where $ \Delta P_t^G = (v_t + \beta^t_{j_t}) x^G_t $ and $ \Delta P_t^{j_t} = (v_t + \beta^t_{j_t}) x^{j_t}_t $. Summing these two inequalities gives
    \begin{align*}
        \Delta P_t = \Delta P_t^G + \Delta P_t^{j_t} \geq \frac{1}{\alpha} \left((B-M) \cdot (\lambda^t - \lambda^{t-1}) + m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t})\right) = \frac{1}{\alpha} \Delta D_t.
    \end{align*}

    \paragraph{Case 4.} If $ v_t \geq \eta^{t-1}_{j_t} $ and $ v_t \geq \lambda^{t-1} $, and there exists some $ i \ne j_t $ with $ v_t - \beta_i > \eta^{t-1}_i $, the sources of allocation include the global allocation $ x^G_t $, the class-specific allocation $ x^{j_t}_t $, and the cross-class allocations $ x^i_t $ for all such $ i $. Let $ I_t \subseteq [K] \setminus \{j_t\} $ denote the set of classes whose dual variables increase at time $ t $. The dual change can be expressed as:
    \begin{align*}
        \Delta D_t = (B-M) \cdot (\lambda^t - \lambda^{t-1}) + m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t}) + \sum_{i \in I_t} m_i \cdot (\eta^t_i - \eta^{t-1}_i)
    \end{align*}
    By the design of $ x^G_t $, $ x^{j_t}_t $, and $ x^i_t $ in Algorithm \ref{alg_LinearToZero} being the maximizers of the stated expressions, we have the following from the first-order conditions:
    \begin{align*}
        \exp\left(\tfrac{\alpha}{B-M} x^G_t\right)         & = \frac{v_t + \beta^t_{j_t}}{\lambda^{t-1} + \beta^t_{j_t}},                                   \\
        \exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) & = \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_{j_t} + \beta^t_{j_t}},                                \\
        \exp\left(\tfrac{\alpha}{m_i} x^i_t\right)         & = \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_i + \beta^t_{j_t} + \beta_i}, \quad \forall i \in I_t.
    \end{align*}
    similarly, we can show that
    \begin{align*}
        \Delta P_t^G     & \geq \frac{1}{\alpha} (B-M) \cdot (\lambda^t - \lambda^{t-1}),                      \\
        \Delta P_t^{j_t} & \geq \frac{1}{\alpha} m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t}),              \\
        \Delta P_t^{i}   & \geq \frac{1}{\alpha} m_i \cdot (\eta^t_i - \eta^{t-1}_i), \quad \forall i \in I_t,
    \end{align*}
    where $ \Delta P_t^G = (v_t + \beta^t_{j_t}) x^G_t $, $ \Delta P_t^{j_t} = (v_t + \beta^t_{j_t}) x^{j_t}_t $, and $ \Delta P_t^{i} = (v_t + \beta^t_{j_t}) x^{i}_t $ for all $ i \in I_t $. Summing over all active components gives
    \begin{align*}
        \Delta P_t & = \Delta P_t^G + \Delta P_t^{j_t} + \sum_{i\in I_t}\Delta P_t^{i} \\
                   & \ge \frac{1}{\alpha}\Big((B-M)(\lambda^t-\lambda^{t-1})
        + m_{j_t}(\eta^t_{j_t}-\eta^{t-1}_{j_t})
        + \sum_{i\in I_t} m_i(\eta^t_i-\eta^{t-1}_i)\Big)                              \\
                   & = \frac{1}{\alpha}\Delta D_t.
    \end{align*}

    \paragraph{Primal Feasibility.}
    We prove that the primal solution production by Algorithm \ref{alg_LinearToZero} is feasible, i.e., $ \sum_{t=1}^T x_t \le B $, for any arrival sequence. We use induction on time $ t $ and the idea of \emph{future feasibility}. An allocation prefix is future-feasible if, even under the worst-case future sequence of arrivals, the total allocation remain within the budget $ B $.

    \textbf{Intuition.} To formalize this, we define a max-potential allocation function $ \Gamma_i^t $ that represents the total amount of resources allocated up to time $ t $, plus the maximum potential future allocation allowed by the current state of the dual variable. If we can show that $ \Gamma_i^t \leq B $ for all $ t $ and all $ i \in [K] $, then it implies that the total allocation $ Z^t = \sum_{s=1}^t x_s \leq B $ for all $ t $, establishing primal feasibility.

    \textbf{Max-potential Allocation Function.} We define the max-potential allocation function for each class $ i \in [K] $ at time $ t $ as follows:
    % \[
    %     \Gamma_i^t = Z^t + \sum_{j\in[K]} \frac{m_j}{\alpha}\ln\Biggl(\frac{\theta+\beta_{j}\cdot \mathbf{1}\{z_j^{t}<m_j\}}{\eta^{t}_j+\beta_{j}\cdot \mathbf{1}\{z_j^{t}<m_j\}}\Biggr) + \frac{B-M}{\alpha}\ln\Biggl(\frac{\theta+\beta_{i}\cdot \mathbf{1}\{z_i^{t}<m_i\}}{\lambda^{t}+\beta_{i}\cdot \mathbf{1}\{z_i^{t}<m_i\}}\Biggr).
    % \]

    \[
        \Gamma_i^t(\eta_j^t, \lambda^t) = Z_t + \sum_{j\in[K]} \Delta(\eta_j^t) + \bar{\Delta}(\lambda^t),
    \]
    where $ \Delta(\eta_j^t) $ and $ \bar{\Delta}(\lambda^t) $ are defined as follows:
    \[
        \Delta(\eta_j^t) = \begin{cases}
            \frac{m_j}{\alpha} \ln\Bigl(\frac{v^*_{j,i} + \beta_j}{\eta_j^t + \beta_j}\Bigr) + \frac{m_j}{\alpha} \ln\Bigl(\frac{\theta }{v^*_{j,i} }\Bigr) & \text{if } \eta_j^t < v^*_{j,i},    \\
            \frac{m_j}{\alpha} \ln\Bigl(\frac{\theta }{\eta_j^t}\Bigr),                                                                                     & \text{if } \eta_j^t \geq v^*_{j,i},
        \end{cases}
    \]
    \[
        \bar{\Delta}(\lambda^t) = \begin{cases}
            \frac{B-M}{\alpha} \ln\Bigl(\frac{v^*_{i,i} + \beta_i}{\lambda^t + \beta_i}\Bigr) + \frac{B-M}{\alpha} \ln\Bigl(\frac{\theta }{v^*_{i,i} }\Bigr) & \text{if } \lambda^t < v^*_{i,i},    \\
            \frac{B-M}{\alpha} \ln\Bigl(\frac{\theta }{\lambda^t}\Bigr),                                                                                     & \text{if } \lambda^t \geq v^*_{i,i}.
        \end{cases}
    \]
    Intuitively, $ \Delta(\eta_j^t) $ captures the maximum potential future allocation for each class $ j $ based on its corresponding dual variable $ \eta_j^t $, while $ \bar{\Delta}(\lambda^t) $ captures the maximum potential future allocation to class $ i $ based on the global dual variable $ \lambda^t $. Altogether, $ \Gamma_i^t(\eta_j^t, \lambda^t) $ represents the maximum potential allocation, considering when the global threshold is only helping class $ i $. For notational simplicity, we will denote $ \Gamma_i^t = \Gamma_i^t(\eta_j^t, \lambda^t) $ in the following proof.

    We will show by induction that $ \Gamma_i^t \leq B $ for all $ t $ and all $ i \in [K] $.

    \textbf{Base Case $ (t = 0) $.} At $ t = 0 $, we have $ Z^0 = Z_j^0 = 0 $ for all $ j\in [K]$, and the algorithm initializes $ \eta_j^0 = 1 $ for all $ j $ and $ \lambda^0 = 1 $. Thus, for any fixed $ i \in [K] $,
    \[
        \Gamma_i^0 = \sum_{j\in[K]} \frac{m_j}{\alpha}\ln\Bigl(\tfrac{\theta+\beta_j}{1+\beta_j}\Bigr)
        + \frac{B-M}{\alpha}\ln\Bigl(\tfrac{\theta+\beta_i}{1+\beta_i}\Bigr).
    \]
    From the design of Algorithm \ref{alg_LinearToZero}, there is a reservation $ A_j $ for each class $ j\in [K] $, therefore, to show that the prefix at time $ t = 0 $ is future-feasible, we show that $ \Gamma_i^0 + \sum_{j\in[K]} A_j \leq B $ for all $ i \in [K] $.

    By the constraints in Theorem \ref{theorem-linear2zero-soft-gfq-upper-bound}, for each "reference" class $ \ell \in [K] $ there exist thresholds $ \{v^*_{k,\ell}\}_{k\in[K]} $ such that
    \begin{align*}
        A_{\ell} + \frac{B-M+m_{\ell}}{\alpha}\ln\Bigl(\tfrac{v^*_{\ell,\ell}+\beta_{\ell}}{1+\beta_{\ell}}\Bigr)                                                                   & = m_{\ell},                                      \\
        A_{k} + \frac{m_{k}}{\alpha}\ln\Bigl(\tfrac{v^*_{k,\ell}+\beta_{k}}{1+\beta_{k}}\Bigr)                                                                                      & = m_{k}, \quad \forall k\in[K]\setminus\{\ell\}, \\
        M+\frac{B-M+m_{\ell}}{\alpha}\ln\Bigl(\tfrac{\theta}{v^*_{\ell,\ell}}\Bigr)+\sum_{k \in[K]\setminus\{\ell\}}\frac{m_{k}}{\alpha}\ln\Bigl(\tfrac{\theta}{v^*_{k,\ell}}\Bigr) & \leq B.
    \end{align*}

    Fixing any $ i \in [K] $ and taking $ \ell = i $, we rearrange the first two constraints and sum over $ k $ to obtain
    \[
        M - \sum_{j\in[K]} A_j = \frac{B-M+m_i}{\alpha}\ln\Bigl(\tfrac{v^*_{i,i}+\beta_i}{1+\beta_i}\Bigr) + \sum_{j\ne i} \frac{m_j}{\alpha}\ln\Bigl(\tfrac{v^*_{j,i}+\beta_j}{1+\beta_j}\Bigr).
    \]
    Decomposing the log terms as
    \[
        \ln\Bigl(\tfrac{\theta+\beta_j}{1+\beta_j}\Bigr) = \ln\Bigl(\tfrac{\theta}{v^*_{j,i}}\Bigr) + \ln\Bigl(\tfrac{v^*_{j,i}+\beta_j}{1+\beta_j}\Bigr) + \ln\Bigl(\tfrac{\theta+\beta_j}{\theta}\cdot\tfrac{v^*_{j,i}}{v^*_{j,i}+\beta_j}\Bigr),
    \] we note that $ \frac{\theta+\beta_j}{\theta}\cdot\frac{v^*_{j,i}}{v^*_{j,i}+\beta_j} \leq 1 $, so each such logarithm is nonpositive. Substituting these decompositions into $ \Gamma_i^0 $ and combining with the last feasibility inequality in Theorem \ref{theorem-linear2zero-soft-gfq-upper-bound}, we obtain
    \[
        \Gamma_i^0 + \sum_{j\in[K]} A_j \le B.
    \].

    \textbf{Induction Step.} Assume $ \Gamma_i^{t-1} \leq B $ for all $ i \in [K] $ at time $ t-1 $. We show that $ \Gamma_i^{t} \leq B $ for all $ i $ at time $ t $, regardless of the valuation $ v_t $ and class $ j_t $ of the arrival at time $ t $. For clarity and ease of exposition, we present the induction step for $ K = 2 $ classes first, and then argue how to extend it to general $ K \geq 2 $ classes. Without loss of generality, let $ j_t = 1 $. We then consider the following cases.

    \emph{Case 1: Quotas met $ (z_1^{t-1} \geq m_1, z_2^{t-1}\geq m_2).$} Firstly, if $ v_t \leq \lambda^{t-1} $, then $ x_t = \frac{m_1}{\alpha}\ln\Bigl(\tfrac{v_t}{\eta_1^{t-1}}\Bigr) $ and $ \eta_1^t = v_t $, therefore we have
    \begin{align*}
        \Gamma_i^t & = Z^{t-1} + x_t  + \frac{m_1}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_1^t}\Bigr) + \frac{m_2}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_2^t}\Bigr) + \frac{B-M}{\alpha} \ln \Bigl(\tfrac{\theta}{\lambda^t}\Bigr)      \\
                   & = Z^{t-1} + \frac{m_1}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_1^{t-1}}\Bigr) + \frac{m_2}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_2^{t-1}}\Bigr) + \frac{B-M}{\alpha} \ln \Bigl(\tfrac{\theta}{\lambda^{t-1}}\Bigr) \\
                   & \leq B, \forall i \in [2],
    \end{align*}
    where the inequality follows from the induction hypothesis. If instead $ v_t > \lambda^{t-1} $, the allocation $ x_t $ includes additionally a global component, meaning $ x_t = \frac{m_1}{\alpha}\ln\Bigl(\tfrac{v_t}{\eta_1^{t-1}}\Bigr) + \frac{B-M}{\alpha}\ln\Bigl(\tfrac{v_t}{\lambda^{t-1}}\Bigr) $, with $ \eta_1^t = v_t $ and $ \lambda^t = v_t $. In that case,
    \begin{align*}
        \Gamma_i^t & = Z^{t-1} + x_t  + \frac{m_1}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_1^t}\Bigr) + \frac{m_2}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_2^t}\Bigr) + \frac{B-M}{\alpha} \ln \Bigl(\tfrac{\theta}{\lambda^t}\Bigr)      \\
                   & = Z^{t-1} + \frac{m_1}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_1^{t-1}}\Bigr) + \frac{m_2}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_2^{t-1}}\Bigr) + \frac{B-M}{\alpha} \ln \Bigl(\tfrac{\theta}{\lambda^{t-1}}\Bigr) \\
                   & \leq B, \forall i \in [2],
    \end{align*}
    where the inequality again follows from the induction hypothesis.

    \emph{Case 2: Quotas not met $ (z_1^{t-1} < m_1, z_2^{t-1}< m_2).$} Assuming first that $ \lambda^{t-1} \geq v_t $, we have $ x_t = \frac{m_1}{\alpha} \ln\Bigl(\tfrac{v_t+\beta_1}{\eta_1^{t-1}+\beta_1}\Bigr) $ and $ \eta_1^t = v_t, \lambda^t = \lambda^{t-1}, \eta_2^t = \eta_2^{t-1} $.

    First, we consider the extreme case where the global allocation only affects class $ 1 $ in the future. Let $ p_1,p_2 $ be such that under some future sequence,
    \begin{align*}
        z_1^{t-1} + x_t + \frac{m_1}{\alpha}\ln\Bigl(\tfrac{p_1+\beta_1}{v_t+\beta_1}\Bigr)
        + \frac{B-M}{\alpha}\ln\Bigl(\tfrac{p_1+\beta_1}{\lambda^{t-1}+\beta_1}\Bigr)              & = m_1, \\
        z_2^{t-1} + \frac{m_2}{\alpha}\ln\Bigl(\tfrac{p_2+\beta_2}{\eta_    2^{t-1}+\beta_2}\Bigr) & = m_2.
    \end{align*}

    We then have,
    \begin{align*}
        \Gamma_1^{t} = Z^{t-1}+ x_t & + \frac{m_1}{\alpha}\ln\left(\frac{p_1 + \beta}{v_t+\beta}\right) + \frac{m_1}{\alpha}\ln\left(\frac{\theta}{p_1}\right) + \frac{m_2}{\alpha}\ln\left(\frac{p_2 + \beta}{\eta^{t-1}_2+\beta}\right) + \frac{m_2}{\alpha}\ln\left(\frac{\theta}{p_2}\right) \\&+  \frac{B-M}{\alpha}\ln\left(\frac{p_1 + \beta}{\lambda^{t-1}+\beta}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta}{p_1}\right) \leq B,
    \end{align*}
    which follows from the induction hypothesis.



    Next, we need to show that $ \Gamma_2^{t} \leq B $ by showing that we have either $ \Gamma_2^{t} \leq \Gamma_1^{t-1} \leq B $ or $ \Gamma_2^{t} \leq \Gamma_2^{t-1} \leq B $.


    we consider the case where the global allocation only affects class $ 2 $ in the future. Let $ q_1,q_2 $ be such that under some future sequence,
    \begin{align*}
        z_1^{t-1} + x_t + \frac{m_1}{\alpha}\ln\Bigl(\tfrac{q_1+\beta_1}{v_t+\beta_1}\Bigr) & = m_1, \\
        z_2^{t-1} + \frac{m_2}{\alpha}\ln\Bigl(\tfrac{q_2+\beta_2}{\eta_2^{t-1}+\beta_2}\Bigr)
        + \frac{B-M}{\alpha}\ln\Bigl(\tfrac{q_2+\beta_2}{\lambda^{t-1}+\beta_2}\Bigr)       & = m_2.
    \end{align*}
    Recall that $ x_t = \frac{m_1}{\alpha} \ln\Bigl(\tfrac{v_t+\beta_1}{\eta_1^{t-1}+\beta_1}\Bigr) $, and summing the two equations above gives
    \begin{align}
        Z^{t-1} + \frac{m_1}{\alpha}\ln\left(\frac{q_1+\beta}{\eta^{t-1}_1+\beta}\right)+\frac{m_2}{\alpha}\ln\left(\frac{q_2+\beta}{\eta^{t-1}_2+\beta}\right) + \frac{B-M}{\alpha}\ln\left(\frac{q_2+\beta}{\lambda^{t-1}+\beta}\right) = M.\label{equation-l2z-sub1}
    \end{align}

    Our goal is to show the following holds:
    \begin{align}
        \Gamma_2^{t} & = Z^{t-1}+ x_t + \frac{m_1}{\alpha}\ln\left(\frac{q_1 + \beta}{v_t+\beta}\right) + \frac{m_1}{\alpha}\ln\left(\frac{\theta}{q_1}\right) + \frac{m_2}{\alpha}\ln\left(\frac{q_2 + \beta}{\eta^{t-1}_2+\beta}\right) + \frac{m_2}{\alpha}\ln\left(\frac{\theta}{q_2}\right) \\&+  \frac{B-M}{\alpha}\ln\left(\frac{q_2 + \beta}{\lambda^{t-1}+\beta}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta}{q_2}\right) \\
                     & = M + \frac{m_1}{\alpha}\ln\left(\frac{\theta}{q_1}\right)+\frac{m_2}{\alpha}\ln\left(\frac{\theta}{q_2}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta}{q_2}\right) \leq B, \label{equation-l2z-target}
    \end{align}

    We first try to show that $ \Gamma_2^{t} \leq \Gamma_1^{t-1} \leq B $. In this case, we have the following from the induction hypothesis of $ \Gamma_1^{t-1} $:
    \begin{align}
         & Z^{t-1} + \frac{m_1}{\alpha}\ln\left(\frac{v_1+\beta}{\eta^{t-1}_1+\beta}\right)+\frac{m_2}{\alpha}\ln\left(\frac{v_2+\beta}{\eta^{t-1}_2+\beta}\right) + \frac{B-M}{\alpha}\ln\left(\frac{v_1+\beta}{\lambda^{t-1}+\beta}\right) = M \label{equation-l2z-sub2} \\
         & M + \frac{m_1}{\alpha}\ln\left(\frac{\theta}{v_1}\right)+\frac{m_2}{\alpha}\ln\left(\frac{\theta}{v_2}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta}{v_1}\right) \leq B \label{equation-l2z-target-2}
    \end{align}

    Subtracting \eqref{equation-l2z-sub2} from \eqref{equation-l2z-sub1}, we have
    \begin{align*}
        m_1\ln\Bigl(\tfrac{v_1+\beta}{q_1+\beta}\Bigr)
        + m_2\ln\Bigl(\tfrac{v_2+\beta}{q_2+\beta}\Bigr)
        + (B-M)\ln\Bigl(\tfrac{v_1+\beta}{q_2+\beta}\Bigr) & = 0
    \end{align*}

    Given that $ v_1 $ corresponds to the case where the global allocation does help class $ 1 $ to satisfy its quota, and $ q_1 $ corresponds to the case where it does not, we know that $ v_1 \leq q_1 $.

    Comparing \eqref{equation-l2z-target} and \eqref{equation-l2z-target-2}, we see that to show $ \Gamma_2^{t} \leq \Gamma_1^{t-1} $, it suffices to show that
    \begin{align*}
        m_1\ln\Bigl(\tfrac{v_1}{q_1}\Bigr)
        + m_2\ln\Bigl(\tfrac{v_2}{q_2}\Bigr)
        + (B-M)\ln\Bigl(\tfrac{v_1}{q_2}\Bigr) & \le 0,
    \end{align*}


    Let $F(x) = m_1\ln\left(\frac{v_1+x}{q_1+x}\right)+m_2\ln\left(\frac{v_2+x}{q_2+x}\right) + (B-M)\ln\left(\frac{v_1+x}{q_2+x}\right)$. If we can show that $F'(x) \geq 0$ for all $x\in[0,\beta]$, we can conclude that $F(0)\leq F(\beta)=0$. Therefore,
    \begin{align*}
        F'(x)= m_1\frac{q_1-v_1}{(q_1+x)(v_1+x)}+m_2\frac{q_2-v_2}{(q_2+x)(v_2+x)} + (B-M)\frac{q_2-v_1}{(q_2+x)(v_1+x)}.
    \end{align*}
    Since $F''(x)\leq 0$, it is only need to show that $F'(\beta)\geq 0$. Based on the well-known inequality of $1+x \geq \ln(x)$, we know that:
    \begin{align*}
        m_1\frac{q_1-v_1}{v_1+x}+m_2\frac{q_2-v_2}{v_2+x} + (B-M)\frac{q_2-v_1}{v_1+x}\geq 0.
    \end{align*}
    Since, $v_1 \leq q_1$, if $q_1 \leq q_2$, we can conclude that $F'(\beta) \geq 0$.


    However, if $q_1 \geq q_2$, we shall show $ \Gamma_2^{t} \leq \Gamma_2^{t-1} \leq B $. From the induction hypothesis of $ \Gamma_2^{t-1} $, we have \eqref{equation-l2z-sub1} and
    \begin{align}
         & Z^{t-1} + \frac{m_1}{\alpha}\ln\left(\frac{r_1+\beta}{\eta^{t-1}_1+\beta}\right)+\frac{m_2}{\alpha}\ln\left(\frac{r_2+\beta}{\eta^{t-1}_2+\beta}\right) + \frac{B-M}{\alpha}\ln\left(\frac{r_2+\beta}{\lambda^{t-1}+\beta}\right) = M\label{euqation-l2z-sub3} \\
         & M + \frac{m_1}{\alpha}\ln\left(\frac{\theta}{r_1}\right)+\frac{m_2}{\alpha}\ln\left(\frac{\theta}{r_2}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta}{r_2}\right) \leq B \label{equation-l2z-target-4}
    \end{align}

    Subtracting \eqref{euqation-l2z-sub3} from \eqref{equation-l2z-sub1}, we have
    \begin{align*}
        m_1\ln\Bigl(\tfrac{r_1+\beta}{q_1+\beta}\Bigr)
        + m_2\ln\Bigl(\tfrac{r_2+\beta}{q_2+\beta}\Bigr)
        + (B-M)\ln\Bigl(\tfrac{r_2+\beta}{q_2+\beta}\Bigr) & = 0
    \end{align*}
    Similarly, we compare \eqref{equation-l2z-target-4} and \eqref{equation-l2z-target} to see that to show $ \Gamma_2^{t} \leq \Gamma_2^{t-1} $, it suffices to show that
    \begin{align*}
        m_1\ln\Bigl(\tfrac{r_1}{q_1}\Bigr)
        + m_2\ln\Bigl(\tfrac{r_2}{q_2}\Bigr)
        + (B-M)\ln\Bigl(\tfrac{r_2}{q_2}\Bigr) & \le 0,
    \end{align*},
    by defining $G(x) = m_1\ln\left(\frac{r_1+x}{q_1+x}\right)+m_2\ln\left(\frac{r_2+x}{q_2+x}\right) + (B-M)\ln\left(\frac{r_2+x}{q_2+x}\right)$, and following the same argument as before, we have $G'(\beta) \geq 0$.

    As a result, at least one of $F(0)$ or $G(0)$ is less than 0 which finishes the proof.

    \textbf{Generalizing to $ K \geq 2 $ classes.} Given $ \Gamma_i^{t-1} \leq B $ for all $ i \in [K] $ at time $ t-1 $, we need to show that $ \Gamma_i^{t} \leq B $ for all $ i $ at time $ t $. Given $ j_t = j $, it is sufficient to show that we have $ \Gamma_i^{t} \leq \Gamma_i^{t-1} $ or $ \Gamma_i^{t} \leq \Gamma_j^{t-1} $ for each $ i \in [K] $. In this case, for any fixed $ i \in [K] $, the transition values from classes $ k \in [K]\setminus\{i, j\} $ are fixed, the proof then follows similarly as the $ K = 2 $ case by considering only classes $ i $ and $ j $.

    Given that $ v_1 $ corresponds to the case where the global allocation does help class $ 1 $ to satisfy its quota, and $ q_1 $ corresponds to the case where it does not, we know that $ v_1 \leq q_1 $.


\end{proof}

\subsection{Necessary Conditions}

The hard instance for linear-to-zero penalties is similar to the zero-to-linear case. We again consider a \textit{Semi-Adaptive} adversary that chooses the hard instance based on the initial reservation for each group.



\begin{definition}[Hard Instance Family for Soft Quota with Linear-to-Zero Penalty]
    Let $\Pi^K$ denote the set of all permutations of $[K]$. For any permutation $\pi \in \Pi^K$ and sufficiently small $\epsilon > 0$, we define the hard instance $I^{\text{L}\to\text{Z}}_\pi$ as a sequence of $K+1$ batches of arrivals constructed as follows:
    \begin{enumerate}
        \item \textbf{Initial batch:} One arrival from each group $j \in [K]$ with value $v = 1$, all arriving simultaneously.
        \item \textbf{Subsequent batches:} For each group $j \in [K]$ in order $\pi_1, \pi_2, \ldots, \pi_K$, a continuum of arrivals with values continuously increasing from $ 1 + \epsilon $ to $ \theta $.
    \end{enumerate}

    Formally, the instance is represented as:
    \begin{align*}
        I^{\text{L}\to\text{Z}}_\pi =  \Biggl\{
        \underbrace{(1,\pi_1), (1, \pi_2), \dots (1, \pi_K)}_{\text{Initial batch}},
         & \underbrace{(1+\epsilon, \pi_1), (1 + 2\epsilon, \pi_1), \dots, (\theta, \pi_1)}_{\text{Arrivals for group $\pi_1$}}, \dots, \\
         & \underbrace{(1+\epsilon, \pi_K), (1 + 2\epsilon, \pi_K), \dots, (\theta_K, \pi_K)}_{\text{Arrivals for group $\pi_K$}}
        \Biggr\},
    \end{align*}
    where each element $(v,j)$ represents an arrival from group $j$ with valuation $v$.
\end{definition}

The key observation is that the initial batch remains identical across all instances $I^{\text{L}\to\text{Z}}_\pi$ for any permutation $\pi \in \Pi^K$. Consequently, any optimal online algorithm must make allocation decisions that are robust against the adversarial choice of arrival order in subsequent batches.

\begin{theorem}[Necessary Conditions for Linear-to-Zero Penalty]\label{theorem-linear2zero-soft-quota-necessary}
    Let $\alpha > 1$ be a competitive ratio. If there exists an $\alpha$-competitive online algorithm for all instances $I^{\text{L}\to\text{Z}}_\pi$ with $\pi \in \Pi^K$, then there must exist utilization functions $\psi^\pi_j: [1, \theta] \to [0, b_j]$ for each group $j \in [K]$ and permutation $\pi \in \Pi^K$, where the budgets for each group $\{b_j\}_{j \in [K]}$ satisfies $\sum_{j\in[K]}b_j \leq 1$, such that the following system of inequalities holds:

    \begin{subequations}\label{equation-linear2zero-necessary-conditions}
        \begin{gather}
            \sum_{j\in [K]} \Psi^\pi_j(1) \geq \frac{1}{\alpha}\left((1-M+m_{\pi_1})v_{\pi} + \sum_{j=2}^K m_{\pi_j}\right), \label{equation-linear2zero-necessary-initial-condition}\\[0.5em]
            \Psi^\pi_{\pi_1}(v_{\pi_1})+\sum_{j=2}^K \Psi^\pi_{\pi_j}(1) \geq \frac{1}{\alpha} \left(\sum_{j=2}^K m_{\pi_j} +(1-M+m_{\pi_1})v_{\pi_1}\right), \label{equation-linear2zero-necessary-first-group-condition}\\
            \forall v_{\pi_1} \in [v_\pi, \theta], \nonumber\\[0.5em]
            \sum_{j=1}^{i-1} \Psi^\pi_{\pi_j}(\theta) + \Psi^\pi_{\pi_i}(v_{\pi_i}) +\sum_{j=i+1}^K \Psi^\pi_{\pi_j}(1) \geq \frac{1}{\alpha} \left((1-M+\sum_{j=1}^{i-1}m_{\pi_j})\theta + m_{\pi_i}\cdot v_{\pi_i}+\sum_{j=i+1}^K m_{\pi_j}\right), \label{equation-linear2zero-necessary-general-condition}\\
            \forall i\in[2,K], \forall v_{\pi_i} \in [1+\epsilon, \theta], \nonumber\\[0.5em]
            \sum_{j\in [K]}\psi^\pi_j (\theta) \leq 1, \label{equation-linear2zero-necessary-boundary-condition}
        \end{gather}
    \end{subequations}
    for all permutations $\pi \in \Pi^K$.

    \noindent where:
    \begin{itemize}
        \item $\Psi^\pi_j(v) := \psi^\pi_j(1)+\int_1^v u \, d\psi^\pi_j(u) - f_j(\psi^\pi_j(v))$ represents the net utility function for group $j$ under permutation $\pi$,
        \item $M := \sum_{j\in[K]}m_j$ denotes the total desired allocation across all groups,
        \item $f_j(\cdot)$ is the linear-to-zero penalty function for group $j$.
    \end{itemize}
\end{theorem}

\begin{proof}
    The proof follows by considering the performance of any $\alpha$-competitive algorithm against the adversarial instances.

    \paragraph{Initial Condition \eqref{equation-linear2zero-necessary-initial-condition}} After the first batch of arrivals, the online algorithm's performance can be captured by the left-hand side of \eqref{equation-linear2zero-necessary-initial-condition}. With the second batch of arrivals for group $\pi_1$, there must exist a critical value $v_\pi \in (1, \theta]$ such that:
    \begin{itemize}
        \item Up until $ v_\pi $, i.e., $ \forall v < v_\pi $, the left-hand side of \eqref{equation-linear2zero-necessary-initial-condition} strictly exceeds the right-hand side, indicating that the current allocation is sufficient to guarantee $ \alpha $ competitiveness.
        \item At $v = v_\pi$, equality holds in \eqref{equation-linear2zero-necessary-initial-condition}, marking the point where the algorithm must begin allocating to maintain competitiveness.
        \item After $ v_\pi $, the algorithm must allocate more resources to prevent the right-hand side from exceeding the left-hand side, which would then violate the $\alpha$-competitiveness requirement.
    \end{itemize}

    Meanwhile, with the initial batch and the second batch for group $ \pi_1 $ with values up to $ v_\pi $, the offline optimal is to allocate $ m_{\pi_j} $ amount of resources to each group $ \pi_j \in [K] $, and allocate the remaining portion $ 1 - M $ to class $ \pi_1 $ with value $ v_{\pi} $, leading to the optimal value as described on the right-hand side of \eqref{equation-linear2zero-necessary-initial-condition}.

    \paragraph{Incremental Condition (I) \eqref{equation-linear2zero-necessary-first-group-condition}} In the second batch of arrival for group $ \pi_1 $, as the value of arrivals increase beyond  $ v_\pi $, the online algorithm must maintain $ \alpha $-competitiveness against the offline optimal, which can be expressed as followed for any $ v_{\pi_1} \in [v_\pi, \theta] $:
    \begin{align*}
        \OPT(I^{\text{L}\to\text{Z}}_\pi) \leq (1-M+m_{\pi_1})v_{\pi_1} + \sum_{j=2}^K m_{\pi_j}.
    \end{align*}
    On the other hand, the performance of the online algorithm is
    \begin{align*}
        \ALG(I^{\text{L}\to\text{Z}}_\pi) \geq \Psi^\pi_{\pi_1}(v_{\pi_1})+\sum_{j=2}^K \Psi^\pi_{\pi_j}(1),
    \end{align*}
    which implies second necessary condition.

    \paragraph{General Incremental Condition \eqref{equation-linear2zero-necessary-general-condition}}
    We now analyze the general case where batches for groups $\pi_1, \ldots, \pi_{i-1}$ have already arrived, we are currently processing arrivals from group $\pi_i$ with maximum observed value $v_{\pi_i} \in [1+\epsilon, \theta]$, and batches for groups $\pi_{i+1}, \ldots, \pi_K$ are yet to arrive.

    The offline optimal strategy in this scenario is to construct an allocation that maximizes total welfare by:
    \begin{itemize}
        \item Allocating $m_{\pi_j}$ units to each completed group $\pi_j$ for $j \in \{1, \ldots, i-1\}$, selecting arrivals with the maximum value $\theta$ from their respective batches.
        \item Allocating $m_{\pi_i}$ units to the current group $\pi_i$, selecting the arrival with value $v_{\pi_i}$.
        \item Allocating $m_{\pi_j}$ units for each group $\pi_j$ for $j \in \{i+1, \ldots, K\}$, using the arrivals with value 1 from the initial batch.
        \item Allocating the remaining capacity $(1-M+\sum_{j=1}^{i-1}m_{\pi_j})$ to any arrival with value $ \theta $ in previous batches, regardless of their group.
    \end{itemize}
    This allocation strategy leads to the following upper bound on the offline optimal performance for any $ v_{\pi_i} \in [1+\epsilon, \theta] $:
    \begin{align*}
        \OPT(I^{\text{L}\to\text{Z}}_\pi) \leq (1-M+\sum_{j=1}^{i-1}m_{\pi_j})\theta + m_{\pi_i} v_{\pi_i}+\sum_{j=i+1}^K m_{\pi_j}.
    \end{align*}
    Meanwhile, the performance of the online algorithm is
    \begin{align*}
        \ALG(I^{\text{L}\to\text{Z}}_\pi) \geq \sum_{j=1}^{i-1} \Psi^\pi_{\pi_j}(\theta) + \Psi^\pi_{\pi_i}(v_{\pi_i}) +\sum_{j=i+1}^K \Psi^\pi_{\pi_j}(1),
    \end{align*}
    which implies the third necessary condition.

    \paragraph{Boundary condition \eqref{equation-linear2zero-necessary-boundary-condition}}
    The final condition ensures that the total allocation across all groups does not exceed the available capacity. Since $\psi^\pi_j(\theta)$ represents the maximum allocation that can be made to group $j$ under permutation $\pi$ when the highest-value arrivals are observed, the constraint $\sum_{j\in [K]}\psi^\pi_j (\theta) \leq 1$ directly enforces the overall capacity constraint.
\end{proof}

From Theorem \ref{theorem-linear2zero-soft-quota-necessary}, we can observe that the initial reservations $ \psi^\pi_j(1) $ for each group $ j $ and the arrival order $ \pi $ play a key role. To obtain the lower bound on the competitive ratio $ \alpha $, we need to solve an optimization problem that satisfies the necessary conditions in \eqref{equation-linear2zero-necessary-conditions} for all permutations $ \pi \in \Pi^K $. Below, we provide a case study for the case of two groups with equal penalty parameters $ \beta_1 = \beta_2 = \beta $.

\begin{corollary}
    [Lower Bound for Two Groups with Linear-to-Zero Penalties]\label{theorem-linear2zero-soft-quota-lower-bound-two-groups}
    For $ K = 2 $ groups with linear-to-zero penalty functions and equal penalty parameters $ \beta_1 = \beta_2 = \beta $, under the family of hard instances $ I^{\text{L}\to\text{Z}}_\pi $, no online algorithm can achieve a competitive ratio better than $ \alpha^* $, where $ \alpha^* $ is the optimal value of the following optimization problem:
    \begin{subequations}\label{equation-linear2zero-two-groups-optimization}
        \begin{align}
            \min \quad        & \alpha                                                                                                                                                        \label{equation-linear2zero-two-groups-objective}   \\[0.5em]
            \text{s.t.} \quad & \frac{1-M+m_{\pi_1}}{\alpha}\ln\left(\frac{v^*_{\pi_1} +\beta}{v_{\pi}+\beta}\right) + \psi_{\pi_1}(1) = m_{\pi_1},                                           \label{equation-linear2zero-two-groups-constraint1} \\[0.3em]
                              & \frac{m_{\pi_2}}{\alpha}\ln\left(\frac{v^*_{\pi_2} +\beta}{1+\beta}\right) + \psi_{\pi_2}(1) = m_{\pi_2},                                                     \label{equation-linear2zero-two-groups-constraint2} \\[0.3em]
                              & 1 \geq m_1 + m_2 + \frac{1-M+m_{\pi_1}}{\alpha}\ln\left(\frac{\theta}{v^*_{\pi_1}}\right)+\frac{m_{\pi_2}}{\alpha}\ln\left(\frac{\theta}{v^*_{\pi_2}}\right), \label{equation-linear2zero-two-groups-constraint3} \\[0.3em]
                              & \sum_{j\in[2]}\left[\psi_{\pi_j}(1)-\beta(m_{\pi_j} - \psi_{\pi_j}(1))^+\right] \geq \frac{1}{\alpha}\left((1-M+m_{\pi_1})v_\pi + m_{\pi_2}\right), \label{equation-linear2zero-two-groups-constraint4}
        \end{align}
    \end{subequations}  for all $\pi\in\Pi^2$.
\end{corollary}

\begin{proof}
    % First, we can rewrite the system of inequalities in Theorem \ref{theorem-linear2zero-soft-quota-necessary} for the case of two groups ($K=2$) with equal penalty parameters ($\beta_1 = \beta_2 = \beta$).

    % \paragraph{Reduction to Binding Relations} The necessary conditions in Theorem \ref{theorem-linear2zero-soft-quota-necessary} are a system of inequalities. Let $v_\pi \in (1,\theta]$ denote the first value at which allocation to $\pi_1$ becomes strictly positive. On any interval where $\psi^\pi_{\pi_i}$ is (right-)differentiable and strictly increasing, the corresponding inequality must hold with equality; otherwise a local decrease of the allocation would preserve feasibility and strictly improve $\alpha$. Thus equalities hold precisely on active intervals and at $v_\pi$ for the initial condition. We therefore retain the original inequality directions:
    % \begin{subequations}\label{equation-linear2zero-two-groups-necessary}
    %     \begin{align}
    %         \sum_{j\in [2]} \Psi^\pi_j(1)                          & \ge \frac{1}{\alpha}\left((1-M+m_{\pi_1})v_{\pi} + m_{\pi_2}\right)                                                             &  & \text{(tight at } v_{\pi}\text{)}, \label{equation-linear2zero-two-groups-initial}                      \\
    %         \Psi^\pi_{\pi_1}(v_{\pi_1})+\Psi^\pi_{\pi_2}(1)        & \ge \frac{1}{\alpha} \left(m_{\pi_2} +(1-M+m_{\pi_1})v_{\pi_1}\right), \quad \forall v_{\pi_1} \in [v_\pi, \theta]              &  & \text{(tight when } \psi^\pi_{\pi_1} \text{ increases)}, \label{equation-linear2zero-two-groups-first}  \\
    %         \Psi^\pi_{\pi_1}(\theta) + \Psi^\pi_{\pi_2}(v_{\pi_2}) & \ge \frac{1}{\alpha} \left((1-M+m_{\pi_1})\theta + m_{\pi_2} v_{\pi_2}\right), \quad \forall v_{\pi_2} \in [1+\epsilon, \theta] &  & \text{(tight when } \psi^\pi_{\pi_2} \text{ increases)}, \label{equation-linear2zero-two-groups-second} \\
    %         \sum_{j\in [2]}\psi^\pi_j (\theta)                     & \le 1                                                                                                                           &  & \text{(may be slack)}. \label{equation-linear2zero-two-groups-capacity}
    %     \end{align}
    % \end{subequations}

    % For $i\in\{1,2\}$, where $\psi^\pi_{\pi_i}$ is absolutely continuous,
    % \[
    %     \Psi^\pi_{\pi_i}(v) = \psi^\pi_{\pi_i}(1) + \int_1^v u\,d\psi^\pi_{\pi_i}(u) - f_{\pi_i}(\psi^\pi_{\pi_i}(v))
    % \]
    % yields, by differentiation,
    % \begin{align*}
    %     \frac{d}{dv}\Psi^\pi_{\pi_i}(v) & = v\frac{d\psi^\pi_{\pi_i}}{dv} - f'_{\pi_i}(\psi^\pi_{\pi_i}(v))\frac{d\psi^\pi_{\pi_i}}{dv}           \\
    %                                     & = \frac{d\psi^\pi_{\pi_i}}{dv}\left[v - f'_{\pi_i}(\psi^\pi_{\pi_i}(v))\right]                          \\
    %                                     & = \begin{cases}
    %                                             (v + \beta)\frac{d\psi^\pi_{\pi_i}}{dv}, & \text{if } \psi^\pi_{\pi_i}(v) < m_{\pi_i},    \\
    %                                             v\frac{d\psi^\pi_{\pi_i}}{dv},           & \text{if } \psi^\pi_{\pi_i}(v) \geq m_{\pi_i}.
    %                                         \end{cases}
    % \end{align*}

    % On any active interval for group $\pi_1$, differentiating \eqref{equation-linear2zero-two-groups-first} gives
    % \begin{align*}
    %     \frac{d\Psi^\pi_{\pi_1}}{dv_{\pi_1}} = \frac{1-M+m_{\pi_1}}{\alpha}.
    % \end{align*}
    % Let $v^*_{\pi_1}$ be the critical value where $\psi^\pi_{\pi_1}(v^*_{\pi_1}) = m_{\pi_1}$. For $v_{\pi_1} \in [v_\pi, v^*_{\pi_1}]$, we have:
    % \begin{align*}
    %     (v_{\pi_1} + \beta)\frac{d\psi^\pi_{\pi_1}}{dv_{\pi_1}} = \frac{1-M+m_{\pi_1}}{\alpha} \quad \Rightarrow \quad \frac{d\psi^\pi_{\pi_1}}{dv_{\pi_1}} = \frac{1-M+m_{\pi_1}}{\alpha(v_{\pi_1}+\beta)}.
    % \end{align*}
    % Integrating from $v_\pi$ to $v^*_{\pi_1}$ and using $\psi^\pi_{\pi_1}(v^*_{\pi_1}) = m_{\pi_1}$:
    % \begin{align*}
    %     m_{\pi_1} - \psi^\pi_{\pi_1}(v_\pi) = \frac{1-M+m_{\pi_1}}{\alpha}\ln\left(\frac{v^*_{\pi_1} + \beta}{v_\pi + \beta}\right).
    % \end{align*}
    % Since $\psi^\pi_{\pi_1}(v_\pi) = \psi^\pi_{\pi_1}(1)$ (no allocation before $v_\pi$), this gives \eqref{equation-linear2zero-two-groups-constraint1}.

    % An identical argument applied to \eqref{equation-linear2zero-two-groups-second} yields \eqref{equation-linear2zero-two-groups-constraint2} for $\pi_2$ on its active interval(s).

    % For $v > v^*_{\pi_i}$, we obtain
    % \begin{align*}
    %     v\frac{d\psi^\pi_{\pi_i}}{dv} = \frac{1-M+m_{\pi_1}}{\alpha} \quad \Rightarrow \quad \psi^\pi_{\pi_i}(v) = m_{\pi_i} + \frac{1-M+m_{\pi_1}}{\alpha}\ln\left(\frac{v}{v^*_{\pi_i}}\right).
    % \end{align*}

    % Substituting these expressions: (i) evaluating at $v=v^*_{\pi_i}$ gives \eqref{equation-linear2zero-two-groups-constraint1}--\eqref{equation-linear2zero-two-groups-constraint2}; (ii) inserting $\psi^\pi_{\pi_i}(\theta) = m_{\pi_i} + \frac{1-M+m_{\pi_1}}{\alpha}\ln(\theta/v^*_{\pi_i})$ in the capacity inequality yields \eqref{equation-linear2zero-two-groups-constraint3}; and (iii) using $\Psi^\pi_j(1)=\psi^\pi_j(1)-\beta(m_{\pi_j}-\psi^\pi_j(1))^+$ in the initial inequality gives \eqref{equation-linear2zero-two-groups-constraint4}.

    % Since the optimization problem must hold for both permutations $\pi \in \Pi^2$, the result follows.

\end{proof}


\subsection{Connections to Hard Quota Constraints}