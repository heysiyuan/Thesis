Consider the case where the penalty function $ f_j $ for each group $ j $ is linearly decreasing before its desired allocation amount $ m_j $ and 0 afterwards. Namely, we have:
\begin{align*}
    f_j(x) = \begin{cases}
                 f_j^L(x) = -\beta_j x + \beta_j m_j, x\in [0, m_j], \\
                 f_j^R(x) = 0, x\in [m_j, \infty),
             \end{cases}
\end{align*}


The offline optimization problem can then be expressed as follows:
\begin{subequations}\label{equation-linear2zero-soft-quota-primal}
    \begin{align}
        \max_{0 \leq x_t \leq r_t}\quad
         & \sum_{t \in [T]} v_tx_t -
        \sum_{j \in [K]}
        \beta_j
        \Bigl[\,m_j - \sum_{t=1}^{T} x_t \cdot\mathbf 1_{\{j_t = j\}}\Bigr]^{+} \\[6pt]
        \text{s.t.}\quad
         & \sum_{t=1}^{T} x_t \leq 1
    \end{align}
\end{subequations}

\paragraph{Remark} We may take an alternative view of the optimization problem by considering it as a subsidy of $ \beta_j $ for the first $ m_j $ units of allocation to group $ j $. In this case, the optimization problem can be reformulated as follows,
\begin{subequations}\label{equation-linear2zero-subsidy}
    \begin{align}
        \max_{0 \leq x_t \leq r_t}\quad
         & \sum_{t \in [T]} (v_t + \beta_{j_t}) x_t -
        \sum_{j \in [K]}
        \beta_j
        \Bigl[\sum_{t \in [T]} x_t \cdot \mathbf{1}_{\{j_t = j\}} - m_j\Bigr]^{+} \\[6pt]
        \text{s.t.}\quad
         & \sum_{t=1}^{T} x_t \leq 1
    \end{align}
\end{subequations}.


\subsection{Sufficient Conditions}

Given the penalty function, we introduce auxiliary variables $ y_j \geq 0 $ for each group $ j\in [K] $ to represent the shortfall from its desired allocation $ m_j $. Thus, we can reformulate the offline optimization problem \eqref{equation-linear2zero-soft-quota-primal} as follows:
\begin{subequations}\label{equation-linear2zero-soft-quota-alternative-primal}
    \begin{align}
        \max_{x_t \geq 0, y_j \geq 0} & \sum_{i\in[K]}\left(\sum_{t\in[T]}v_t \cdot x_t \cdot \mathbf{1}_{\{j_t = j\}} - \beta_j \cdot y_j\right) \\
        \text{s.t. }                  & y_j \geq m_j - \sum_{t\in[T]}x_t \cdot \mathbf{1}_{\{j_t = j\}}\label{linear2zero-y-constraint}           \\
                                      & \sum_{t\in[T]} x_t \leq B \label{linear2zero-budget-constraint}
    \end{align}
\end{subequations}

Introducing dual variables $ \eta_j \geq 0 $ for constraints \eqref{linear2zero-y-constraint} and $ \lambda \geq 0 $ for the capacity constraint \eqref{linear2zero-budget-constraint}, we can derive its corresponding dual problem:
\begin{subequations}
    \begin{align}
        \min~        & (B-M)\cdot\lambda + \sum_{j\in[K]}m_j \cdot \eta_j              \\
        \text{s.t. } & \eta_{j_t} \geq v_t, \quad \forall t\in [T]                     \\
                     & \lambda \geq \eta_j \geq \lambda-\beta_j, \quad \forall j\in[K]
    \end{align}
\end{subequations}

Based on the online primal dual framework, now we present an algorithm that updates the dual variables at each time step and based on the updates of the dual variables, makes the allocation decisions of the primal variables. The updates of the dual variables directly follows from the \OPT in the offline setting. From solving the offline optimization problem \eqref{equation-linear2zero-soft-quota-primal}, we observe that
\begin{align*}
    \OPT = (B-M)\cdot v^{\max} + \sum_{j\in[K]} m_j\cdot \max \{v^{\max}_j, v^{\max}-\beta_j\},
\end{align*}
where $ v^{\max}_j $ is the maximum value among all arrivals from group $ j $, and $ v^{\max} = \max_{j\in[K]} v^{\max}_j $.
Intuitively,we need to select $ \lambda $ in a way that it updates according to $ v^{\max} $, and  $\eta_{j_t} $ according to $ \max \{v^{\max}_{j_t}, v^{\max}-\beta_{j_t}\} $ at each time step $ t $. Based on this intuition, we present the algorithm Linear-To-Zero in Algorithm \ref{alg_LinearToZero}.
\begin{algorithm}
    \caption{Linear-To-Zero}
    \label{alg_LinearToZero}
    \begin{algorithmic}[1]

        \Require $B, \theta$; $\{m_j, \beta_j, A_j\}_{j\in[K]}$ and $\alpha$
        \State Initialize $\eta_j^0 = 1$, $z_j^0 = 0$ for all $j \in [K]$, and $\lambda^0 = 1$

        \While{agent $t$ arrives}
        \State Obtain the agent's value $ v_t $ and class $j_t$

        \If{$z_{j_t}^{t-1} = 0$}
        \State $y_{j_t} = A_{j_t}$
        \Else
        \State $y_{j_t} = 0$
        \EndIf

        \State $x^G_t = \argmax_a \left\{ \frac{v_t + \beta^t_{j_t}}{\lambda^{t-1} + \beta^t_{j_t}} \cdot a - \int_0^{a} \exp\left(\frac{\alpha}{B-M} u\right)du \right\}$

        \State $x^{j_t}_t = \argmax_a \left\{ \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_{j_t} + \beta^t_{j_t}} \cdot a - \int_0^{a} \exp\left(\frac{\alpha}{m_{j_t}} u\right)du \right\}$

        \For{each $i \in [K]\setminus\{j_t\}$}
        \State $x^i_t = \argmax_a \left\{ \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_{j_t} + \beta^t_{j_t} + \beta_i} \cdot a - \int_0^{a} \exp\left(\frac{\alpha}{m_i} u\right)du \right\}$
        \EndFor

        \State $x_t = y_{j_t} + x^G_t + \sum_{j\in[K]} x^j_t$ \Comment{Total allocation}

        \State Update $z_{j_t}^t = z_{j_t}^{t-1} + x_t$

        \State Update:
        \[
            \eta^t_{j_t} = \max\{\eta^{t-1}_{j_t}, v_t\}, \quad
            \lambda^t = \max\{\lambda^{t-1}, v_t\}, \quad
            \eta^t_i = \max\{\eta^{t-1}_i, v_t - \beta_i\} \ \forall i \neq j_t
        \]

        \EndWhile

    \end{algorithmic}
\end{algorithm}

In Algorithm \ref{alg_LinearToZero}, $ z_j^t $ denotes the utilization level of class $ j $ by time $ t $, and $ \beta_j^t  = \beta_j \cdot \mathbf{1}_{\{z_j^{t-1} < m_j\}} $ indicates whether the subsidy for class $ j $ is still active at time $ t $.

Depending on the value $ v_t $, there are four sources of allocation for agent $ t $:
\begin{itemize}
    \item {\bf Initial Reservation $ y_{j_t} $:} if class $ j_t $ has not yet reached its reserved allocation amount $ A_{j_t} $, allocate $ y_{j_t} = A_{j_t} $ to agent $ t $.
    \item {\bf Global Allocation $ x^G_t $:} allocate an additional amount $ x^G_t $ based on the global dual variable $ \lambda^{t-1} $ which is shared across all classes. $ x^G_t $ is only non-zero when the agent's value $ v_t $ exceeds the current global dual variable $ \lambda^{t-1} $, meaning $ v_t $ is the highest value seen so far.
    \item {\bf Class-specific Allocation $ x^{j_t}_t $:} allocate an additional amount $ x^{j_t}_t $ based on the class-specific dual variable $ \eta^{t-1}_{j_t} $. This allocation is only non-zero when the agent's value $ v_t $ exceeds the current class-specific dual variable $ \eta^{t-1}_{j_t} $.
    \item {\bf Cross-class Allocation $ x^i_t $:} For every other class $ i \in [K]\setminus\{j_t\} $, allocate an additional amount $ x^i_t $ based on the adjusted class-specific dual variable $ \eta^{t-1}_i + \beta_i $. Cross-class allocation is only non-zero when the agent's value $ v_t $ is high enough to compensate the under allocation penalty for class $ i $.
\end{itemize}


In addition to the problem parameters, Algorithm \ref{alg_LinearToZero} additionally takes as input $ \{A_j\}_{\forall j\in [K]} $ as reservations for each class and $ \alpha $. In the following theorem, we provide sufficient conditions on designing $ \{A_j\}_{\forall j\in [K]} $ and $ \alpha $ such that Algorithm \ref{alg_LinearToZero} achieves the competitive ratio of $ \alpha $.

\begin{theorem}\label{theorem-linear2zero-soft-gfq-upper-bound}
    For any $ \alpha \geq \alpha^* $, Algorithm \ref{alg_LinearToZero} is $ \alpha $-competitive and produces a feasible solution, where $ \alpha^* $ is the optimal value of the following optimization problem:
    \begin{align*}
        \min_{\{A_j\}_{\forall j \in[K]}} & \alpha                                                                                                                                                                                                                                                                                            \\
        \text{s.t. }                      & \sum_{j\in[K]}A_j \geq \frac{B}{\alpha}                                                                                                                                                                                                                                                           \\
                                          & \begin{cases}A_{j} + \frac{B-M+m_{j}}{\alpha}\ln\left(\frac{v^*_{j,j}+\beta_{j}}{1+\beta_{j}}\right)=m_{j}                           \\
                                                A_{i} + \frac{m_{i}}{\alpha}\ln\left(\frac{v^*_{i,j}+\beta_{i}}{1+\beta_{i}}\right)=m_{i}, \quad \forall i \in[K]/\{j\} \\
                                                M+\frac{B-M+m_{j}}{\alpha}\ln\left(\frac{\theta}{v^*_{j,j}}\right)+\sum_{i \in[K]/\{j\}}\frac{m_{i}}{\alpha}\ln\left(\frac{\theta}{v^*_{i,j}}\right) \leq B
                                            \end{cases} \\
                                          & \hspace{5.5cm} ,\forall j \in [K]
    \end{align*}
\end{theorem}

\begin{proof}
    The proof to Theorem \ref{theorem-linear2zero-soft-gfq-upper-bound} follows the primal-dual analysis framework. First, we need to show that the dual variables are feasible at each step. second, we need to show that at each time step $ t $, the incremental inequality holds for the change in the primal and dual objectives. Lastly, we need to show that the obtained primal solution is feasible by showing that the total allocation does not exceed the capacity $ B $, regardless of the arrival sequence.

    \paragraph{Dual Feasibility.} Based on the update rules in Algorithm \ref{alg_LinearToZero}, it is straightforward to verify that the dual variables $ \eta_j^t $ and $ \lambda^t $ satisfy the dual feasibility conditions at each time step $ t $.

        {\color{red} TODO: Further consolidate the proofs by showing that it holds for any source of allocation, and then apply to different cases.}
    \paragraph{Incremental Inequality.} Let $ \Delta P_t= P_t - P_{t-1} $ and $ \Delta D_t = D_t - D_{t-1} $ denote the changes in the primal and dual objectives at time step $ t $, respectively. First, note that the primal change at each time step $ t $ can be expressed as the sum of the value obtained and the subsidy provided:
    \begin{align*}
        \Delta P_t = v_t\cdot x_t + \beta^t_{j_t} \cdot x_t = (v_t + \beta^t_{j_t}) \cdot x_t.
    \end{align*}
    Based on the arriving agent's value $ v_t $, we consider the following cases for the change in the dual objective:
    \paragraph{Case 1.} If $ v_t < \eta^{t-1}_{j_t} $ and $ v_t < \lambda^{t-1} $, then no dual variable is updated at time $ t $, and thus $ \Delta D_t = 0 $. It is also easy to see that the primal allocation $ x_t = 0 $ in this case, leading to $ \Delta P_t = 0 $. Therefore, we have $ \Delta P_t = 0 \geq \Delta D_t/\alpha $.

    \paragraph{Case 2.} If $ v_t \geq \eta^{t-1}_{j_t} $ but $ v_t < \lambda^{t-1} $, the only source of allocation is the class-specific allocation $ x^{j_t}_t $. The dual change can be expressed as:
    \begin{align*}
        \Delta D_t = m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t})
    \end{align*}
    By the design of $ x^{j_t}_t $ in Algorithm \ref{alg_LinearToZero} being the maximizer of the stated expression, we have the following from the first-order condition:
    \begin{align*}
        \exp(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t) = \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_{j_t} + \beta^t_{j_t}}.
    \end{align*}
    Rearranging the terms gives
    \begin{align*}
        v_t = (\eta^{t-1}_{j_t} + \beta^t_{j_t})\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) - \beta^t_{j_t}.
    \end{align*}
    Given the update rule $ \eta^t_{j_t} = \max\{\eta^{t-1}_{j_t}, v_t\} = v_t $ in this case, we have
    \begin{align*}
        \Delta D_t & = m_{j_t}\cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t})                                                                                                     \\
                   & = m_{j_t} \cdot \left((\eta^{t-1}_{j_t} + \beta^t_{j_t})\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) - \beta^t_{j_t} - \eta^{t-1}_{j_t}\right) \\
                   & = m_{j_t} \cdot (\eta^{t-1}_{j_t} + \beta^t_{j_t})\left(\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) - 1\right),
    \end{align*}
    and similarly,
    \begin{align*}
        \Delta P_t & = (v_t + \beta^t_{j_t}) x^{j_t}_t                                                                 \\
                   & = (\eta^{t-1}_{j_t} + \beta^t_{j_t})\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) x^{j_t}_t.
    \end{align*}
    To show $ \Delta P_t \geq \Delta D_t/\alpha $, we need to verify the following inequality:
    \begin{align*}
        (\eta^{t-1}_{j_t} + \beta^t_{j_t})\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) x^{j_t}_t \geq \frac{1}{\alpha} m_{j_t} \cdot (\eta^{t-1}_{j_t} + \beta^t_{j_t})\left(\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) - 1\right).
    \end{align*}
    Given that $ \eta^{t-1}_{j_t} + \beta^t_{j_t} > 0 $, we can divide both sides by this positive factor. Furthermore, let $ y := \frac{\alpha}{m_{j_t}} x^{j_t}_t \geq 0 $, then $ x^{j_t}_t = (m_{j_t}/\alpha) y $, and the inequality to prove becomes
    \begin{align*}
        e^{y} \cdot \frac{m_{j_t}}{\alpha} y & \geq \frac{m_{j_t}}{\alpha} \bigl(e^{y} - 1\bigr) \\
        e^{y} y                              & \geq e^{y} - 1
    \end{align*}
    Define $ g(y) := e^{y} y - e^{y} + 1 = e^{y}(y-1) + 1 $. Given that $ g(0) = 0, g'(y) = e^{y}(y-1) + e^{y} = e^{y} y \geq 0 $ for all $ y \geq 0 $, $ g $ is non-decreasing on $ [0, \infty) $ and $ g(y) \geq g(0) = 0 $ for all $ y \geq 0 $, which is equivalent to $ e^{y} y \geq e^{y} - 1 $ for all $ y \geq 0 $. Therefore, we obtain $ \Delta P_t \geq \Delta D_t/\alpha $.

    \paragraph{Case 3.} If $ v_t \geq \eta^{t-1}_{j_t} $ and $ v_t \geq \lambda^{t-1} $, but $ v_t - \beta_i \leq \eta^{t-1}_i $ for all $ i \ne j_t $, the only sources of allocation are the global allocation $ x^G_t $ and the class-specific allocation $ x^{j_t}_t $. The dual change can be expressed as:
    \begin{align*}
        \Delta D_t = (B-M) \cdot (\lambda^t - \lambda^{t-1}) + m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t})
    \end{align*}
    By the design of $ x^G_t $ and $ x^{j_t}_t $ in Algorithm \ref{alg_LinearToZero} being the maximizers of the stated expressions, we have the following from the first-order conditions:
    \begin{align*}
        \exp\left(\tfrac{\alpha}{B-M} x^G_t\right)         & = \frac{v_t + \beta^t_{j_t}}{\lambda^{t-1} + \beta^t_{j_t}},    \\
        \exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) & = \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_{j_t} + \beta^t_{j_t}}.
    \end{align*}
    Applying the same change-of-variable and rearrangement arguments as in Case 2 to both $ x^G_t $ and $ x^{j_t}_t $, we can show that
    \begin{align*}
        \Delta P_t^G     & \geq \frac{1}{\alpha} (B-M) \cdot (\lambda^t - \lambda^{t-1}),         \\
        \Delta P_t^{j_t} & \geq \frac{1}{\alpha} m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t}),
    \end{align*}
    where $ \Delta P_t^G = (v_t + \beta^t_{j_t}) x^G_t $ and $ \Delta P_t^{j_t} = (v_t + \beta^t_{j_t}) x^{j_t}_t $. Summing these two inequalities gives
    \begin{align*}
        \Delta P_t = \Delta P_t^G + \Delta P_t^{j_t} \geq \frac{1}{\alpha} \left((B-M) \cdot (\lambda^t - \lambda^{t-1}) + m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t})\right) = \frac{1}{\alpha} \Delta D_t.
    \end{align*}

    \paragraph{Case 4.} If $ v_t \geq \eta^{t-1}_{j_t} $ and $ v_t \geq \lambda^{t-1} $, and there exists some $ i \ne j_t $ with $ v_t - \beta_i > \eta^{t-1}_i $, the sources of allocation include the global allocation $ x^G_t $, the class-specific allocation $ x^{j_t}_t $, and the cross-class allocations $ x^i_t $ for all such $ i $. Let $ I_t \subseteq [K] \setminus \{j_t\} $ denote the set of classes whose dual variables increase at time $ t $. The dual change can be expressed as:
    \begin{align*}
        \Delta D_t = (B-M) \cdot (\lambda^t - \lambda^{t-1}) + m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t}) + \sum_{i \in I_t} m_i \cdot (\eta^t_i - \eta^{t-1}_i)
    \end{align*}
    By the design of $ x^G_t $, $ x^{j_t}_t $, and $ x^i_t $ in Algorithm \ref{alg_LinearToZero} being the maximizers of the stated expressions, we have the following from the first-order conditions:
    \begin{align*}
        \exp\left(\tfrac{\alpha}{B-M} x^G_t\right)         & = \frac{v_t + \beta^t_{j_t}}{\lambda^{t-1} + \beta^t_{j_t}},                                   \\
        \exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) & = \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_{j_t} + \beta^t_{j_t}},                                \\
        \exp\left(\tfrac{\alpha}{m_i} x^i_t\right)         & = \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_i + \beta^t_{j_t} + \beta_i}, \quad \forall i \in I_t.
    \end{align*}
    similarly, we can show that
    \begin{align*}
        \Delta P_t^G     & \geq \frac{1}{\alpha} (B-M) \cdot (\lambda^t - \lambda^{t-1}),                      \\
        \Delta P_t^{j_t} & \geq \frac{1}{\alpha} m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t}),              \\
        \Delta P_t^{i}   & \geq \frac{1}{\alpha} m_i \cdot (\eta^t_i - \eta^{t-1}_i), \quad \forall i \in I_t,
    \end{align*}
    where $ \Delta P_t^G = (v_t + \beta^t_{j_t}) x^G_t $, $ \Delta P_t^{j_t} = (v_t + \beta^t_{j_t}) x^{j_t}_t $, and $ \Delta P_t^{i} = (v_t + \beta^t_{j_t}) x^{i}_t $ for all $ i \in I_t $. Summing over all active components gives
    \begin{align*}
        \Delta P_t & = \Delta P_t^G + \Delta P_t^{j_t} + \sum_{i\in I_t}\Delta P_t^{i} \\
                   & \ge \frac{1}{\alpha}\Big((B-M)(\lambda^t-\lambda^{t-1})
        + m_{j_t}(\eta^t_{j_t}-\eta^{t-1}_{j_t})
        + \sum_{i\in I_t} m_i(\eta^t_i-\eta^{t-1}_i)\Big)                              \\
                   & = \frac{1}{\alpha}\Delta D_t.
    \end{align*}

\end{proof}

\paragraph{Primal Feasibility.} We now show that the allocations produced by Algorithm~\ref{alg_LinearToZero} are feasible, i.e.,
\[
    \sum_{t=1}^T x_t \;\le\; B
\]
for every realization of the arrival sequence.
Recall that $ z_j^t $ denotes the cumulative allocation to class $ j $ up to and including time $ t $, we further define $ Z^t := \sum_{j\in[K]} z_j^t = \sum_{s=1}^t x_s $ to represent the total utilization by time $ t $. Meanwhile, the dual variables satisfy $ n^0_j = 1 $ for all classes $ j\in[K] $ and $ \lambda^0 = 1 $ at time $ t=0 $, and are updated over time as maxima of observed values. As a result, they are non-decreasing over time and in the range $[1,\theta]$.

We now define a potential function to help us prove the capacity constraint. For any group $ i \in [K] $ and time $ t\ge 0 $, define
\begin{align*}
    \Gamma_i^t := Z^t + \sum_{j\in[K]} \frac{m_j}{\alpha} \ln\Biggl(\frac{\theta + \beta_j \mathbf 1_{\{z_j^t < m_j\}}}{\eta_j^t + \beta_j \mathbf 1_{\{z_j^t < m_j\}}}\Biggr) + \frac{B-M}{\alpha} \ln\Biggl(\frac{\theta + \beta_i \mathbf 1_{\{z_i^t < m_i\}}}{\lambda^t + \beta_i \mathbf 1_{\{z_i^t < m_i\}}}\Biggr).
\end{align*}

\emph{Step 1: Base bound at $t=0$.}
At time $t=0$ we have $Z^0= z_j^0=0$ for all $j$, and $\eta_j^0=\lambda^0=1$. Hence, by the definition of $\Gamma_i^t$,
\begin{align*}
    \Gamma_i^0 = & 0 + \sum_{j\in[K]} \frac{m_j}{\alpha} \ln\Biggl(\frac{\theta + \beta_j}{1 + \beta_j}\Biggr) + \frac{B-M}{\alpha} \ln\Biggl(\frac{\theta + \beta_i}{1 + \beta_i}\Biggr)
\end{align*}
We show that by the choice of $\{A_j\}_{j\in[K]}$ and $\alpha$ in Theorem \ref{theorem-linear2zero-soft-gfq-upper-bound}, we have $\Gamma_i^0 \le B - \sum_{j\in[K]} A_j$ for all $i\in[K]$. For each "reference" class $ l \in [K] $, the problem introduce auxiliary thresholds $ \{v^*_{k,l}\}_{k\in[K]} $ such that the following constraints hold:
\begin{align*}
    A_{l} + \frac{B-M+m_{l}}{\alpha}\ln\left(\frac{v^*_{l,l}+\beta_{l}}{1+\beta_{l}}\right)=m_{l}                           \\
    A_{k} + \frac{m_{i}}{\alpha}\ln\left(\frac{v^*_{k,l}+\beta_{i}}{1+\beta_{i}}\right)=m_{i}, \quad \forall k \in[K]/\{l\} \\
    M+\frac{B-M+m_{l}}{\alpha}\ln\left(\frac{\theta}{v^*_{l,l}}\right)+\sum_{i \in[K]/\{l\}}\frac{m_{i}}{\alpha}\ln\left(\frac{\theta}{v^*_{i,l}}\right) \leq B
\end{align*}

Fix any $ i \in [K] $ and apply these constraints with reference index $ l = i $. Rearranging the equalities gives
\begin{align}
    \frac{B-M+m_i}{\alpha}\ln\Biggl(\frac{v^*_{i,i}+\beta_i}{1+\beta_i}\Biggr) & = m_i - A_i,\label{equation-reference-ai}                       \\[-0.2em]
    \frac{m_j}{\alpha}\ln\Biggl(\frac{v^*_{j,i}+\beta_j}{1+\beta_j}\Biggr)     & = m_j - A_j,\quad \forall j\neq i.\label{equation-reference-aj}
\end{align}
Summing \eqref{equation-reference-ai} and \eqref{equation-reference-aj} over all $ j $ yields
\begin{align}
    M - \sum_{j \in [K]} A_j = & \frac{B-M+m_i}{\alpha}\ln\Biggl(\frac{v^*_{i,i}+\beta_i}{1+\beta_i}\Biggr)
    + \sum_{j\neq i}\frac{m_j}{\alpha}\ln\Biggl(\frac{v^*_{j,i}+\beta_j}{1+\beta_j}\Biggr). \label{equation-reference-M-minus-A}
\end{align}

Then we have
\begin{align}
    \Gamma_i^0 - \sum_{j\in[K]} A_j
     & = \Gamma_i^0 - M + M - \sum_{j\in[K]} A_j \nonumber                                           \\
     & = \Gamma_i^0 - M - \frac{B-M+m_i}{\alpha}\ln\Biggl(\frac{v^*_{i,i}+\beta_i}{1+\beta_i}\Biggr)
    - \sum_{j\neq i}\frac{m_j}{\alpha}\ln\Biggl(\frac{v^*_{j,i}+\beta_j}{1+\beta_j}\Biggr). \label{equation-reference-Gamma-minus-A}
\end{align}

For each $ j\in[K] $, we decompose the logarithms in $\Gamma_i^0$ as follows:
\begin{align*}
    \ln\Biggl(\frac{\theta+\beta_j}{1+\beta_j}\Biggr)
     & = \ln\Biggl(\frac{\theta}{v^*_{j,i}}\Biggr)
    + \ln\Biggl(\frac{v^*_{j,i}+\beta_j}{1+\beta_j}\Biggr)
    + \ln\Biggl(\frac{\theta+\beta_j}{\theta}\cdot\frac{v^*_{j,i}}{v^*_{j,i}+\beta_j}\Biggr).
\end{align*}

Substituting these into the expression for $\Gamma_i^0$ and then into \eqref{equation-reference-Gamma-minus-A}, we have
\begin{align*}
    \Gamma_i^0 + \sum_j A_j = & M + \frac{B-M+m_i}{\alpha}\ln\Biggl(\frac{\theta}{v^*_{i,i}}\Biggr)
    + \sum_{j\neq i}\frac{m_j}{\alpha}\ln\Biggl(\frac{\theta}{v^*_{j,i}}\Biggr) + \sum_{j\in[K]} \frac{m_j}{\alpha} \ln\Biggl(\frac{\theta+\beta_j}{\theta}\cdot\frac{v^*_{j,i}}{v^*_{j,i}+\beta_j}\Biggr) \\
                              & \leq M + \frac{B-M+m_i}{\alpha}\ln\Biggl(\frac{\theta}{v^*_{i,i}}\Biggr)
    + \sum_{j\neq i}\frac{m_j}{\alpha}\ln\Biggl(\frac{\theta}{v^*_{j,i}}\Biggr)                                                                                                                            \\
                              & \leq B
\end{align*}
where the first inequality follows since each factor $ \dfrac{\theta+\beta_j}{\theta}\cdot\dfrac{v^*_{j,i}}{v^*_{j,i}+\beta_j} $ is at most $1$, so every logarithm in the last sum is non-positive, and the second inequality follows from the third feasibility inequality in the design problem.








% \medskip
% \noindent\emph{Step 2: $\Gamma_i^t$ is non-increasing in $t$.}
% Fix $i\in[K]$ and consider a time step $t\ge 1$.
% Define
% \[
%     \Delta Z_t := Z^t - Z^{t-1} = x_t,
%     \qquad
%     \Delta\Gamma_i^t := \Gamma_i^t - \Gamma_i^{t-1}.
% \]
% We claim that $\Delta\Gamma_i^t \le 0$ for all $t$, i.e.,
% $\Gamma_i^t$ is non-increasing over time.

% By Algorithm~\ref{alg_LinearToZero},
% the total allocation to the arrival at time $t$ decomposes as
% \[
%     x_t = y_{j_t} + x_t^{G} + \sum_{j\in[K]} x_t^j,
% \]
% where $y_{j_t}$ is the initial reservation (possibly zero), $x_t^G$ is the global allocation, and $x_t^j$ are the class-specific and cross-class allocations.

% The only terms in $\Gamma_i^t$ that can change at time $t$ are:
% \begin{itemize}
%     \item the utilization term $Z^t$, which increases by $x_t$;
%     \item the logarithmic term associated with class $j_t$, through the update of $\eta_{j_t}$ and of the indicator $\mathbf 1_{\{z_{j_t}^t < m_{j_t}\}}$;
%     \item the logarithmic term associated with index $i$ in the last summand, through the update of $\lambda$ and possibly of the indicator $\mathbf 1_{\{z_i^t<m_i\}}$ when $i=j_t$.
% \end{itemize}
% All other class-specific logarithmic terms remain unchanged at step~$t$.

% Using the first-order optimality conditions already derived in the “Incremental Inequality” part of the proof, we obtain for each active component:
% \begin{itemize}
%     \item If $x_t^{G} > 0$, then
%           \[
%               \exp\Bigl(\tfrac{\alpha}{B-M} x_t^{G}\Bigr)
%               = \frac{v_t + \beta_{j_t}^t}{\lambda^{t-1} + \beta_{j_t}^t},
%           \]
%           which implies
%           \[
%               x_t^{G}
%               + \frac{B-M}{\alpha}
%               \ln\Biggl(
%               \frac{\lambda^{t} + \beta_i \mathbf 1_{\{z_i^t<m_i\}}}
%               {\lambda^{t-1} + \beta_i \mathbf 1_{\{z_i^{t-1}<m_i\}}}
%               \Biggr)
%               = 0.
%           \]
%     \item If $x_t^{j_t} > 0$, then
%           \[
%               \exp\Bigl(\tfrac{\alpha}{m_{j_t}} x_t^{j_t}\Bigr)
%               = \frac{v_t + \beta_{j_t}^t}{\eta_{j_t}^{t-1} + \beta_{j_t}^t},
%           \]
%           and similarly
%           \[
%               x_t^{j_t}
%               + \frac{m_{j_t}}{\alpha}
%               \ln\Biggl(
%               \frac{\eta_{j_t}^{t} + \beta_{j_t} \mathbf 1_{\{z_{j_t}^t<m_{j_t}\}}}
%               {\eta_{j_t}^{t-1} + \beta_{j_t} \mathbf 1_{\{z_{j_t}^{t-1}<m_{j_t}\}}}
%               \Biggr)
%               = 0.
%           \]
%     \item For each cross-class term $x_t^j>0$ with $j\neq j_t$, the first-order condition
%           \[
%               \exp\Bigl(\tfrac{\alpha}{m_j} x_t^j\Bigr)
%               = \frac{v_t + \beta_{j_t}^t}{\eta_{j_t}^{t-1} + \beta_{j_t}^t + \beta_j}
%           \]
%           yields
%           \[
%               x_t^{j}
%               + \frac{m_j}{\alpha}
%               \ln\Biggl(
%               \frac{\eta_{j}^{t} + \beta_j \mathbf 1_{\{z_j^t<m_j\}}}
%               {\eta_{j}^{t-1} + \beta_j \mathbf 1_{\{z_j^{t-1}<m_j\}}}
%               \Biggr)
%               = 0,
%           \]
%           since the dual update for class $j$ is precisely
%           $\eta_j^t = \max\{\eta_j^{t-1}, v_t - \beta_j\}$.
% \end{itemize}
% Thus, each active primal increment $x_t^G$, $x_t^{j_t}$, or $x_t^j$ is exactly offset by the decrease in the corresponding logarithmic term appearing in $\Gamma_i^t$. If a particular component is inactive at time $t$ (the corresponding $x_t^\cdot = 0$), the associated log term does not increase; hence its contribution to $\Delta\Gamma_i^t$ is non-positive.

% The reservation component $y_{j_t}$ is only used once per class and is upper-bounded by $A_{j_t}$. Reserving $A_{j_t}$ units simply reduces the “free” part of the capacity from $B$ to $B-\sum_{j}A_j$ and is already accounted for in the base inequality \eqref{eq:base-B}. Therefore, putting all components together we obtain
% \[
%     \Delta\Gamma_i^t \;\le\; 0
%     \quad\text{for every }t\ge 1,\ \text{and every }i\in[K].
% \]
% Hence, for all $t$ and $i$,
% \begin{equation}
%     \Gamma_i^t \;\le\; \Gamma_i^0.
%     \label{eq:Gamma-monotone}
% \end{equation}

% \medskip
% \noindent\emph{Step 3: Concluding the capacity bound.}
% By definition \eqref{eq:Gamma-def}, all logarithmic terms in $\Gamma_i^t$ are non-negative: indeed,
% $\eta_j^t \le \theta + \beta_j$ and
% $\lambda^t \le \theta + \beta_i$ for all $t$,
% so each ratio inside a logarithm is at least 1. Thus
% \[
%     Z^t \;\le\; \Gamma_i^t
%     \quad\text{for all }t\text{ and all }i\in[K].
% \]
% Combining this with \eqref{eq:Gamma-monotone} and the base inequality \eqref{eq:base-B} yields
% \[
%     Z^t
%     \;\le\; \Gamma_i^t
%     \;\le\; \Gamma_i^0
%     \;\le\; B - \sum_{j\in[K]} A_j
%     \quad\text{for all } t.
% \]
% Adding back the (at most) $\sum_{j\in[K]}A_j$ units of initial reservations gives
% \[
%     \sum_{s=1}^t x_s = Z^t + \sum_{j\in[K]} (\text{reservation to class }j)
%     \;\le\; B
%     \quad\text{for every }t.
% \]
% Since this holds for every finite horizon $t$, we conclude that the total allocation produced by Algorithm~\ref{alg_LinearToZero} never exceeds the capacity $B$, irrespective of the arrival sequence and for any number of classes $K$.

% This proves primal feasibility.

% In this step, we need to show that the primal solutions are feasible. Specifically, for any trajectory of arrivals, the total allocation must never exceed the capacity $B$. We prove this by induction. To show that any arrival sequence always yields a feasible solution, it suffices to argue that if, at each step, the allocation is chosen so that it remains feasible under the worst-case future sequence, then the algorithm’s allocation will always be feasible. In other words, we prove that if the utilization level before time $t$, $Z^{t-1} = \sum_{j \in [K]} z^{t-1}_j$, is future-feasible, then—regardless of the valuation and class identity of the arrival at time $t$, the updated utilization $Z^{t-1} + x^t$ is also future-feasible.

%     In the base case, we have $z^0_j = 0$ for all $j \in [K]$. Additionally, based on the algorithm design, we know that regardless of the valuations of each class, there is an allocation of $A_j$ assigned for the arrivals of those classes. As a result, in this case, we have:
%     \begin{align*}
%         \sum_{j\in[K]} A_j + \frac{m_j}{\alpha}\ln\left(\frac{\theta+\beta_{j}\cdot \mathbf{1}\{z_j^{t}<m_j\}}{1+\beta_{j}\cdot \mathbf{1}\{z_j^{t}<m_j\}}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta+\beta_{i}\cdot \mathbf{1}\{z_i^{t}<m_i\}}{1+\beta_{i}\cdot \mathbf{1}\{z_i^{t}<m_i\}}\right) \leq B, \qquad \forall i \in[K].
%     \end{align*}
%     It is easy to verify that this is indeed exactly similar as the constraint \eqref{} form Theorem \ref{}.
%     Now, in the induction hypothesis, let us assume 
%     \begin{align*}
%         Z^{t-1}+ \sum_{j\in[K]}\frac{m_j}{\alpha}\ln\left(\frac{\theta+\beta_{j}\cdot \mathbf{1}\{z_j^{t-1}<m_j\}}{\eta^{t-1}_j+\beta_{j}\cdot \mathbf{1}\{z_j^{t-1}<m_j\}}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta+\beta_{i}\cdot \mathbf{1}\{z_i^{t-1}<m_i\}}{\eta^{t-1}_i+\beta_{i}\cdot \mathbf{1}\{z_i^{t-1}<m_i\}}\right) \leq B, \qquad \forall i \in[K],
%     \end{align*}
%     Then we want to show that 
%     \begin{align*}
%         Z^{t-1}+ x_t+\sum_{j\in[K]}\frac{m_j}{\alpha}\ln\left(\frac{\theta+\beta_{j}\cdot \mathbf{1}\{z_j^{t}<m_j\}}{\eta^{t}_j+\beta_{j}\cdot \mathbf{1}\{z_j^{t}<m_j\}}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta+\beta_{i}\cdot \mathbf{1}\{z_i^{t}<m_i\}}{\eta^{t}_i+\beta_{i}\cdot \mathbf{1}\{z_i^{t}<m_i\}}\right) \leq B, \qquad \forall i \in[K].
%     \end{align*}
%     Here, we star with the simple case where there are only 2 classes ($K=2$), and then we explain how our proof can be generalized for any number of classes. Without loss of generality, we can assume that the arrival at time $t$ is form the first class. Based on the status of $z^{t-1}_j\leq m_j$ for all $j\in[K]$, there are different cases which we will explore all of them. 
%     \begin{description}
%         \item[Case 1: $z^{t-1}_1 \geq m_1$ and $z^{t-1}_2 \geq m_2$:] Let us assume $\eta^{t-1}\geq v_t$. Then, in this case, based on the algorithm \ref{}, $x_t = \frac{m_1}{\alpha}\ln(\frac{v_t}{\eta_1^{t-1}})$ and $\eta^t_1 = v_t$. As a result, we can see that:
%         \begin{align*}
%              &Z^{t-1}+ x_t + \frac{m_1}{\alpha}\ln\left(\frac{\theta}{v_t}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta}{\lambda^{t}}\right) + \frac{m_2}{\alpha}\ln\left(\frac{\theta}{\eta^{t}_2}\right) \\
%              =&Z^{t-1}+ \frac{m_1}{\alpha}\ln\left(\frac{\theta} {\eta^{t-1}_1}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta}{\lambda^{t-1}}\right) + \frac{m_2}{\alpha}\ln\left(\frac{\theta}{\eta^{t-1}_2}\right)\leq B, \qquad \forall i \in[K]
%         \end{align*}
%         Here, the first equality, is based on the allocation of the algorithm and the inequality is based on the induction hypothesis. Furthermore, in the case that $\lambda^{t-1}< v_t$, it is again easy to verify that the the inequality holds. 

%         \item[Case 2: $z^{t-1}_1 < m_1$ and $z^{t-1}_2 < m_2$:] In this let us again first assume that $\lambda^{t-1} \geq v_t$. As a result, similar to the previous case, $x_t = \frac{m_1}{\alpha}\ln(\frac{v_t+\beta}{\eta^{t-1}_1+\beta})$. Therefore, we need to show that 
%         \begin{align*}
%             Z^{t-1}+ x_t &+ \frac{m_1}{\alpha}\ln\left(\frac{p_1 + \beta}{v_t+\beta}\right) + \frac{m_1}{\alpha}\ln\left(\frac{\theta}{p_1}\right) + \frac{m_2}{\alpha}\ln\left(\frac{p_2 + \beta}{\eta^{t-1}_2+\beta}\right) + \frac{m_2}{\alpha}\ln\left(\frac{\theta}{p_2}\right) \\&+  \frac{B-M}{\alpha}\ln\left(\frac{p_1 + \beta}{\lambda^{t-1}+\beta}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta}{p_1}\right) \leq B
%         \end{align*}
%         where $p_1$ and $p_2$ are such that $z^{t-1}_1 + x_t + \frac{m_1}{\alpha} \ln(\frac{p_1 + \beta}{v_t + \beta}) + \frac{B-M}{\alpha}\ln(\frac{p_1 + \beta}{\lambda^{t-1}+\beta}) = m_1$ and $z^{t-1}_2 + \frac{m_2}{\alpha} \ln(\frac{p_2 + \beta}{\eta^{t-1}_2 + \beta}) = m_2$ Based on the the allocation at this case, we can see that this is again directly followed by the induction hypothesis. In addition, we need to prove that 
%         \begin{align*}
%             Z^{t-1}+ x_t &+ \frac{m_1}{\alpha}\ln\left(\frac{q_1 + \beta}{v_t+\beta}\right) + \frac{m_1}{\alpha}\ln\left(\frac{\theta}{p_1}\right) + \frac{m_2}{\alpha}\ln\left(\frac{q_2 + \beta}{\eta^{t-1}_2+\beta}\right) + \frac{m_2}{\alpha}\ln\left(\frac{\theta}{q_2}\right) \\&+  \frac{B-M}{\alpha}\ln\left(\frac{q_2 + \beta}{\lambda^{t-1}+\beta}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta}{q_2}\right) \leq B
%         \end{align*}
%         where $q_1$ and $q_2$ are such that $z^{t-1}_1 + x_t + \frac{m_1}{\alpha} \ln(\frac{q_1 + \beta}{v_t + \beta}) = m_1$ and $z^{t-1}_2 + \frac{m_2}{\alpha} \ln(\frac{q_2 + \beta}{\eta^{t-1}_2 + \beta})  + \frac{B-M}{\alpha}\ln(\frac{q_2 + \beta}{\lambda^{t-1}+\beta})= m_2$. As a result, we have
%         \begin{align}
%             Z^{t-1} + \frac{m_1}{\alpha}\ln\left(\frac{q_1+\beta}{\eta^{t-1}_1+\beta}\right)+\frac{m_2}{\alpha}\ln\left(\frac{q_2+\beta}{\eta^{t-1}_2+\beta}\right) + \frac{B-M}{\alpha}\ln\left(\frac{q_2+\beta}{\lambda^{t-1}+\beta}\right) = M,
%         \end{align}
%         Therefor, it remains to show that and the final utilization level under Algorithm \ref{alg_new_design} 
%         \begin{align}
%             M + \frac{m_1}{\alpha}\ln\left(\frac{\theta}{q_1}\right)+\frac{m_2}{\alpha}\ln\left(\frac{\theta}{q_2}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta}{q_2}\right) \label{eq:final_utilization} \leq B
%         \end{align}
%         Based on the induction hypothesis recall that, 
%         \begin{align*}
%             &Z^{t-1} + \frac{m_1}{\alpha}\ln\left(\frac{v_1+\beta}{\eta^{t-1}_1+\beta}\right)+\frac{m_2}{\alpha}\ln\left(\frac{v_2+\beta}{\eta^{t-1}_2+\beta}\right) + \frac{B-M}{\alpha}\ln\left(\frac{v_1+\beta}{\lambda^{t-1}+\beta}\right) = M\\
%             &M + \frac{m_1}{\alpha}\ln\left(\frac{\theta}{v_1}\right)+\frac{m_2}{\alpha}\ln\left(\frac{\theta}{v_2}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta}{v_1}\right) \leq B\\
%             &Z^{t-1} + \frac{m_1}{\alpha}\ln\left(\frac{r_1+\beta}{\eta^{t-1}_1+\beta}\right)+\frac{m_2}{\alpha}\ln\left(\frac{r_2+\beta}{\eta^{t-1}_2+\beta}\right) + \frac{B-M}{\alpha}\ln\left(\frac{r_2+\beta}{\lambda^{t-1}+\beta}\right) = M\\
%             &M + \frac{m_1}{\alpha}\ln\left(\frac{\theta}{r_1}\right)+\frac{m_2}{\alpha}\ln\left(\frac{\theta}{r_2}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta}{r_2}\right) \leq B
%         \end{align*}
%         As a result, we have
%         \begin{align*}
%             &m_1\ln\left(\frac{v_1+\beta}{q_1+\beta}\right)+m_2\ln\left(\frac{v_2+\beta}{q_2+\beta}\right) + (B-M)\ln\left(\frac{v_1+\beta}{q_2+\beta}\right) = 0\\
%             &m_1\ln\left(\frac{r_1+\beta}{q_1+\beta}\right)+m_2\ln\left(\frac{r_2+\beta}{q_2+\beta}\right) + (B-M)\ln\left(\frac{r_2+\beta}{q_2+\beta}\right) = 0
%         \end{align*}
%         Therefore, if we can show that at least one of the following inequalities hold, it completes the proof.
%         \begin{align*}
%             &m_1\ln\left(\frac{v_1}{q_1}\right)+m_2\ln\left(\frac{v_2}{q_2}\right) + (B-M)\ln\left(\frac{v_1}{q_2}\right) \leq 0\\
%             &m_1\ln\left(\frac{r_1}{q_1}\right)+m_2\ln\left(\frac{r_2}{q_2}\right) + (B-M)\ln\left(\frac{r_2}{q_2}\right) \leq 0
%         \end{align*}
%         Let $F(x) = m_1\ln\left(\frac{v_1+x}{q_1+x}\right)+m_2\ln\left(\frac{v_2+x}{q_2+x}\right) + (B-M)\ln\left(\frac{v_1+x}{q_2+x}\right)$. If we can show that $F'(x) \geq 0$ for all $x\in[0,\beta]$, we can conclude that $F(0)\leq F(\beta)=0$. Therefore, 
%         \begin{align*}
%             F'(x)= m_1\frac{q_1-v_1}{(q_1+x)(v_1+x)}+m_2\frac{q_2-v_2}{(q_2+x)(v_2+x)} + (B-M)\frac{q_2-v_1}{(q_2+x)(v_1+x)}. 
%         \end{align*}
%         Since $F''(x)\leq 0$, it is only need to show that $F'(\beta)\geq 0$. Based on the well-known inequality of $1+x \geq \ln(x)$, we know that:
%         \begin{align*}
%             m_1\frac{q_1-v_1}{v_1+x}+m_2\frac{q_2-v_2}{v_2+x} + (B-M)\frac{q_2-v_1}{v_1+x}\geq 0. 
%         \end{align*}
%         Since, $v_1 \leq q_1$, if $q_1 \leq q_2$, we can conclude that $F'(\beta) \geq 0$. However, if $q_1 \geq q_2$, we again can conclude $G'(\beta)\geq 0$ where $G(x) = m_1\ln\left(\frac{r_1+x}{q_1+x}\right)+m_2\ln\left(\frac{r_2+x}{q_2+x}\right) + (B-M)\ln\left(\frac{r_2+x}{q_2+x}\right)$. As a result, at least one of $F(0)$ or $G(0)$ is less than 0 which finishes the proof.

%         \textbf{The other cases are similar to this one.}
%     \end{description}
%     Now, we argue that the case with higher number of classes is essentially similar to the 2 class case. The reason 
%     \end{description}


\subsection{Necessary Conditions}

The hard instance for linear-to-zero penalties is similar to the zero-to-linear case. We again consider a \textit{Semi-Adaptive} adversary that chooses the hard instance based on the initial reservation for each group.



\begin{definition}[Hard Instance Family for Soft Quota with Linear-to-Zero Penalty]
    Let $\Pi^K$ denote the set of all permutations of $[K]$. For any permutation $\pi \in \Pi^K$ and sufficiently small $\epsilon > 0$, we define the hard instance $I^{\text{L}\to\text{Z}}_\pi$ as a sequence of $K+1$ batches of arrivals constructed as follows:
    \begin{enumerate}
        \item \textbf{Initial batch:} One arrival from each group $j \in [K]$ with value $v = 1$, all arriving simultaneously.
        \item \textbf{Subsequent batches:} For each group $j \in [K]$ in order $\pi_1, \pi_2, \ldots, \pi_K$, a continuum of arrivals with values continuously increasing from $ 1 + \epsilon $ to $ \theta $.
    \end{enumerate}

    Formally, the instance is represented as:
    \begin{align*}
        I^{\text{L}\to\text{Z}}_\pi =  \Biggl\{
        \underbrace{(1,\pi_1), (1, \pi_2), \dots (1, \pi_K)}_{\text{Initial batch}},
         & \underbrace{(1+\epsilon, \pi_1), (1 + 2\epsilon, \pi_1), \dots, (\theta, \pi_1)}_{\text{Arrivals for group $\pi_1$}}, \dots, \\
         & \underbrace{(1+\epsilon, \pi_K), (1 + 2\epsilon, \pi_K), \dots, (\theta_K, \pi_K)}_{\text{Arrivals for group $\pi_K$}}
        \Biggr\},
    \end{align*}
    where each element $(v,j)$ represents an arrival from group $j$ with valuation $v$.
\end{definition}

The key observation is that the initial batch remains identical across all instances $I^{\text{L}\to\text{Z}}_\pi$ for any permutation $\pi \in \Pi^K$. Consequently, any optimal online algorithm must make allocation decisions that are robust against the adversarial choice of arrival order in subsequent batches.

\begin{theorem}[Necessary Conditions for Linear-to-Zero Penalty]\label{theorem-linear2zero-soft-quota-necessary}
    Let $\alpha > 1$ be a competitive ratio. If there exists an $\alpha$-competitive online algorithm for all instances $I^{\text{L}\to\text{Z}}_\pi$ with $\pi \in \Pi^K$, then there must exist utilization functions $\psi^\pi_j: [1, \theta] \to [0, b_j]$ for each group $j \in [K]$ and permutation $\pi \in \Pi^K$, where the budgets for each group $\{b_j\}_{j \in [K]}$ satisfies $\sum_{j\in[K]}b_j \leq 1$, such that the following system of inequalities holds:

    \begin{subequations}\label{equation-linear2zero-necessary-conditions}
        \begin{gather}
            \sum_{j\in [K]} \Psi^\pi_j(1) \geq \frac{1}{\alpha}\left((1-M+m_{\pi_1})v_{\pi} + \sum_{j=2}^K m_{\pi_j}\right), \label{equation-linear2zero-necessary-initial-condition}\\[0.5em]
            \Psi^\pi_{\pi_1}(v_{\pi_1})+\sum_{j=2}^K \Psi^\pi_{\pi_j}(1) \geq \frac{1}{\alpha} \left(\sum_{j=2}^K m_{\pi_j} +(1-M+m_{\pi_1})v_{\pi_1}\right), \label{equation-linear2zero-necessary-first-group-condition}\\
            \forall v_{\pi_1} \in [v_\pi, \theta], \nonumber\\[0.5em]
            \sum_{j=1}^{i-1} \Psi^\pi_{\pi_j}(\theta) + \Psi^\pi_{\pi_i}(v_{\pi_i}) +\sum_{j=i+1}^K \Psi^\pi_{\pi_j}(1) \geq \frac{1}{\alpha} \left((1-M+\sum_{j=1}^{i-1}m_{\pi_j})\theta + m_{\pi_i}\cdot v_{\pi_i}+\sum_{j=i+1}^K m_{\pi_j}\right), \label{equation-linear2zero-necessary-general-condition}\\
            \forall i\in[2,K], \forall v_{\pi_i} \in [1+\epsilon, \theta], \nonumber\\[0.5em]
            \sum_{j\in [K]}\psi^\pi_j (\theta) \leq 1, \label{equation-linear2zero-necessary-boundary-condition}
        \end{gather}
    \end{subequations}
    for all permutations $\pi \in \Pi^K$.

    \noindent where:
    \begin{itemize}
        \item $\Psi^\pi_j(v) := \psi^\pi_j(1)+\int_1^v u \, d\psi^\pi_j(u) - f_j(\psi^\pi_j(v))$ represents the net utility function for group $j$ under permutation $\pi$,
        \item $M := \sum_{j\in[K]}m_j$ denotes the total desired allocation across all groups,
        \item $f_j(\cdot)$ is the linear-to-zero penalty function for group $j$.
    \end{itemize}
\end{theorem}

\begin{proof}
    The proof follows by considering the performance of any $\alpha$-competitive algorithm against the adversarial instances.

    \paragraph{Initial Condition \eqref{equation-linear2zero-necessary-initial-condition}} After the first batch of arrivals, the online algorithm's performance can be captured by the left-hand side of \eqref{equation-linear2zero-necessary-initial-condition}. With the second batch of arrivals for group $\pi_1$, there must exist a critical value $v_\pi \in (1, \theta]$ such that:
    \begin{itemize}
        \item Up until $ v_\pi $, i.e., $ \forall v < v_\pi $, the left-hand side of \eqref{equation-linear2zero-necessary-initial-condition} strictly exceeds the right-hand side, indicating that the current allocation is sufficient to guarantee $ \alpha $ competitiveness.
        \item At $v = v_\pi$, equality holds in \eqref{equation-linear2zero-necessary-initial-condition}, marking the point where the algorithm must begin allocating to maintain competitiveness.
        \item After $ v_\pi $, the algorithm must allocate more resources to prevent the right-hand side from exceeding the left-hand side, which would then violate the $\alpha$-competitiveness requirement.
    \end{itemize}

    Meanwhile, with the initial batch and the second batch for group $ \pi_1 $ with values up to $ v_\pi $, the offline optimal is to allocate $ m_{\pi_j} $ amount of resources to each group $ \pi_j \in [K] $, and allocate the remaining portion $ 1 - M $ to class $ \pi_1 $ with value $ v_{\pi} $, leading to the optimal value as described on the right-hand side of \eqref{equation-linear2zero-necessary-initial-condition}.

    \paragraph{Incremental Condition (I) \eqref{equation-linear2zero-necessary-first-group-condition}} In the second batch of arrival for group $ \pi_1 $, as the value of arrivals increase beyond  $ v_\pi $, the online algorithm must maintain $ \alpha $-competitiveness against the offline optimal, which can be expressed as followed for any $ v_{\pi_1} \in [v_\pi, \theta] $:
    \begin{align*}
        \OPT(I^{\text{L}\to\text{Z}}_\pi) \leq (1-M+m_{\pi_1})v_{\pi_1} + \sum_{j=2}^K m_{\pi_j}.
    \end{align*}
    On the other hand, the performance of the online algorithm is
    \begin{align*}
        \ALG(I^{\text{L}\to\text{Z}}_\pi) \geq \Psi^\pi_{\pi_1}(v_{\pi_1})+\sum_{j=2}^K \Psi^\pi_{\pi_j}(1),
    \end{align*}
    which implies second necessary condition.

    \paragraph{General Incremental Condition \eqref{equation-linear2zero-necessary-general-condition}}
    We now analyze the general case where batches for groups $\pi_1, \ldots, \pi_{i-1}$ have already arrived, we are currently processing arrivals from group $\pi_i$ with maximum observed value $v_{\pi_i} \in [1+\epsilon, \theta]$, and batches for groups $\pi_{i+1}, \ldots, \pi_K$ are yet to arrive.

    The offline optimal strategy in this scenario is to construct an allocation that maximizes total welfare by:
    \begin{itemize}
        \item Allocating $m_{\pi_j}$ units to each completed group $\pi_j$ for $j \in \{1, \ldots, i-1\}$, selecting arrivals with the maximum value $\theta$ from their respective batches.
        \item Allocating $m_{\pi_i}$ units to the current group $\pi_i$, selecting the arrival with value $v_{\pi_i}$.
        \item Allocating $m_{\pi_j}$ units for each group $\pi_j$ for $j \in \{i+1, \ldots, K\}$, using the arrivals with value 1 from the initial batch.
        \item Allocating the remaining capacity $(1-M+\sum_{j=1}^{i-1}m_{\pi_j})$ to any arrival with value $ \theta $ in previous batches, regardless of their group.
    \end{itemize}
    This allocation strategy leads to the following upper bound on the offline optimal performance for any $ v_{\pi_i} \in [1+\epsilon, \theta] $:
    \begin{align*}
        \OPT(I^{\text{L}\to\text{Z}}_\pi) \leq (1-M+\sum_{j=1}^{i-1}m_{\pi_j})\theta + m_{\pi_i} v_{\pi_i}+\sum_{j=i+1}^K m_{\pi_j}.
    \end{align*}
    Meanwhile, the performance of the online algorithm is
    \begin{align*}
        \ALG(I^{\text{L}\to\text{Z}}_\pi) \geq \sum_{j=1}^{i-1} \Psi^\pi_{\pi_j}(\theta) + \Psi^\pi_{\pi_i}(v_{\pi_i}) +\sum_{j=i+1}^K \Psi^\pi_{\pi_j}(1),
    \end{align*}
    which implies the third necessary condition.

    \paragraph{Boundary condition \eqref{equation-linear2zero-necessary-boundary-condition}}
    The final condition ensures that the total allocation across all groups does not exceed the available capacity. Since $\psi^\pi_j(\theta)$ represents the maximum allocation that can be made to group $j$ under permutation $\pi$ when the highest-value arrivals are observed, the constraint $\sum_{j\in [K]}\psi^\pi_j (\theta) \leq 1$ directly enforces the overall capacity constraint.
\end{proof}

From Theorem \ref{theorem-linear2zero-soft-quota-necessary}, we can observe that the initial reservations $ \psi^\pi_j(1) $ for each group $ j $ and the arrival order $ \pi $ play a key role. To obtain the lower bound on the competitive ratio $ \alpha $, we need to solve an optimization problem that satisfies the necessary conditions in \eqref{equation-linear2zero-necessary-conditions} for all permutations $ \pi \in \Pi^K $. Below, we provide a case study for the case of two groups with equal penalty parameters $ \beta_1 = \beta_2 = \beta $.

\begin{corollary}
    [Lower Bound for Two Groups with Linear-to-Zero Penalties]\label{theorem-linear2zero-soft-quota-lower-bound-two-groups}
    For $ K = 2 $ groups with linear-to-zero penalty functions and equal penalty parameters $ \beta_1 = \beta_2 = \beta $, under the family of hard instances $ I^{\text{L}\to\text{Z}}_\pi $, no online algorithm can achieve a competitive ratio better than $ \alpha^* $, where $ \alpha^* $ is the optimal value of the following optimization problem:
    \begin{subequations}\label{equation-linear2zero-two-groups-optimization}
        \begin{align}
            \min \quad        & \alpha                                                                                                                                                        \label{equation-linear2zero-two-groups-objective}   \\[0.5em]
            \text{s.t.} \quad & \frac{1-M+m_{\pi_1}}{\alpha}\ln\left(\frac{v^*_{\pi_1} +\beta}{v_{\pi}+\beta}\right) + \psi_{\pi_1}(1) = m_{\pi_1},                                           \label{equation-linear2zero-two-groups-constraint1} \\[0.3em]
                              & \frac{m_{\pi_2}}{\alpha}\ln\left(\frac{v^*_{\pi_2} +\beta}{1+\beta}\right) + \psi_{\pi_2}(1) = m_{\pi_2},                                                     \label{equation-linear2zero-two-groups-constraint2} \\[0.3em]
                              & 1 \geq m_1 + m_2 + \frac{1-M+m_{\pi_1}}{\alpha}\ln\left(\frac{\theta}{v^*_{\pi_1}}\right)+\frac{m_{\pi_2}}{\alpha}\ln\left(\frac{\theta}{v^*_{\pi_2}}\right), \label{equation-linear2zero-two-groups-constraint3} \\[0.3em]
                              & \sum_{j\in[2]}\left[\psi_{\pi_j}(1)-\beta(m_{\pi_j} - \psi_{\pi_j}(1))^+\right] \geq \frac{1}{\alpha}\left((1-M+m_{\pi_1})v_\pi + m_{\pi_2}\right), \label{equation-linear2zero-two-groups-constraint4}
        \end{align}
    \end{subequations}  for all $\pi\in\Pi^2$.
\end{corollary}

\begin{proof}
    % First, we can rewrite the system of inequalities in Theorem \ref{theorem-linear2zero-soft-quota-necessary} for the case of two groups ($K=2$) with equal penalty parameters ($\beta_1 = \beta_2 = \beta$).

    % \paragraph{Reduction to Binding Relations} The necessary conditions in Theorem \ref{theorem-linear2zero-soft-quota-necessary} are a system of inequalities. Let $v_\pi \in (1,\theta]$ denote the first value at which allocation to $\pi_1$ becomes strictly positive. On any interval where $\psi^\pi_{\pi_i}$ is (right-)differentiable and strictly increasing, the corresponding inequality must hold with equality; otherwise a local decrease of the allocation would preserve feasibility and strictly improve $\alpha$. Thus equalities hold precisely on active intervals and at $v_\pi$ for the initial condition. We therefore retain the original inequality directions:
    % \begin{subequations}\label{equation-linear2zero-two-groups-necessary}
    %     \begin{align}
    %         \sum_{j\in [2]} \Psi^\pi_j(1)                          & \ge \frac{1}{\alpha}\left((1-M+m_{\pi_1})v_{\pi} + m_{\pi_2}\right)                                                             &  & \text{(tight at } v_{\pi}\text{)}, \label{equation-linear2zero-two-groups-initial}                      \\
    %         \Psi^\pi_{\pi_1}(v_{\pi_1})+\Psi^\pi_{\pi_2}(1)        & \ge \frac{1}{\alpha} \left(m_{\pi_2} +(1-M+m_{\pi_1})v_{\pi_1}\right), \quad \forall v_{\pi_1} \in [v_\pi, \theta]              &  & \text{(tight when } \psi^\pi_{\pi_1} \text{ increases)}, \label{equation-linear2zero-two-groups-first}  \\
    %         \Psi^\pi_{\pi_1}(\theta) + \Psi^\pi_{\pi_2}(v_{\pi_2}) & \ge \frac{1}{\alpha} \left((1-M+m_{\pi_1})\theta + m_{\pi_2} v_{\pi_2}\right), \quad \forall v_{\pi_2} \in [1+\epsilon, \theta] &  & \text{(tight when } \psi^\pi_{\pi_2} \text{ increases)}, \label{equation-linear2zero-two-groups-second} \\
    %         \sum_{j\in [2]}\psi^\pi_j (\theta)                     & \le 1                                                                                                                           &  & \text{(may be slack)}. \label{equation-linear2zero-two-groups-capacity}
    %     \end{align}
    % \end{subequations}

    % For $i\in\{1,2\}$, where $\psi^\pi_{\pi_i}$ is absolutely continuous,
    % \[
    %     \Psi^\pi_{\pi_i}(v) = \psi^\pi_{\pi_i}(1) + \int_1^v u\,d\psi^\pi_{\pi_i}(u) - f_{\pi_i}(\psi^\pi_{\pi_i}(v))
    % \]
    % yields, by differentiation,
    % \begin{align*}
    %     \frac{d}{dv}\Psi^\pi_{\pi_i}(v) & = v\frac{d\psi^\pi_{\pi_i}}{dv} - f'_{\pi_i}(\psi^\pi_{\pi_i}(v))\frac{d\psi^\pi_{\pi_i}}{dv}           \\
    %                                     & = \frac{d\psi^\pi_{\pi_i}}{dv}\left[v - f'_{\pi_i}(\psi^\pi_{\pi_i}(v))\right]                          \\
    %                                     & = \begin{cases}
    %                                             (v + \beta)\frac{d\psi^\pi_{\pi_i}}{dv}, & \text{if } \psi^\pi_{\pi_i}(v) < m_{\pi_i},    \\
    %                                             v\frac{d\psi^\pi_{\pi_i}}{dv},           & \text{if } \psi^\pi_{\pi_i}(v) \geq m_{\pi_i}.
    %                                         \end{cases}
    % \end{align*}

    % On any active interval for group $\pi_1$, differentiating \eqref{equation-linear2zero-two-groups-first} gives
    % \begin{align*}
    %     \frac{d\Psi^\pi_{\pi_1}}{dv_{\pi_1}} = \frac{1-M+m_{\pi_1}}{\alpha}.
    % \end{align*}
    % Let $v^*_{\pi_1}$ be the critical value where $\psi^\pi_{\pi_1}(v^*_{\pi_1}) = m_{\pi_1}$. For $v_{\pi_1} \in [v_\pi, v^*_{\pi_1}]$, we have:
    % \begin{align*}
    %     (v_{\pi_1} + \beta)\frac{d\psi^\pi_{\pi_1}}{dv_{\pi_1}} = \frac{1-M+m_{\pi_1}}{\alpha} \quad \Rightarrow \quad \frac{d\psi^\pi_{\pi_1}}{dv_{\pi_1}} = \frac{1-M+m_{\pi_1}}{\alpha(v_{\pi_1}+\beta)}.
    % \end{align*}
    % Integrating from $v_\pi$ to $v^*_{\pi_1}$ and using $\psi^\pi_{\pi_1}(v^*_{\pi_1}) = m_{\pi_1}$:
    % \begin{align*}
    %     m_{\pi_1} - \psi^\pi_{\pi_1}(v_\pi) = \frac{1-M+m_{\pi_1}}{\alpha}\ln\left(\frac{v^*_{\pi_1} + \beta}{v_\pi + \beta}\right).
    % \end{align*}
    % Since $\psi^\pi_{\pi_1}(v_\pi) = \psi^\pi_{\pi_1}(1)$ (no allocation before $v_\pi$), this gives \eqref{equation-linear2zero-two-groups-constraint1}.

    % An identical argument applied to \eqref{equation-linear2zero-two-groups-second} yields \eqref{equation-linear2zero-two-groups-constraint2} for $\pi_2$ on its active interval(s).

    % For $v > v^*_{\pi_i}$, we obtain
    % \begin{align*}
    %     v\frac{d\psi^\pi_{\pi_i}}{dv} = \frac{1-M+m_{\pi_1}}{\alpha} \quad \Rightarrow \quad \psi^\pi_{\pi_i}(v) = m_{\pi_i} + \frac{1-M+m_{\pi_1}}{\alpha}\ln\left(\frac{v}{v^*_{\pi_i}}\right).
    % \end{align*}

    % Substituting these expressions: (i) evaluating at $v=v^*_{\pi_i}$ gives \eqref{equation-linear2zero-two-groups-constraint1}--\eqref{equation-linear2zero-two-groups-constraint2}; (ii) inserting $\psi^\pi_{\pi_i}(\theta) = m_{\pi_i} + \frac{1-M+m_{\pi_1}}{\alpha}\ln(\theta/v^*_{\pi_i})$ in the capacity inequality yields \eqref{equation-linear2zero-two-groups-constraint3}; and (iii) using $\Psi^\pi_j(1)=\psi^\pi_j(1)-\beta(m_{\pi_j}-\psi^\pi_j(1))^+$ in the initial inequality gives \eqref{equation-linear2zero-two-groups-constraint4}.

    % Since the optimization problem must hold for both permutations $\pi \in \Pi^2$, the result follows.

\end{proof}


\subsection{Connections to Hard Quota Constraints}