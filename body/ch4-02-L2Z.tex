Consider the case where the penalty function $ f_j $ for each group $ j $ is linearly decreasing before its desired allocation amount $ m_j $ and 0 afterwards. Namely, we have:
\begin{align*}
    f_j(x) = \begin{cases}
                 f_j^L(x) = -\beta_j x + \beta_j m_j, x\in [0, m_j], \\
                 f_j^R(x) = 0, x\in [m_j, \infty),
             \end{cases}
\end{align*}


The offline optimization problem can then be expressed as follows:
\begin{subequations}\label{equation-linear2zero-soft-quota-primal}
    \begin{align}
        \max_{0 \leq x_t \leq r_t}\quad
         & \sum_{t \in [T]} v_tx_t -
        \sum_{j \in [K]}
        \beta_j
        \Bigl[\,m_j - \sum_{t=1}^{T} x_t \cdot\mathbf 1_{\{j_t = j\}}\Bigr]^{+} \\[6pt]
        \text{s.t.}\quad
         & \sum_{t=1}^{T} x_t \leq 1
    \end{align}
\end{subequations}

\paragraph{Remark} We may take an alternative view of the optimization problem by considering it as a subsidy of $ \beta_j $ for the first $ m_j $ units of allocation to group $ j $. In this case, the optimization problem can be reformulated as follows,
\begin{subequations}\label{equation-linear2zero-subsidy}
    \begin{align}
        \max_{0 \leq x_t \leq r_t}\quad
         & \sum_{t \in [T]} (v_t + \beta_{j_t}) x_t -
        \sum_{j \in [K]}
        \beta_j
        \Bigl[\sum_{t \in [T]} x_t \cdot \mathbf{1}_{\{j_t = j\}} - m_j\Bigr]^{+} \\[6pt]
        \text{s.t.}\quad
         & \sum_{t=1}^{T} x_t \leq 1
    \end{align}
\end{subequations}.


\subsection{Sufficient Conditions}

Given the penalty function, we introduce auxiliary variables $ y_j \geq 0 $ for each group $ j\in [K] $ to represent the shortfall from its desired allocation $ m_j $. Thus, we can reformulate the offline optimization problem \eqref{equation-linear2zero-soft-quota-primal} as follows:
\begin{subequations}\label{equation-linear2zero-soft-quota-alternative-primal}
    \begin{align}
        \max_{x_t \geq 0, y_j \geq 0} & \sum_{i\in[K]}\left(\sum_{t\in[T]}v_t \cdot x_t \cdot \mathbf{1}_{\{j_t = j\}} - \beta_j \cdot y_j\right) \\
        \text{s.t. }                  & y_j \geq m_j - \sum_{t\in[T]}x_t \cdot \mathbf{1}_{\{j_t = j\}}\label{linear2zero-y-constraint}           \\
                                      & \sum_{t\in[T]} x_t \leq B \label{linear2zero-budget-constraint}
    \end{align}
\end{subequations}

Introducing dual variables $ \eta_j \geq 0 $ for constraints \eqref{linear2zero-y-constraint} and $ \lambda \geq 0 $ for the capacity constraint \eqref{linear2zero-budget-constraint}, we can derive its corresponding dual problem:
\begin{subequations}
    \begin{align}
        \min~        & (B-M)\cdot\lambda + \sum_{j\in[K]}m_j \cdot \eta_j              \\
        \text{s.t. } & \eta_{j_t} \geq v_t, \quad \forall t\in [T]                     \\
                     & \lambda \geq \eta_j \geq \lambda-\beta_j, \quad \forall j\in[K]
    \end{align}
\end{subequations}

Based on the online primal dual framework, now we present an algorithm that updates the dual variables at each time step and based on the updates of the dual variables, makes the allocation decisions of the primal variables. The updates of the dual variables directly follows from the \OPT in the offline setting. From solving the offline optimization problem \eqref{equation-linear2zero-soft-quota-primal}, we observe that
\begin{align*}
    \OPT = (B-M)\cdot v^{\max} + \sum_{j\in[K]} m_j\cdot \max \{v^{\max}_j, v^{\max}-\beta_j\},
\end{align*}
where $ v^{\max}_j $ is the maximum value among all arrivals from group $ j $, and $ v^{\max} = \max_{j\in[K]} v^{\max}_j $.
Intuitively,we need to select $ \lambda $ in a way that it updates according to $ v^{\max} $, and  $\eta_{j_t} $ according to $ \max \{v^{\max}_{j_t}, v^{\max}-\beta_{j_t}\} $ at each time step $ t $. Based on this intuition, we present the algorithm Linear-To-Zero in Algorithm \ref{alg_LinearToZero}.
\begin{algorithm}
    \caption{Linear-To-Zero}
    \label{alg_LinearToZero}
    \begin{algorithmic}[1]

        \Require $B, \theta$; $\{m_j, \beta_j, A_j\}_{j\in[K]}$ and $\alpha$
        \State Initialize $\eta_j^0 = 1$, $z_j^0 = 0$ for all $j \in [K]$, and $\lambda^0 = 1$

        \While{agent $t$ arrives}
        \State Obtain the agent's value $ v_t $ and class $j_t$

        \If{$z_{j_t}^{t-1} = 0$}
        \State $y_{j_t} = A_{j_t}$
        \Else
        \State $y_{j_t} = 0$
        \EndIf

        \State $x^G_t = \argmax_a \left\{ \frac{v_t + \beta^t_{j_t}}{\lambda^{t-1} + \beta^t_{j_t}} \cdot a - \int_0^{a} \exp\left(\frac{\alpha}{m_{j_t}} u\right)du \right\}$

        \State $x^{j_t}_t = \argmax_a \left\{ \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_{j_t} + \beta^t_{j_t}} \cdot a - \int_0^{a} \exp\left(\frac{\alpha}{1-M} u\right)du \right\}$

        \For{each $i \in [K]\setminus\{j_t\}$}
        \State $x^i_t = \argmax_a \left\{ \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_{j_t} + \beta^t_{j_t} + \beta_i} \cdot a - \int_0^{a} \exp\left(\frac{\alpha}{m_i} u\right)du \right\}$
        \EndFor

        \State $x_t = y_{j_t} + x^G_t + \sum_{j\in[K]} x^j_t$ \Comment{Total allocation}

        \State Update $z_{j_t}^t = z_{j_t}^{t-1} + x_t$

        \State Update:
        \[
            \eta^t_{j_t} = \max\{\eta^{t-1}_{j_t}, v_t\}, \quad
            \lambda^t = \max\{\lambda^{t-1}, v_t\}, \quad
            \eta^t_i = \max\{\eta^{t-1}_i, v_t - \beta_i\} \ \forall i \neq j_t
        \]

        \EndWhile

    \end{algorithmic}
\end{algorithm}

In Algorithm \ref{alg_LinearToZero}, $ z_j^t $ denotes the utilization level of class $ j $ by time $ t $, and $ \beta_j^t  = \beta_j \cdot \mathbf{1}_{\{z_j^{t-1} < m_j\}} $ indicates whether the subsidy for class $ j $ is still active at time $ t $.

Depending on the value $ v_t $, there are four sources of allocation for agent $ t $:
\begin{itemize}
    \item {\bf Initial Reservation $ y_{j_t} $:} if class $ j_t $ has not yet reached its reserved allocation amount $ A_{j_t} $, allocate $ y_{j_t} = A_{j_t} $ to agent $ t $.
    \item {\bf Global Allocation $ x^G_t $:} allocate an additional amount $ x^G_t $ based on the global dual variable $ \lambda^{t-1} $ which is shared across all classes. $ x^G_t $ is only non-zero when the agent's value $ v_t $ exceeds the current global dual variable $ \lambda^{t-1} $, meaning $ v_t $ is the highest value seen so far.
    \item {\bf Class-specific Allocation $ x^{j_t}_t $:} allocate an additional amount $ x^{j_t}_t $ based on the class-specific dual variable $ \eta^{t-1}_{j_t} $. This allocation is only non-zero when the agent's value $ v_t $ exceeds the current class-specific dual variable $ \eta^{t-1}_{j_t} $.
    \item {\bf Cross-class Allocation $ x^i_t $:} For every other class $ i \in [K]\setminus\{j_t\} $, allocate an additional amount $ x^i_t $ based on the adjusted class-specific dual variable $ \eta^{t-1}_i + \beta_i $. Cross-class allocation is only non-zero when the agent's value $ v_t $ is high enough to compensate the under allocation penalty for class $ i $.
\end{itemize}


In addition to the problem parameters, Algorithm \ref{alg_LinearToZero} additionally takes as input $ \{A_j\}_{\forall j\in [K]} $ as reservations for each class and $ \alpha $. In the following theorem, we provide sufficient conditions on designing $ \{A_j\}_{\forall j\in [K]} $ and $ \alpha $ such that Algorithm \ref{alg_LinearToZero} achieves the competitive ratio of $ \alpha $.

\begin{theorem}\label{theorem-linear2zero-soft-gfq-upper-bound}
    For any $ \alpha \geq \alpha^* $, Algorithm \ref{alg_LinearToZero} is $ \alpha $-competitive and produces a feasible solution, where $ \alpha^* $ is the optimal value of the following optimization problem:
    \begin{align*}
        \min_{\{A_j\}_{\forall j \in[K]}} & \alpha                                                                                                                                                                                                                                                                                            \\
        \text{s.t. }                      & \sum_{j\in[K]}A_j \geq \frac{B}{\alpha}                                                                                                                                                                                                                                                           \\
                                          & \begin{cases}A_{j} + \frac{B-M+m_{j}}{\alpha}\ln\left(\frac{v^*_{j,j}+\beta_{j}}{1+\beta_{j}}\right)=m_{j}                           \\
                                                A_{i} + \frac{m_{i}}{\alpha}\ln\left(\frac{v^*_{i,j}+\beta_{i}}{1+\beta_{i}}\right)=m_{i}, \quad \forall i \in[K]/\{j\} \\
                                                M+\frac{B-M+m_{j}}{\alpha}\ln\left(\frac{\theta}{v^*_{j,j}}\right)+\sum_{i \in[K]/\{j\}}\frac{m_{i}}{\alpha}\ln\left(\frac{\theta}{v^*_{i,j}}\right) \leq B
                                            \end{cases} \\
                                          & \hspace{5.5cm} ,\forall j \in [K]
    \end{align*}
\end{theorem}

\begin{proof}
    The proof to Theorem \ref{theorem-linear2zero-soft-gfq-upper-bound} follows the primal-dual analysis framework. First, we need to show that the dual variables are feasible at each step. second, we need to show that at each time step $ t $, the incremental inequality holds for the change in the primal and dual objectives. Lastly, we need to show that the obtained primal solution is feasible by showing that the total allocation does not exceed the capacity $ B $, regardless of the arrival sequence.

    \paragraph{Dual Feasibility.} Based on the update rules in Algorithm \ref{alg_LinearToZero}, it is straightforward to verify that the dual variables $ \eta_j^t $ and $ \lambda^t $ satisfy the dual feasibility conditions at each time step $ t $.

    \paragraph{Incremental Inequality.} Let $ \Delta P_t= P_t - P_{t-1} $ and $ \Delta D_t = D_t - D_{t-1} $ denote the changes in the primal and dual objectives at time step $ t $, respectively. First, note that the primal change at each time step $ t $ can be expressed as the sum of the value obtained and the subsidy provided:
    \begin{align*}
        \Delta P_t = v_t\cdot x_t + \beta^t_{j_t} \cdot x_t = (v_t + \beta^t_{j_t}) \cdot x_t.
    \end{align*}
    Based on the arriving agent's value $ v_t $, we consider the following cases for the change in the dual objective:
    \paragraph{Case 1.} If $ v_t \geq \eta^{t-1}_{j_t} $ but $ v_t < \lambda^{t-1} $, the only source of allocation is the class-specific allocation $ x^{j_t}_t $. The dual change can be expressed as:
    \begin{align*}
        \Delta D_t = m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t})
    \end{align*}
    By the design of $ x^{j_t}_t $ in Algorithm \ref{alg_LinearToZero} being the maximizer of the stated expression, we have the following from the first-order condition:
    \begin{align*}
        \exp(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t) = \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_{j_t} + \beta^t_{j_t}}.
    \end{align*}
    Rearranging the terms gives
    \begin{align*}
        v_t = (\eta^{t-1}_{j_t} + \beta^t_{j_t})\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) - \beta^t_{j_t}.
    \end{align*}
    Given the update rule $ \eta^t_{j_t} = \max\{\eta^{t-1}_{j_t}, v_t\} = v_t $ in this case, we have
    \begin{align*}
        \Delta D_t & = m_{j_t}\cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t})                                                                                                     \\
                   & = m_{j_t} \cdot \left((\eta^{t-1}_{j_t} + \beta^t_{j_t})\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) - \beta^t_{j_t} - \eta^{t-1}_{j_t}\right) \\
                   & = m_{j_t} \cdot (\eta^{t-1}_{j_t} + \beta^t_{j_t})\left(\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) - 1\right),
    \end{align*}
    and similarly,
    \begin{align*}
        \Delta P_t & = (v_t + \beta^t_{j_t}) x^{j_t}_t                                                                 \\
                   & = (\eta^{t-1}_{j_t} + \beta^t_{j_t})\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) x^{j_t}_t.
    \end{align*}
    To show $ \Delta P_t \geq \Delta D_t/\alpha $, we need to verify the following inequality:
    \begin{align*}
        (\eta^{t-1}_{j_t} + \beta^t_{j_t})\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) x^{j_t}_t \geq \frac{1}{\alpha} m_{j_t} \cdot (\eta^{t-1}_{j_t} + \beta^t_{j_t})\left(\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) - 1\right).
    \end{align*}
    Given that $ \eta^{t-1}_{j_t} + \beta^t_{j_t} > 0 $, we can divide both sides by this positive factor. Furthermore, let $ y := \frac{\alpha}{m_{j_t}} x^{j_t}_t \geq 0 $, then $ x^{j_t}_t = (m_{j_t}/\alpha) y $, and the inequality to prove becomes
    \begin{align*}
        e^{y} \cdot \frac{m_{j_t}}{\alpha} y & \geq \frac{m_{j_t}}{\alpha} \bigl(e^{y} - 1\bigr) \\
        e^{y} y                              & \geq e^{y} - 1
    \end{align*}
    Define $ g(y) := e^{y} y - e^{y} + 1 = e^{y}(y-1) + 1 $. Given that $ g(0) = 0, g'(y) = e^{y}(y-1) + e^{y} = e^{y} y \geq 0 $ for all $ y \geq 0 $, $ g $ is non-decreasing on $ [0, \infty) $ and $ g(y) \geq g(0) = 0 $ for all $ y \geq 0 $, which is equivalent to $ e^{y} y \geq e^{y} - 1 $ for all $ y \geq 0 $. Therefore, we obtain $ \Delta P_t \geq \Delta D_t/\alpha $ in Case 1.


    % \paragraph{Case 2.} If $ v_t < \eta^{t-1}_{j_t} $, then the dual variable $ \eta_{j_t} $ is updated to $ v_t $ and the allocation for group $ j_t $ is $ 0 $. Thus, we have:
    % \begin{align*}
    %     \Delta D_t = m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t}) =  m_{j_t} \cdot (v_t - \eta^{t-1}_{j_t}) \le 0,
    % \end{align*}
    % and
    % \begin{align*}
    %     \Delta P_t = v_t \cdot x_t + \beta^t_{j_t} \cdot x_t = (v_t + \beta^t_{j_t}) \cdot x_t \ge 0.
    % \end{align*}
    % Therefore, the inequality $ \Delta P_t \ge \Delta D_t/\alpha $ holds.

    % \paragraph{Case 3.} If $ v_t \geq \lambda^{t-1} $, then the dual variable $ \lambda $ is updated to $ v_t $, and the allocation is made to the group with the highest value. Without loss of generality, assume that $ j_t = 1 $ is the group with the highest value. Then, we have:
    % \begin{align*}
    %     \Delta D_t = (B-M) \cdot (\lambda^t - \lambda^{t-1}) + m_1 \cdot (\eta^t_1 - \eta^{t-1}_1) \ge 0,
    % \end{align*}
    % and
    % \begin{align*}
    %     \Delta P_t = v_t \cdot x_t + \beta^t_{j_t} \cdot x_t = (v_t + \beta^t_{j_t}) \cdot x_t \ge 0.
    % \end{align*}
    % Therefore, the inequality $ \Delta P_t \ge \Delta D_t/\alpha $ holds.

    % \paragraph{Case 4.} If $ \eta^{t-1}_{j_t} \geq \lambda^{t-1} $, then the dual variable $ \eta_{j_t} $ is updated according to the class-specific update rule, and the allocation is made to the group with the highest value. The analysis for this case is similar to Case 3, and the inequality $ \Delta P_t \ge \Delta D_t/\alpha $ also holds.
\end{proof}


\subsection{Necessary Conditions}

The hard instance for linear-to-zero penalties is similar to the zero-to-linear case. We again consider a \textit{Semi-Adaptive} adversary that chooses the hard instance based on the initial reservation for each group.



\begin{definition}[Hard Instance Family for Soft Quota with Linear-to-Zero Penalty]
    Let $\Pi^K$ denote the set of all permutations of $[K]$. For any permutation $\pi \in \Pi^K$ and sufficiently small $\epsilon > 0$, we define the hard instance $I^{\text{L}\to\text{Z}}_\pi$ as a sequence of $K+1$ batches of arrivals constructed as follows:
    \begin{enumerate}
        \item \textbf{Initial batch:} One arrival from each group $j \in [K]$ with value $v = 1$, all arriving simultaneously.
        \item \textbf{Subsequent batches:} For each group $j \in [K]$ in order $\pi_1, \pi_2, \ldots, \pi_K$, a continuum of arrivals with values continuously increasing from $ 1 + \epsilon $ to $ \theta $.
    \end{enumerate}

    Formally, the instance is represented as:
    \begin{align*}
        I^{\text{L}\to\text{Z}}_\pi =  \Biggl\{
        \underbrace{(1,\pi_1), (1, \pi_2), \dots (1, \pi_K)}_{\text{Initial batch}},
         & \underbrace{(1+\epsilon, \pi_1), (1 + 2\epsilon, \pi_1), \dots, (\theta, \pi_1)}_{\text{Arrivals for group $\pi_1$}}, \dots, \\
         & \underbrace{(1+\epsilon, \pi_K), (1 + 2\epsilon, \pi_K), \dots, (\theta_K, \pi_K)}_{\text{Arrivals for group $\pi_K$}}
        \Biggr\},
    \end{align*}
    where each element $(v,j)$ represents an arrival from group $j$ with valuation $v$.
\end{definition}

The key observation is that the initial batch remains identical across all instances $I^{\text{L}\to\text{Z}}_\pi$ for any permutation $\pi \in \Pi^K$. Consequently, any optimal online algorithm must make allocation decisions that are robust against the adversarial choice of arrival order in subsequent batches.

\begin{theorem}[Necessary Conditions for Linear-to-Zero Penalty]\label{theorem-linear2zero-soft-quota-necessary}
    Let $\alpha > 1$ be a competitive ratio. If there exists an $\alpha$-competitive online algorithm for all instances $I^{\text{L}\to\text{Z}}_\pi$ with $\pi \in \Pi^K$, then there must exist utilization functions $\psi^\pi_j: [1, \theta] \to [0, b_j]$ for each group $j \in [K]$ and permutation $\pi \in \Pi^K$, where the budgets for each group $\{b_j\}_{j \in [K]}$ satisfies $\sum_{j\in[K]}b_j \leq 1$, such that the following system of inequalities holds:

    \begin{subequations}\label{equation-linear2zero-necessary-conditions}
        \begin{gather}
            \sum_{j\in [K]} \Psi^\pi_j(1) \geq \frac{1}{\alpha}\left((1-M+m_{\pi_1})v_{\pi} + \sum_{j=2}^K m_{\pi_j}\right), \label{equation-linear2zero-necessary-initial-condition}\\[0.5em]
            \Psi^\pi_{\pi_1}(v_{\pi_1})+\sum_{j=2}^K \Psi^\pi_{\pi_j}(1) \geq \frac{1}{\alpha} \left(\sum_{j=2}^K m_{\pi_j} +(1-M+m_{\pi_1})v_{\pi_1}\right), \label{equation-linear2zero-necessary-first-group-condition}\\
            \forall v_{\pi_1} \in [v_\pi, \theta], \nonumber\\[0.5em]
            \sum_{j=1}^{i-1} \Psi^\pi_{\pi_j}(\theta) + \Psi^\pi_{\pi_i}(v_{\pi_i}) +\sum_{j=i+1}^K \Psi^\pi_{\pi_j}(1) \geq \frac{1}{\alpha} \left((1-M+\sum_{j=1}^{i-1}m_{\pi_j})\theta + m_{\pi_i}\cdot v_{\pi_i}+\sum_{j=i+1}^K m_{\pi_j}\right), \label{equation-linear2zero-necessary-general-condition}\\
            \forall i\in[2,K], \forall v_{\pi_i} \in [1+\epsilon, \theta], \nonumber\\[0.5em]
            \sum_{j\in [K]}\psi^\pi_j (\theta) \leq 1, \label{equation-linear2zero-necessary-boundary-condition}
        \end{gather}
    \end{subequations}
    for all permutations $\pi \in \Pi^K$.

    \noindent where:
    \begin{itemize}
        \item $\Psi^\pi_j(v) := \psi^\pi_j(1)+\int_1^v u \, d\psi^\pi_j(u) - f_j(\psi^\pi_j(v))$ represents the net utility function for group $j$ under permutation $\pi$,
        \item $M := \sum_{j\in[K]}m_j$ denotes the total desired allocation across all groups,
        \item $f_j(\cdot)$ is the linear-to-zero penalty function for group $j$.
    \end{itemize}
\end{theorem}

\begin{proof}
    The proof follows by considering the performance of any $\alpha$-competitive algorithm against the adversarial instances.

    \paragraph{Initial Condition \eqref{equation-linear2zero-necessary-initial-condition}} After the first batch of arrivals, the online algorithm's performance can be captured by the left-hand side of \eqref{equation-linear2zero-necessary-initial-condition}. With the second batch of arrivals for group $\pi_1$, there must exist a critical value $v_\pi \in (1, \theta]$ such that:
    \begin{itemize}
        \item Up until $ v_\pi $, i.e., $ \forall v < v_\pi $, the left-hand side of \eqref{equation-linear2zero-necessary-initial-condition} strictly exceeds the right-hand side, indicating that the current allocation is sufficient to guarantee $ \alpha $ competitiveness.
        \item At $v = v_\pi$, equality holds in \eqref{equation-linear2zero-necessary-initial-condition}, marking the point where the algorithm must begin allocating to maintain competitiveness.
        \item After $ v_\pi $, the algorithm must allocate more resources to prevent the right-hand side from exceeding the left-hand side, which would then violate the $\alpha$-competitiveness requirement.
    \end{itemize}

    Meanwhile, with the initial batch and the second batch for group $ \pi_1 $ with values up to $ v_\pi $, the offline optimal is to allocate $ m_{\pi_j} $ amount of resources to each group $ \pi_j \in [K] $, and allocate the remaining portion $ 1 - M $ to class $ \pi_1 $ with value $ v_{\pi} $, leading to the optimal value as described on the right-hand side of \eqref{equation-linear2zero-necessary-initial-condition}.

    \paragraph{Incremental Condition (I) \eqref{equation-linear2zero-necessary-first-group-condition}} In the second batch of arrival for group $ \pi_1 $, as the value of arrivals increase beyond  $ v_\pi $, the online algorithm must maintain $ \alpha $-competitiveness against the offline optimal, which can be expressed as followed for any $ v_{\pi_1} \in [v_\pi, \theta] $:
    \begin{align*}
        \OPT(I^{\text{L}\to\text{Z}}_\pi) \leq (1-M+m_{\pi_1})v_{\pi_1} + \sum_{j=2}^K m_{\pi_j}.
    \end{align*}
    On the other hand, the performance of the online algorithm is
    \begin{align*}
        \ALG(I^{\text{L}\to\text{Z}}_\pi) \geq \Psi^\pi_{\pi_1}(v_{\pi_1})+\sum_{j=2}^K \Psi^\pi_{\pi_j}(1),
    \end{align*}
    which implies second necessary condition.

    \paragraph{General Incremental Condition \eqref{equation-linear2zero-necessary-general-condition}}
    We now analyze the general case where batches for groups $\pi_1, \ldots, \pi_{i-1}$ have already arrived, we are currently processing arrivals from group $\pi_i$ with maximum observed value $v_{\pi_i} \in [1+\epsilon, \theta]$, and batches for groups $\pi_{i+1}, \ldots, \pi_K$ are yet to arrive.

    The offline optimal strategy in this scenario is to construct an allocation that maximizes total welfare by:
    \begin{itemize}
        \item Allocating $m_{\pi_j}$ units to each completed group $\pi_j$ for $j \in \{1, \ldots, i-1\}$, selecting arrivals with the maximum value $\theta$ from their respective batches.
        \item Allocating $m_{\pi_i}$ units to the current group $\pi_i$, selecting the arrival with value $v_{\pi_i}$.
        \item Allocating $m_{\pi_j}$ units for each group $\pi_j$ for $j \in \{i+1, \ldots, K\}$, using the arrivals with value 1 from the initial batch.
        \item Allocating the remaining capacity $(1-M+\sum_{j=1}^{i-1}m_{\pi_j})$ to any arrival with value $ \theta $ in previous batches, regardless of their group.
    \end{itemize}
    This allocation strategy leads to the following upper bound on the offline optimal performance for any $ v_{\pi_i} \in [1+\epsilon, \theta] $:
    \begin{align*}
        \OPT(I^{\text{L}\to\text{Z}}_\pi) \leq (1-M+\sum_{j=1}^{i-1}m_{\pi_j})\theta + m_{\pi_i} v_{\pi_i}+\sum_{j=i+1}^K m_{\pi_j}.
    \end{align*}
    Meanwhile, the performance of the online algorithm is
    \begin{align*}
        \ALG(I^{\text{L}\to\text{Z}}_\pi) \geq \sum_{j=1}^{i-1} \Psi^\pi_{\pi_j}(\theta) + \Psi^\pi_{\pi_i}(v_{\pi_i}) +\sum_{j=i+1}^K \Psi^\pi_{\pi_j}(1),
    \end{align*}
    which implies the third necessary condition.

    \paragraph{Boundary condition \eqref{equation-linear2zero-necessary-boundary-condition}}
    The final condition ensures that the total allocation across all groups does not exceed the available capacity. Since $\psi^\pi_j(\theta)$ represents the maximum allocation that can be made to group $j$ under permutation $\pi$ when the highest-value arrivals are observed, the constraint $\sum_{j\in [K]}\psi^\pi_j (\theta) \leq 1$ directly enforces the overall capacity constraint.
\end{proof}

From Theorem \ref{theorem-linear2zero-soft-quota-necessary}, we can observe that the initial reservations $ \psi^\pi_j(1) $ for each group $ j $ and the arrival order $ \pi $ play a key role. To obtain the lower bound on the competitive ratio $ \alpha $, we need to solve an optimization problem that satisfies the necessary conditions in \eqref{equation-linear2zero-necessary-conditions} for all permutations $ \pi \in \Pi^K $. Below, we provide a case study for the case of two groups with equal penalty parameters $ \beta_1 = \beta_2 = \beta $.

\begin{corollary}
    [Lower Bound for Two Groups with Linear-to-Zero Penalties]\label{theorem-linear2zero-soft-quota-lower-bound-two-groups}
    For $ K = 2 $ groups with linear-to-zero penalty functions and equal penalty parameters $ \beta_1 = \beta_2 = \beta $, under the family of hard instances $ I^{\text{L}\to\text{Z}}_\pi $, no online algorithm can achieve a competitive ratio better than $ \alpha^* $, where $ \alpha^* $ is the optimal value of the following optimization problem:
    \begin{subequations}\label{equation-linear2zero-two-groups-optimization}
        \begin{align}
            \min \quad        & \alpha                                                                                                                                                        \label{equation-linear2zero-two-groups-objective}   \\[0.5em]
            \text{s.t.} \quad & \frac{1-M+m_{\pi_1}}{\alpha}\ln\left(\frac{v^*_{\pi_1} +\beta}{v_{\pi}+\beta}\right) + \psi_{\pi_1}(1) = m_{\pi_1},                                           \label{equation-linear2zero-two-groups-constraint1} \\[0.3em]
                              & \frac{m_{\pi_2}}{\alpha}\ln\left(\frac{v^*_{\pi_2} +\beta}{1+\beta}\right) + \psi_{\pi_2}(1) = m_{\pi_2},                                                     \label{equation-linear2zero-two-groups-constraint2} \\[0.3em]
                              & 1 \geq m_1 + m_2 + \frac{1-M+m_{\pi_1}}{\alpha}\ln\left(\frac{\theta}{v^*_{\pi_1}}\right)+\frac{m_{\pi_2}}{\alpha}\ln\left(\frac{\theta}{v^*_{\pi_2}}\right), \label{equation-linear2zero-two-groups-constraint3} \\[0.3em]
                              & \sum_{j\in[2]}\left[\psi_{\pi_j}(1)-\beta(m_{\pi_j} - \psi_{\pi_j}(1))^+\right] \geq \frac{1}{\alpha}\left((1-M+m_{\pi_1})v_\pi + m_{\pi_2}\right), \label{equation-linear2zero-two-groups-constraint4}
        \end{align}
    \end{subequations}  for all $\pi\in\Pi^2$.
\end{corollary}

\begin{proof}
    % First, we can rewrite the system of inequalities in Theorem \ref{theorem-linear2zero-soft-quota-necessary} for the case of two groups ($K=2$) with equal penalty parameters ($\beta_1 = \beta_2 = \beta$).

    % \paragraph{Reduction to Binding Relations} The necessary conditions in Theorem \ref{theorem-linear2zero-soft-quota-necessary} are a system of inequalities. Let $v_\pi \in (1,\theta]$ denote the first value at which allocation to $\pi_1$ becomes strictly positive. On any interval where $\psi^\pi_{\pi_i}$ is (right-)differentiable and strictly increasing, the corresponding inequality must hold with equality; otherwise a local decrease of the allocation would preserve feasibility and strictly improve $\alpha$. Thus equalities hold precisely on active intervals and at $v_\pi$ for the initial condition. We therefore retain the original inequality directions:
    % \begin{subequations}\label{equation-linear2zero-two-groups-necessary}
    %     \begin{align}
    %         \sum_{j\in [2]} \Psi^\pi_j(1)                          & \ge \frac{1}{\alpha}\left((1-M+m_{\pi_1})v_{\pi} + m_{\pi_2}\right)                                                             &  & \text{(tight at } v_{\pi}\text{)}, \label{equation-linear2zero-two-groups-initial}                      \\
    %         \Psi^\pi_{\pi_1}(v_{\pi_1})+\Psi^\pi_{\pi_2}(1)        & \ge \frac{1}{\alpha} \left(m_{\pi_2} +(1-M+m_{\pi_1})v_{\pi_1}\right), \quad \forall v_{\pi_1} \in [v_\pi, \theta]              &  & \text{(tight when } \psi^\pi_{\pi_1} \text{ increases)}, \label{equation-linear2zero-two-groups-first}  \\
    %         \Psi^\pi_{\pi_1}(\theta) + \Psi^\pi_{\pi_2}(v_{\pi_2}) & \ge \frac{1}{\alpha} \left((1-M+m_{\pi_1})\theta + m_{\pi_2} v_{\pi_2}\right), \quad \forall v_{\pi_2} \in [1+\epsilon, \theta] &  & \text{(tight when } \psi^\pi_{\pi_2} \text{ increases)}, \label{equation-linear2zero-two-groups-second} \\
    %         \sum_{j\in [2]}\psi^\pi_j (\theta)                     & \le 1                                                                                                                           &  & \text{(may be slack)}. \label{equation-linear2zero-two-groups-capacity}
    %     \end{align}
    % \end{subequations}

    % For $i\in\{1,2\}$, where $\psi^\pi_{\pi_i}$ is absolutely continuous,
    % \[
    %     \Psi^\pi_{\pi_i}(v) = \psi^\pi_{\pi_i}(1) + \int_1^v u\,d\psi^\pi_{\pi_i}(u) - f_{\pi_i}(\psi^\pi_{\pi_i}(v))
    % \]
    % yields, by differentiation,
    % \begin{align*}
    %     \frac{d}{dv}\Psi^\pi_{\pi_i}(v) & = v\frac{d\psi^\pi_{\pi_i}}{dv} - f'_{\pi_i}(\psi^\pi_{\pi_i}(v))\frac{d\psi^\pi_{\pi_i}}{dv}           \\
    %                                     & = \frac{d\psi^\pi_{\pi_i}}{dv}\left[v - f'_{\pi_i}(\psi^\pi_{\pi_i}(v))\right]                          \\
    %                                     & = \begin{cases}
    %                                             (v + \beta)\frac{d\psi^\pi_{\pi_i}}{dv}, & \text{if } \psi^\pi_{\pi_i}(v) < m_{\pi_i},    \\
    %                                             v\frac{d\psi^\pi_{\pi_i}}{dv},           & \text{if } \psi^\pi_{\pi_i}(v) \geq m_{\pi_i}.
    %                                         \end{cases}
    % \end{align*}

    % On any active interval for group $\pi_1$, differentiating \eqref{equation-linear2zero-two-groups-first} gives
    % \begin{align*}
    %     \frac{d\Psi^\pi_{\pi_1}}{dv_{\pi_1}} = \frac{1-M+m_{\pi_1}}{\alpha}.
    % \end{align*}
    % Let $v^*_{\pi_1}$ be the critical value where $\psi^\pi_{\pi_1}(v^*_{\pi_1}) = m_{\pi_1}$. For $v_{\pi_1} \in [v_\pi, v^*_{\pi_1}]$, we have:
    % \begin{align*}
    %     (v_{\pi_1} + \beta)\frac{d\psi^\pi_{\pi_1}}{dv_{\pi_1}} = \frac{1-M+m_{\pi_1}}{\alpha} \quad \Rightarrow \quad \frac{d\psi^\pi_{\pi_1}}{dv_{\pi_1}} = \frac{1-M+m_{\pi_1}}{\alpha(v_{\pi_1}+\beta)}.
    % \end{align*}
    % Integrating from $v_\pi$ to $v^*_{\pi_1}$ and using $\psi^\pi_{\pi_1}(v^*_{\pi_1}) = m_{\pi_1}$:
    % \begin{align*}
    %     m_{\pi_1} - \psi^\pi_{\pi_1}(v_\pi) = \frac{1-M+m_{\pi_1}}{\alpha}\ln\left(\frac{v^*_{\pi_1} + \beta}{v_\pi + \beta}\right).
    % \end{align*}
    % Since $\psi^\pi_{\pi_1}(v_\pi) = \psi^\pi_{\pi_1}(1)$ (no allocation before $v_\pi$), this gives \eqref{equation-linear2zero-two-groups-constraint1}.

    % An identical argument applied to \eqref{equation-linear2zero-two-groups-second} yields \eqref{equation-linear2zero-two-groups-constraint2} for $\pi_2$ on its active interval(s).

    % For $v > v^*_{\pi_i}$, we obtain
    % \begin{align*}
    %     v\frac{d\psi^\pi_{\pi_i}}{dv} = \frac{1-M+m_{\pi_1}}{\alpha} \quad \Rightarrow \quad \psi^\pi_{\pi_i}(v) = m_{\pi_i} + \frac{1-M+m_{\pi_1}}{\alpha}\ln\left(\frac{v}{v^*_{\pi_i}}\right).
    % \end{align*}

    % Substituting these expressions: (i) evaluating at $v=v^*_{\pi_i}$ gives \eqref{equation-linear2zero-two-groups-constraint1}--\eqref{equation-linear2zero-two-groups-constraint2}; (ii) inserting $\psi^\pi_{\pi_i}(\theta) = m_{\pi_i} + \frac{1-M+m_{\pi_1}}{\alpha}\ln(\theta/v^*_{\pi_i})$ in the capacity inequality yields \eqref{equation-linear2zero-two-groups-constraint3}; and (iii) using $\Psi^\pi_j(1)=\psi^\pi_j(1)-\beta(m_{\pi_j}-\psi^\pi_j(1))^+$ in the initial inequality gives \eqref{equation-linear2zero-two-groups-constraint4}.

    % Since the optimization problem must hold for both permutations $\pi \in \Pi^2$, the result follows.

\end{proof}


\subsection{Connections to Hard Quota Constraints}