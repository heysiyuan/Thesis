\chapter{Proofs of the Sufficient Conditions}

\section{Proof of Theorem \ref{theorem-linear2zero-soft-gfq-upper-bound}}

\begin{proof}
    The proof to Theorem \ref{theorem-linear2zero-soft-gfq-upper-bound} follows the primal-dual analysis framework. First, we need to show that the dual variables are feasible at each step. Second, we need to show that at each time step $ t $, the incremental inequality holds for the change in the primal and dual objectives. Lastly, we need to show that the obtained primal solution is feasible by showing that the total allocation does not exceed the capacity $ B $, regardless of the arrival sequence.

    \paragraph{Dual Feasibility.} Based on the update rules in Algorithm \ref{alg_LinearToZero}, it is straightforward to verify that the dual variables $ \eta_j^t $ and $ \lambda^t $ satisfy the dual feasibility conditions at each time step $ t $.


    \paragraph{Incremental Inequality.} Let $ \Delta P_t= P_t - P_{t-1} $ and $ \Delta D_t = D_t - D_{t-1} $ denote the changes in the primal and dual objectives at time step $ t $, respectively. First, note that the primal change at each time step $ t $ can be expressed as the sum of the value obtained and the subsidy provided:
    \begin{align*}
        \Delta P_t = v_t\cdot x_t + \beta^t_{j_t} \cdot x_t = (v_t + \beta^t_{j_t}) \cdot x_t.
    \end{align*}
    Based on the arriving agent's value $ v_t $, we consider the following cases for the change in the dual objective:
    \paragraph{Case 1.} If $ v_t < \eta^{t-1}_{j_t} $ and $ v_t < \lambda^{t-1} $, then no dual variable is updated at time $ t $, and thus $ \Delta D_t = 0 $. It is also easy to see that the primal allocation $ x_t = 0 $ in this case, leading to $ \Delta P_t = 0 $. Therefore, we have $ \Delta P_t = 0 \geq \Delta D_t/\alpha $.

    \paragraph{Case 2.} If $ v_t \geq \eta^{t-1}_{j_t} $ but $ v_t < \lambda^{t-1} $, the only source of allocation is the class-specific allocation $ x^{j_t}_t $. The dual change can be expressed as:
    \begin{align*}
        \Delta D_t = m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t})
    \end{align*}
    By the design of $ x^{j_t}_t $ in Algorithm \ref{alg_LinearToZero} being the maximizer of the stated expression, we have the following from the first-order condition:
    \begin{align*}
        \exp(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t) = \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_{j_t} + \beta^t_{j_t}}.
    \end{align*}
    Rearranging the terms gives
    \begin{align*}
        v_t = (\eta^{t-1}_{j_t} + \beta^t_{j_t})\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) - \beta^t_{j_t}.
    \end{align*}
    Given the update rule $ \eta^t_{j_t} = \max\{\eta^{t-1}_{j_t}, v_t\} = v_t $ in this case, we have
    \begin{align*}
        \Delta D_t & = m_{j_t}\cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t})                                                                                                     \\
                   & = m_{j_t} \cdot \left((\eta^{t-1}_{j_t} + \beta^t_{j_t})\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) - \beta^t_{j_t} - \eta^{t-1}_{j_t}\right) \\
                   & = m_{j_t} \cdot (\eta^{t-1}_{j_t} + \beta^t_{j_t})\left(\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) - 1\right),
    \end{align*}
    and similarly,
    \begin{align*}
        \Delta P_t & = (v_t + \beta^t_{j_t}) x^{j_t}_t                                                                 \\
                   & = (\eta^{t-1}_{j_t} + \beta^t_{j_t})\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) x^{j_t}_t.
    \end{align*}
    To show $ \Delta P_t \geq \Delta D_t/\alpha $, we need to verify the following inequality:
    \begin{align*}
        (\eta^{t-1}_{j_t} + \beta^t_{j_t})\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) x^{j_t}_t \geq \frac{1}{\alpha} m_{j_t} \cdot (\eta^{t-1}_{j_t} + \beta^t_{j_t})\left(\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) - 1\right).
    \end{align*}
    Given that $ \eta^{t-1}_{j_t} + \beta^t_{j_t} > 0 $, we can divide both sides by this positive factor. Furthermore, let $ y := \frac{\alpha}{m_{j_t}} x^{j_t}_t \geq 0 $, then $ x^{j_t}_t = (m_{j_t}/\alpha) y $, and the inequality to prove becomes
    \begin{align*}
        e^{y} \cdot \frac{m_{j_t}}{\alpha} y & \geq \frac{m_{j_t}}{\alpha} \bigl(e^{y} - 1\bigr) \\
        e^{y} y                              & \geq e^{y} - 1
    \end{align*}
    Define $ g(y) := e^{y} y - e^{y} + 1 = e^{y}(y-1) + 1 $. Given that $ g(0) = 0, g'(y) = e^{y}(y-1) + e^{y} = e^{y} y \geq 0 $ for all $ y \geq 0 $, $ g $ is non-decreasing on $ [0, \infty) $ and $ g(y) \geq g(0) = 0 $ for all $ y \geq 0 $, which is equivalent to $ e^{y} y \geq e^{y} - 1 $ for all $ y \geq 0 $. Therefore, we obtain $ \Delta P_t \geq \Delta D_t/\alpha $.

    \paragraph{Case 3.} If $ v_t \geq \eta^{t-1}_{j_t} $ and $ v_t \geq \lambda^{t-1} $, but $ v_t - \beta_i \leq \eta^{t-1}_i $ for all $ i \ne j_t $, the only sources of allocation are the global allocation $ x^G_t $ and the class-specific allocation $ x^{j_t}_t $. The dual change can be expressed as:
    \begin{align*}
        \Delta D_t = (B-M) \cdot (\lambda^t - \lambda^{t-1}) + m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t})
    \end{align*}
    By the design of $ x^G_t $ and $ x^{j_t}_t $ in Algorithm \ref{alg_LinearToZero} being the maximizers of the stated expressions, we have the following from the first-order conditions:
    \begin{align*}
        \exp\left(\tfrac{\alpha}{B-M} x^G_t\right)         & = \frac{v_t + \beta^t_{j_t}}{\lambda^{t-1} + \beta^t_{j_t}},    \\
        \exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) & = \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_{j_t} + \beta^t_{j_t}}.
    \end{align*}
    Applying the same change-of-variable and rearrangement arguments as in Case 2 to both $ x^G_t $ and $ x^{j_t}_t $, we can show that
    \begin{align*}
        \Delta P_t^G     & \geq \frac{1}{\alpha} (B-M) \cdot (\lambda^t - \lambda^{t-1}),         \\
        \Delta P_t^{j_t} & \geq \frac{1}{\alpha} m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t}),
    \end{align*}
    where $ \Delta P_t^G = (v_t + \beta^t_{j_t}) x^G_t $ and $ \Delta P_t^{j_t} = (v_t + \beta^t_{j_t}) x^{j_t}_t $. Summing these two inequalities gives
    \begin{align*}
        \Delta P_t = \Delta P_t^G + \Delta P_t^{j_t} \geq \frac{1}{\alpha} \left((B-M) \cdot (\lambda^t - \lambda^{t-1}) + m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t})\right) = \frac{1}{\alpha} \Delta D_t.
    \end{align*}

    \paragraph{Case 4.} If $ v_t \geq \eta^{t-1}_{j_t} $ and $ v_t \geq \lambda^{t-1} $, and there exists some $ i \ne j_t $ with $ v_t - \beta_i > \eta^{t-1}_i $, the sources of allocation include the global allocation $ x^G_t $, the class-specific allocation $ x^{j_t}_t $, and the cross-class allocations $ x^i_t $ for all such $ i $. Let $ I_t \subseteq [K] \setminus \{j_t\} $ denote the set of classes whose dual variables increase at time $ t $. The dual change can be expressed as:
    \begin{align*}
        \Delta D_t = (B-M) \cdot (\lambda^t - \lambda^{t-1}) + m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t}) + \sum_{i \in I_t} m_i \cdot (\eta^t_i - \eta^{t-1}_i)
    \end{align*}
    By the design of $ x^G_t $, $ x^{j_t}_t $, and $ x^i_t $ in Algorithm \ref{alg_LinearToZero} being the maximizers of the stated expressions, we have the following from the first-order conditions:
    \begin{align*}
        \exp\left(\tfrac{\alpha}{B-M} x^G_t\right)         & = \frac{v_t + \beta^t_{j_t}}{\lambda^{t-1} + \beta^t_{j_t}},                                   \\
        \exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) & = \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_{j_t} + \beta^t_{j_t}},                                \\
        \exp\left(\tfrac{\alpha}{m_i} x^i_t\right)         & = \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_i + \beta^t_{j_t} + \beta_i}, \quad \forall i \in I_t.
    \end{align*}
    similarly, we can show that
    \begin{align*}
        \Delta P_t^G     & \geq \frac{1}{\alpha} (B-M) \cdot (\lambda^t - \lambda^{t-1}),                      \\
        \Delta P_t^{j_t} & \geq \frac{1}{\alpha} m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t}),              \\
        \Delta P_t^{i}   & \geq \frac{1}{\alpha} m_i \cdot (\eta^t_i - \eta^{t-1}_i), \quad \forall i \in I_t,
    \end{align*}
    where $ \Delta P_t^G = (v_t + \beta^t_{j_t}) x^G_t $, $ \Delta P_t^{j_t} = (v_t + \beta^t_{j_t}) x^{j_t}_t $, and $ \Delta P_t^{i} = (v_t + \beta^t_{j_t}) x^{i}_t $ for all $ i \in I_t $. Summing over all active components gives
    \begin{align*}
        \Delta P_t & = \Delta P_t^G + \Delta P_t^{j_t} + \sum_{i\in I_t}\Delta P_t^{i} \\
                   & \ge \frac{1}{\alpha}\Big((B-M)(\lambda^t-\lambda^{t-1})
        + m_{j_t}(\eta^t_{j_t}-\eta^{t-1}_{j_t})
        + \sum_{i\in I_t} m_i(\eta^t_i-\eta^{t-1}_i)\Big)                              \\
                   & = \frac{1}{\alpha}\Delta D_t.
    \end{align*}

    \paragraph{Primal Feasibility.}
    We prove that the primal solution production by Algorithm \ref{alg_LinearToZero} is feasible, i.e., $ \sum_{t=1}^T x_t \le B $, for any arrival sequence. We use induction on time $ t $ and the idea of \emph{future feasibility}. An allocation prefix is future-feasible if, even under the worst-case future sequence of arrivals, the total allocation remain within the budget $ B $.

    \textbf{Intuition.} To formalize this, we define a max-potential allocation function $ \Gamma_i^t $ that represents the total amount of resources allocated up to time $ t $, plus the maximum potential future allocation allowed by the current state of the dual variable. If we can show that $ \Gamma_i^t \leq B $ for all $ t $ and all $ i \in [K] $, then it implies that the total allocation $ Z^t = \sum_{s=1}^t x_s \leq B $ for all $ t $, establishing primal feasibility.

    \textbf{Max-potential Allocation Function.} We define the max-potential allocation function for each class $ i \in [K] $ at time $ t $ as follows:
    % \[
    %     \Gamma_i^t = Z^t + \sum_{j\in[K]} \frac{m_j}{\alpha}\ln\Biggl(\frac{\theta+\beta_{j}\cdot \mathbf{1}\{z_j^{t}<m_j\}}{\eta^{t}_j+\beta_{j}\cdot \mathbf{1}\{z_j^{t}<m_j\}}\Biggr) + \frac{B-M}{\alpha}\ln\Biggl(\frac{\theta+\beta_{i}\cdot \mathbf{1}\{z_i^{t}<m_i\}}{\lambda^{t}+\beta_{i}\cdot \mathbf{1}\{z_i^{t}<m_i\}}\Biggr).
    % \]

    \[
        \Gamma_i^t(\eta_j^t, \lambda^t) = Z_t + \sum_{j\in[K]} \Delta(\eta_j^t) + \bar{\Delta}(\lambda^t),
    \]
    where $ \Delta(\eta_j^t) $ and $ \bar{\Delta}(\lambda^t) $ are defined as follows:
    \[
        \Delta(\eta_j^t) = \begin{cases}
            \frac{m_j}{\alpha} \ln\Bigl(\frac{v^*_{j,i} + \beta_j}{\eta_j^t + \beta_j}\Bigr) + \frac{m_j}{\alpha} \ln\Bigl(\frac{\theta }{v^*_{j,i} }\Bigr) & \text{if } \eta_j^t < v^*_{j,i},    \\
            \frac{m_j}{\alpha} \ln\Bigl(\frac{\theta }{\eta_j^t}\Bigr),                                                                                     & \text{if } \eta_j^t \geq v^*_{j,i},
        \end{cases}
    \]
    \[
        \bar{\Delta}(\lambda^t) = \begin{cases}
            \frac{B-M}{\alpha} \ln\Bigl(\frac{v^*_{i,i} + \beta_i}{\lambda^t + \beta_i}\Bigr) + \frac{B-M}{\alpha} \ln\Bigl(\frac{\theta }{v^*_{i,i} }\Bigr) & \text{if } \lambda^t < v^*_{i,i},    \\
            \frac{B-M}{\alpha} \ln\Bigl(\frac{\theta }{\lambda^t}\Bigr),                                                                                     & \text{if } \lambda^t \geq v^*_{i,i}.
        \end{cases}
    \]
    Intuitively, $ \Delta(\eta_j^t) $ captures the maximum potential future allocation for each class $ j $ based on its corresponding dual variable $ \eta_j^t $, while $ \bar{\Delta}(\lambda^t) $ captures the maximum potential future allocation to class $ i $ based on the global dual variable $ \lambda^t $. Altogether, $ \Gamma_i^t(\eta_j^t, \lambda^t) $ represents the maximum potential allocation, considering when the global threshold is only helping class $ i $. For notational simplicity, we will denote $ \Gamma_i^t = \Gamma_i^t(\eta_j^t, \lambda^t) $ in the following proof.

    We will show by induction that $ \Gamma_i^t \leq B $ for all $ t $ and all $ i \in [K] $.

    \textbf{Base Case $ (t = 0) $.} At $ t = 0 $, we have $ Z^0 = Z_j^0 = 0 $ for all $ j\in [K]$, and the algorithm initializes $ \eta_j^0 = 1 $ for all $ j $ and $ \lambda^0 = 1 $. Thus, for any fixed $ i \in [K] $,
    \[
        \Gamma_i^0 = \sum_{j\in[K]} \frac{m_j}{\alpha}\ln\Bigl(\tfrac{\theta+\beta_j}{1+\beta_j}\Bigr)
        + \frac{B-M}{\alpha}\ln\Bigl(\tfrac{\theta+\beta_i}{1+\beta_i}\Bigr).
    \]
    From the design of Algorithm \ref{alg_LinearToZero}, there is a reservation $ A_j $ for each class $ j\in [K] $, therefore, to show that the prefix at time $ t = 0 $ is future-feasible, we show that $ \Gamma_i^0 + \sum_{j\in[K]} A_j \leq B $ for all $ i \in [K] $.

    By the constraints in Theorem \ref{theorem-linear2zero-soft-gfq-upper-bound}, for each "reference" class $ \ell \in [K] $ there exist thresholds $ \{v^*_{k,\ell}\}_{k\in[K]} $ such that
    \begin{align*}
        A_{\ell} + \frac{B-M+m_{\ell}}{\alpha}\ln\Bigl(\tfrac{v^*_{\ell,\ell}+\beta_{\ell}}{1+\beta_{\ell}}\Bigr)                                                                   & = m_{\ell},                                      \\
        A_{k} + \frac{m_{k}}{\alpha}\ln\Bigl(\tfrac{v^*_{k,\ell}+\beta_{k}}{1+\beta_{k}}\Bigr)                                                                                      & = m_{k}, \quad \forall k\in[K]\setminus\{\ell\}, \\
        M+\frac{B-M+m_{\ell}}{\alpha}\ln\Bigl(\tfrac{\theta}{v^*_{\ell,\ell}}\Bigr)+\sum_{k \in[K]\setminus\{\ell\}}\frac{m_{k}}{\alpha}\ln\Bigl(\tfrac{\theta}{v^*_{k,\ell}}\Bigr) & \leq B.
    \end{align*}

    Fixing any $ i \in [K] $ and taking $ \ell = i $, we rearrange the first two constraints and sum over $ k $ to obtain
    \[
        M - \sum_{j\in[K]} A_j = \frac{B-M+m_i}{\alpha}\ln\Bigl(\tfrac{v^*_{i,i}+\beta_i}{1+\beta_i}\Bigr) + \sum_{j\ne i} \frac{m_j}{\alpha}\ln\Bigl(\tfrac{v^*_{j,i}+\beta_j}{1+\beta_j}\Bigr).
    \]
    Decomposing the log terms as
    \[
        \ln\Bigl(\tfrac{\theta+\beta_j}{1+\beta_j}\Bigr) = \ln\Bigl(\tfrac{\theta}{v^*_{j,i}}\Bigr) + \ln\Bigl(\tfrac{v^*_{j,i}+\beta_j}{1+\beta_j}\Bigr) + \ln\Bigl(\tfrac{\theta+\beta_j}{\theta}\cdot\tfrac{v^*_{j,i}}{v^*_{j,i}+\beta_j}\Bigr),
    \] we note that $ \frac{\theta+\beta_j}{\theta}\cdot\frac{v^*_{j,i}}{v^*_{j,i}+\beta_j} \leq 1 $, so each such logarithm is nonpositive. Substituting these decompositions into $ \Gamma_i^0 $ and combining with the last feasibility inequality in Theorem \ref{theorem-linear2zero-soft-gfq-upper-bound}, we obtain
    \[
        \Gamma_i^0 + \sum_{j\in[K]} A_j \le B.
    \].

    \textbf{Induction Step.} Assume $ \Gamma_i^{t-1} \leq B $ for all $ i \in [K] $ at time $ t-1 $. We show that $ \Gamma_i^{t} \leq B $ for all $ i $ at time $ t $, regardless of the valuation $ v_t $ and class $ j_t $ of the arrival at time $ t $. For clarity and ease of exposition, we present the induction step for $ K = 2 $ classes first, and then argue how to extend it to general $ K \geq 2 $ classes. Without loss of generality, let $ j_t = 1 $. We then consider the following cases.

    \emph{Case 1: Quotas met $ (z_1^{t-1} \geq m_1, z_2^{t-1}\geq m_2).$} Firstly, if $ v_t \leq \lambda^{t-1} $, then $ x_t = \frac{m_1}{\alpha}\ln\Bigl(\tfrac{v_t}{\eta_1^{t-1}}\Bigr) $ and $ \eta_1^t = v_t $, therefore we have
    \begin{align*}
        \Gamma_i^t & = Z^{t-1} + x_t  + \frac{m_1}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_1^t}\Bigr) + \frac{m_2}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_2^t}\Bigr) + \frac{B-M}{\alpha} \ln \Bigl(\tfrac{\theta}{\lambda^t}\Bigr)      \\
                   & = Z^{t-1} + \frac{m_1}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_1^{t-1}}\Bigr) + \frac{m_2}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_2^{t-1}}\Bigr) + \frac{B-M}{\alpha} \ln \Bigl(\tfrac{\theta}{\lambda^{t-1}}\Bigr) \\
                   & \leq B, \forall i \in [2],
    \end{align*}
    where the inequality follows from the induction hypothesis. If instead $ v_t > \lambda^{t-1} $, the allocation $ x_t $ includes additionally a global component, meaning $ x_t = \frac{m_1}{\alpha}\ln\Bigl(\tfrac{v_t}{\eta_1^{t-1}}\Bigr) + \frac{B-M}{\alpha}\ln\Bigl(\tfrac{v_t}{\lambda^{t-1}}\Bigr) $, with $ \eta_1^t = v_t $ and $ \lambda^t = v_t $. In that case,
    \begin{align*}
        \Gamma_i^t & = Z^{t-1} + x_t  + \frac{m_1}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_1^t}\Bigr) + \frac{m_2}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_2^t}\Bigr) + \frac{B-M}{\alpha} \ln \Bigl(\tfrac{\theta}{\lambda^t}\Bigr)      \\
                   & = Z^{t-1} + \frac{m_1}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_1^{t-1}}\Bigr) + \frac{m_2}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_2^{t-1}}\Bigr) + \frac{B-M}{\alpha} \ln \Bigl(\tfrac{\theta}{\lambda^{t-1}}\Bigr) \\
                   & \leq B, \forall i \in [2],
    \end{align*}
    where the inequality again follows from the induction hypothesis.

    \emph{Case 2: Quotas not met $ (z_1^{t-1} < m_1, z_2^{t-1}< m_2).$} Assuming first that $ \lambda^{t-1} \geq v_t $, we have $ x_t = \frac{m_1}{\alpha} \ln\Bigl(\tfrac{v_t+\beta_1}{\eta_1^{t-1}+\beta_1}\Bigr) $ and $ \eta_1^t = v_t, \lambda^t = \lambda^{t-1}, \eta_2^t = \eta_2^{t-1} $.

    First, we consider the extreme case where the global allocation only affects class $ 1 $ in the future. Let $ p_1,p_2 $ be such that under some future sequence,
    \begin{align*}
        z_1^{t-1} + x_t + \frac{m_1}{\alpha}\ln\Bigl(\tfrac{p_1+\beta_1}{v_t+\beta_1}\Bigr)
        + \frac{B-M}{\alpha}\ln\Bigl(\tfrac{p_1+\beta_1}{\lambda^{t-1}+\beta_1}\Bigr)              & = m_1, \\
        z_2^{t-1} + \frac{m_2}{\alpha}\ln\Bigl(\tfrac{p_2+\beta_2}{\eta_    2^{t-1}+\beta_2}\Bigr) & = m_2.
    \end{align*}

    We then have,
    \begin{align*}
        \Gamma_1^{t} = Z^{t-1}+ x_t & + \frac{m_1}{\alpha}\ln\left(\frac{p_1 + \beta}{v_t+\beta}\right) + \frac{m_1}{\alpha}\ln\left(\frac{\theta}{p_1}\right) + \frac{m_2}{\alpha}\ln\left(\frac{p_2 + \beta}{\eta^{t-1}_2+\beta}\right) + \frac{m_2}{\alpha}\ln\left(\frac{\theta}{p_2}\right) \\&+  \frac{B-M}{\alpha}\ln\left(\frac{p_1 + \beta}{\lambda^{t-1}+\beta}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta}{p_1}\right) \leq B,
    \end{align*}
    which follows from the induction hypothesis.



    Next, we need to show that $ \Gamma_2^{t} \leq B $ by showing that we have either $ \Gamma_2^{t} \leq \Gamma_1^{t-1} \leq B $ or $ \Gamma_2^{t} \leq \Gamma_2^{t-1} \leq B $.


    we consider the case where the global allocation only affects class $ 2 $ in the future. Let $ q_1,q_2 $ be such that under some future sequence,
    \begin{align*}
        z_1^{t-1} + x_t + \frac{m_1}{\alpha}\ln\Bigl(\tfrac{q_1+\beta_1}{v_t+\beta_1}\Bigr) & = m_1, \\
        z_2^{t-1} + \frac{m_2}{\alpha}\ln\Bigl(\tfrac{q_2+\beta_2}{\eta_2^{t-1}+\beta_2}\Bigr)
        + \frac{B-M}{\alpha}\ln\Bigl(\tfrac{q_2+\beta_2}{\lambda^{t-1}+\beta_2}\Bigr)       & = m_2.
    \end{align*}
    Recall that $ x_t = \frac{m_1}{\alpha} \ln\Bigl(\tfrac{v_t+\beta_1}{\eta_1^{t-1}+\beta_1}\Bigr) $, and summing the two equations above gives
    \begin{align}
        Z^{t-1} + \frac{m_1}{\alpha}\ln\left(\frac{q_1+\beta}{\eta^{t-1}_1+\beta}\right)+\frac{m_2}{\alpha}\ln\left(\frac{q_2+\beta}{\eta^{t-1}_2+\beta}\right) + \frac{B-M}{\alpha}\ln\left(\frac{q_2+\beta}{\lambda^{t-1}+\beta}\right) = M.\label{equation-l2z-sub1}
    \end{align}

    Our goal is to show the following holds:
    \begin{align}
        \Gamma_2^{t} & = Z^{t-1}+ x_t + \frac{m_1}{\alpha}\ln\left(\frac{q_1 + \beta}{v_t+\beta}\right) + \frac{m_1}{\alpha}\ln\left(\frac{\theta}{q_1}\right) + \frac{m_2}{\alpha}\ln\left(\frac{q_2 + \beta}{\eta^{t-1}_2+\beta}\right) + \frac{m_2}{\alpha}\ln\left(\frac{\theta}{q_2}\right) \\&+  \frac{B-M}{\alpha}\ln\left(\frac{q_2 + \beta}{\lambda^{t-1}+\beta}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta}{q_2}\right) \\
                     & = M + \frac{m_1}{\alpha}\ln\left(\frac{\theta}{q_1}\right)+\frac{m_2}{\alpha}\ln\left(\frac{\theta}{q_2}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta}{q_2}\right) \leq B, \label{equation-l2z-target}
    \end{align}

    We first try to show that $ \Gamma_2^{t} \leq \Gamma_1^{t-1} \leq B $. In this case, we have the following from the induction hypothesis of $ \Gamma_1^{t-1} $:
    \begin{align}
         & Z^{t-1} + \frac{m_1}{\alpha}\ln\left(\frac{v_1+\beta}{\eta^{t-1}_1+\beta}\right)+\frac{m_2}{\alpha}\ln\left(\frac{v_2+\beta}{\eta^{t-1}_2+\beta}\right) + \frac{B-M}{\alpha}\ln\left(\frac{v_1+\beta}{\lambda^{t-1}+\beta}\right) = M \label{equation-l2z-sub2} \\
         & M + \frac{m_1}{\alpha}\ln\left(\frac{\theta}{v_1}\right)+\frac{m_2}{\alpha}\ln\left(\frac{\theta}{v_2}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta}{v_1}\right) \leq B \label{equation-l2z-target-2}
    \end{align}

    Subtracting \eqref{equation-l2z-sub2} from \eqref{equation-l2z-sub1}, we have
    \begin{align*}
        m_1\ln\Bigl(\tfrac{v_1+\beta}{q_1+\beta}\Bigr)
        + m_2\ln\Bigl(\tfrac{v_2+\beta}{q_2+\beta}\Bigr)
        + (B-M)\ln\Bigl(\tfrac{v_1+\beta}{q_2+\beta}\Bigr) & = 0
    \end{align*}

    Given that $ v_1 $ corresponds to the case where the global allocation does help class $ 1 $ to satisfy its quota, and $ q_1 $ corresponds to the case where it does not, we know that $ v_1 \leq q_1 $.

    Comparing \eqref{equation-l2z-target} and \eqref{equation-l2z-target-2}, we see that to show $ \Gamma_2^{t} \leq \Gamma_1^{t-1} $, it suffices to show that
    \begin{align*}
        m_1\ln\Bigl(\tfrac{v_1}{q_1}\Bigr)
        + m_2\ln\Bigl(\tfrac{v_2}{q_2}\Bigr)
        + (B-M)\ln\Bigl(\tfrac{v_1}{q_2}\Bigr) & \le 0,
    \end{align*}


    Let $F(x) = m_1\ln\left(\frac{v_1+x}{q_1+x}\right)+m_2\ln\left(\frac{v_2+x}{q_2+x}\right) + (B-M)\ln\left(\frac{v_1+x}{q_2+x}\right)$. If we can show that $F'(x) \geq 0$ for all $x\in[0,\beta]$, we can conclude that $F(0)\leq F(\beta)=0$. Therefore,
    \begin{align*}
        F'(x)= m_1\frac{q_1-v_1}{(q_1+x)(v_1+x)}+m_2\frac{q_2-v_2}{(q_2+x)(v_2+x)} + (B-M)\frac{q_2-v_1}{(q_2+x)(v_1+x)}.
    \end{align*}
    Since $F''(x)\leq 0$, it is only need to show that $F'(\beta)\geq 0$. Based on the well-known inequality of $1+x \geq \ln(x)$, we know that:
    \begin{align*}
        m_1\frac{q_1-v_1}{v_1+x}+m_2\frac{q_2-v_2}{v_2+x} + (B-M)\frac{q_2-v_1}{v_1+x}\geq 0.
    \end{align*}
    Since, $v_1 \leq q_1$, if $q_1 \leq q_2$, we can conclude that $F'(\beta) \geq 0$.


    However, if $q_1 \geq q_2$, we shall show $ \Gamma_2^{t} \leq \Gamma_2^{t-1} \leq B $. From the induction hypothesis of $ \Gamma_2^{t-1} $, we have \eqref{equation-l2z-sub1} and
    \begin{align}
         & Z^{t-1} + \frac{m_1}{\alpha}\ln\left(\frac{r_1+\beta}{\eta^{t-1}_1+\beta}\right)+\frac{m_2}{\alpha}\ln\left(\frac{r_2+\beta}{\eta^{t-1}_2+\beta}\right) + \frac{B-M}{\alpha}\ln\left(\frac{r_2+\beta}{\lambda^{t-1}+\beta}\right) = M\label{euqation-l2z-sub3} \\
         & M + \frac{m_1}{\alpha}\ln\left(\frac{\theta}{r_1}\right)+\frac{m_2}{\alpha}\ln\left(\frac{\theta}{r_2}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta}{r_2}\right) \leq B \label{equation-l2z-target-4}
    \end{align}

    Subtracting \eqref{euqation-l2z-sub3} from \eqref{equation-l2z-sub1}, we have
    \begin{align*}
        m_1\ln\Bigl(\tfrac{r_1+\beta}{q_1+\beta}\Bigr)
        + m_2\ln\Bigl(\tfrac{r_2+\beta}{q_2+\beta}\Bigr)
        + (B-M)\ln\Bigl(\tfrac{r_2+\beta}{q_2+\beta}\Bigr) & = 0
    \end{align*}
    Similarly, we compare \eqref{equation-l2z-target-4} and \eqref{equation-l2z-target} to see that to show $ \Gamma_2^{t} \leq \Gamma_2^{t-1} $, it suffices to show that
    \begin{align*}
        m_1\ln\Bigl(\tfrac{r_1}{q_1}\Bigr)
        + m_2\ln\Bigl(\tfrac{r_2}{q_2}\Bigr)
        + (B-M)\ln\Bigl(\tfrac{r_2}{q_2}\Bigr) & \le 0,
    \end{align*},
    by defining $G(x) = m_1\ln\left(\frac{r_1+x}{q_1+x}\right)+m_2\ln\left(\frac{r_2+x}{q_2+x}\right) + (B-M)\ln\left(\frac{r_2+x}{q_2+x}\right)$, and following the same argument as before, we have $G'(\beta) \geq 0$.

    As a result, at least one of $F(0)$ or $G(0)$ is less than 0 which finishes the proof.

    \textbf{Generalizing to $ K \geq 2 $ classes.} Given $ \Gamma_i^{t-1} \leq B $ for all $ i \in [K] $ at time $ t-1 $, we need to show that $ \Gamma_i^{t} \leq B $ for all $ i $ at time $ t $. Given $ j_t = j $, it is sufficient to show that we have $ \Gamma_i^{t} \leq \Gamma_i^{t-1} $ or $ \Gamma_i^{t} \leq \Gamma_j^{t-1} $ for each $ i \in [K] $. In this case, for any fixed $ i \in [K] $, the transition values from classes $ k \in [K]\setminus\{i, j\} $ are fixed, the proof then follows similarly as the $ K = 2 $ case by considering only classes $ i $ and $ j $.

    Given that $ v_1 $ corresponds to the case where the global allocation does help class $ 1 $ to satisfy its quota, and $ q_1 $ corresponds to the case where it does not, we know that $ v_1 \leq q_1 $.


\end{proof}


\section{Proof of Theorem \ref{theorem-zero2linear-soft-gfq-upper-bound}}

\begin{proof}
    The proof follows similar structure as that of Theorem \ref{theorem-linear2zero-soft-gfq-upper-bound}.
    \paragraph{Dual Feasibility.}
    Recall the dual for the reformulation \eqref{equation-zero2linear-soft-quota-alternative-primal}:
    \begin{subequations}
        \begin{align}
            \min~        & (B-M)\cdot\bar{\lambda} + \sum_{j\in[K]}m_j \cdot \bar{\eta}_j                            \\
            \text{s.t. } & \bar{\eta}_{j_t} \geq v_t, \quad \forall t\in [T]                                         \\
                         & \bar{\lambda} + \bar{\beta}_j \geq \bar{\eta}_j \geq \bar{\lambda}, \quad \forall j\in[K]
        \end{align}
    \end{subequations}
    Based on Algorithm \ref{alg_ZeroToLinear}, $\bar\eta_j^t$ and $\bar\lambda^t$ are non-decreasing in $t$ and satisfy:
    \begin{itemize}
        \item $\bar\eta_{j_t}^t = \max\{\bar\eta_{j_t}^{t-1}, v_t\} \ge v_t$.
        \item $\bar\eta_i^t = \max\{\bar\eta_i^{t-1}, \bar\lambda^t\} \ge \bar\lambda^t$ for all $i$.
        \item If $ i \neq j_t $ and $ \bar\eta_i^t $ updates, then we have $ \bar\eta_i^t = \bar\lambda^t \leq \bar\lambda^t + \bar\beta_i $. If it does not update, then $ \bar\eta_i^t = \bar\eta_i^{t-1} \leq \bar\lambda^{t-1} + \bar\beta_i \leq \bar\lambda^t + \bar\beta_i $.
        \item For $ j_t $, if $ \bar\eta_{j_t}^t = v_t $ and $ \bar\lambda^t = v_t - \bar\beta_{j_t} $ (when $ \bar\lambda $ updates), then $ \bar\eta_{j_t}^t = v_t \leq \bar\lambda^t + \bar\beta_{j_t} $. Otherwise $ \bar\lambda $ does not increase and the previous inequality persists.
    \end{itemize}
    Therefore the dual variables remain feasible at each time $ t $.

    \paragraph{Incremental Inequality.}
    Let $\Delta P_t := P_t-P_{t-1}$ and $\Delta D_t := D_t-D_{t-1}$, and define $ \bar\beta^t_j := \bar\beta_j\cdot \mathbf{1}_{\{z_j^{t-1}>m_j\}} $ and $ \tilde{v}_t := v_t-\bar\beta^t_{j_t} $ to be the penalty at time $ t $ and the penalty-adjusted value respectively.

    Then we have $ \Delta P_t = \tilde v_t\,x_t $, and $ \Delta D_t = (B-M)(\bar\lambda^t-\bar\lambda^{t-1}) + \sum_{j\in[K]} m_j(\bar\eta_j^t-\bar\eta_j^{t-1}) $. We need to show that $ \Delta P_t \geq \Delta D_t/\alpha $ . Note that we will use the following key inequality in the analysis:  $e^{y}y\ge e^{y}-1$ for all $y\ge 0$.

    \paragraph{Case 1.} If $ v_t < \bar \eta_{j_t}^{t-1} $ and $ v_t - \bar \beta_{j_t} < \bar{\lambda}_{t-1} $, then there are no updates to the duals, leading to $ \Delta D_t = 0 $. Moreoever, we have $ x_t = 0 $ from Algorithm \ref{alg_ZeroToLinear}, hence $ \Delta P_t = 0 $, and the incremental inequality holds.

    \paragraph{Case 2.} If only $\bar\eta_{j_t}$ updates, i.e., $ v_t \ge \bar\eta_{j_t}^{t-1} $ but $ v_t-\bar\beta_{j_t} < \bar\lambda^{t-1} $, then $\bar\lambda^t=\bar\lambda^{t-1}$ and $\bar\eta_{j_t}^t=v_t$, while all other $\bar\eta_i$ remain unchanged. We have $ \Delta D_t = m_{j_t}(\bar\eta_{j_t}^t-\bar\eta_{j_t}^{t-1}) $.

    By first-order condition of Algorithm \ref{alg_ZeroToLinear}, if $ x^{j_t}_t>0 $, we have
    \[
        \exp\Bigl(\tfrac{\alpha}{m_{j_t}}x^{j_t}_t\Bigr) = \frac{\tilde v_t}{\bar\eta^{t-1}_{j_t}-\bar\beta^t_{j_t}}.
    \]
    Using $\bar\eta_{j_t}^t=v_t$ and $\tilde v_t=v_t-\bar\beta^t_{j_t}$, we can rewrite
    \[    \bar\eta_{j_t}^t-\bar\eta_{j_t}^{t-1}
        = (\bar\eta_{j_t}^{t-1}-\bar\beta^t_{j_t})\Bigl(\exp\Bigl(\tfrac{\alpha}{m_{j_t}}x^{j_t}_t\Bigr)-1\Bigr),
    \]
    leading to $ \Delta D_t = m_{j_t}(\bar\eta_{j_t}^{t-1}-\bar\beta^t_{j_t})\Bigl(\exp\Bigl(\tfrac{\alpha}{m_{j_t}}x^{j_t}_t\Bigr)-1\Bigr) $. Given the primal increment $ \Delta P_t = \tilde v_t\,x^{j_t}_t = (\bar\eta_{j_t}^{t-1}-\bar\beta^t_{j_t})\exp\Bigl(\tfrac{\alpha}{m_{j_t}}x^{j_t}_t\Bigr)\,x^{j_t}_t $, we can conclude the incremental inequality as follows:
    Defining $ y:=\tfrac{\alpha}{m_{j_t}}x^{j_t}_t\ge 0 $, we have $ x^{j_t}_t=(m_{j_t}/\alpha)y $, and the key inequality implies
    \[    \Delta P_t
        = (\bar\eta_{j_t}^{t-1}-\bar\beta^t_{j_t})e^{y}\cdot\frac{m_{j_t}}{\alpha}y
        \ge \frac{m_{j_t}}{\alpha}(\bar\eta_{j_t}^{t-1}-\bar\beta^t_{j_t})(e^{y}-1)
        = \frac{1}{\alpha}\Delta D_t.
    \]

    \paragraph{Case 3.} If $ \bar\lambda$ updates, but no other $\bar\eta_i$ is raised, i.e., $ v_t-\bar\beta_{j_t} \ge \bar\lambda^{t-1} $ but $ v_t < \bar\eta_i^{t-1} $ for all $ i\ne j_t $, then $\bar\lambda^t=v_t-\bar\beta_{j_t}$ and all $\bar\eta_i$ remain unchanged. We have $ \Delta D_t = (B-M)(\bar\lambda^t-\bar\lambda^{t-1}) + m_{j_t}(\bar\eta_{j_t}^t-\bar\eta_{j_t}^{t-1}) $. We will analyze the two components separately.

    Given the first-order condition of Algorithm \ref{alg_ZeroToLinear}, if $ x^G_t>0 $, we have
    \[
        \exp\Bigl(\tfrac{\alpha}{B-M}x^G_t\Bigr) = \frac{\tilde v_t}{\bar\lambda^{t-1}-\bar\beta^t_{j_t}+\bar\beta_{j_t}}.
    \]
    Given that $\bar\lambda^t=v_t-\bar\beta_{j_t}$ and $\tilde v_t=v_t-\bar\beta^t_{j_t}$, we can rewrite
    \begin{align*}
        \bar\lambda^t-\bar\lambda^{t-1}
         & = (v_t-\bar\beta_{j_t})-\bar\lambda^{t-1}
        = (v_t-\bar\beta^t_{j_t})-(\bar\lambda^{t-1}-\bar\beta^t_{j_t}+\bar\beta_{j_t})                                   \\
         & = (\bar\lambda^{t-1}-\bar\beta^t_{j_t}+\bar\beta_{j_t})\Bigl(\exp\Bigl(\tfrac{\alpha}{B-M}x^G_t\Bigr)-1\Bigr).
    \end{align*}
    Leading to
    \[
        (B-M)(\bar\lambda^t-\bar\lambda^{t-1})
        =(B-M)(\bar\lambda^{t-1}-\bar\beta^t_{j_t}+\bar\beta_{j_t})\Bigl(\exp\Bigl(\tfrac{\alpha}{B-M}x^G_t\Bigr)-1\Bigr).
    \]
    The primal gain from $ x^G_t $ is
    \begin{align*}
        \Delta P_t^G
         & = \tilde v_t\,x^G_t
        = (\bar\lambda^{t-1}-\bar\beta^t_{j_t}+\bar\beta_{j_t})\exp\Bigl(\tfrac{\alpha}{B-M}x^G_t\Bigr)\,x^G_t.
    \end{align*}
    Let $ y:=\tfrac{\alpha}{B-M}x^G_t\ge 0 $. Then $ x^G_t=((B-M)/\alpha)y $, and the key inequality gives
    \begin{align*}
        \Delta P_t^G
         & \ge \frac{B-M}{\alpha}(\bar\lambda^{t-1}-\bar\beta^t_{j_t}+\bar\beta_{j_t})\Bigl(\exp\Bigl(\tfrac{\alpha}{B-M}x^G_t\Bigr)-1\Bigr) \\
         & = \frac{1}{\alpha}(B-M)(\bar\lambda^t-\bar\lambda^{t-1}).
    \end{align*}
    For the class-specific component, if $ v_t < \bar\eta_{j_t}^{t-1} $, then $ x^{j_t}_t=0 $ and $\bar\eta_{j_t}$ does not contribute to $\Delta D_t$. Otherwise, if $ v_t \ge \bar\eta_{j_t}^{t-1} $, then $\bar\eta_{j_t}$ updates and the same argument as Case 2 shows
    \[\Delta P_t^{j_t}\ge \frac{1}{\alpha}m_{j_t}(\bar\eta_{j_t}^t-\bar\eta_{j_t}^{t-1}).\]
    Combining the active components yields
    \begin{align*}
        \Delta P_t \ge \frac{1}{\alpha}\Bigl((B-M)(\bar\lambda^t-\bar\lambda^{t-1}) + m_{j_t}(\bar\eta_{j_t}^t-\bar\eta_{j_t}^{t-1})\Bigr)=\frac{1}{\alpha}\Delta D_t.
    \end{align*}

    \paragraph{Case 4.} If $\bar\lambda$ updates and there are other $\bar\eta_i$ raised to the new $\bar\lambda^t$, i.e., $ v_t-\bar\beta_{j_t} \ge \bar\lambda^{t-1} $ and there exists a nonempty set $ I_t\subseteq[K]\setminus\{j_t\} $ such that $ \bar\eta_i^{t}=\bar\lambda^{t}>\bar\eta_i^{t-1} $ for all $ i\in I_t $, then we have
    \[
        \Delta D_t=(B-M)(\bar\lambda^t-\bar\lambda^{t-1}) + m_{j_t}(\bar\eta_{j_t}^t-\bar\eta_{j_t}^{t-1}) + \sum_{i\in I_t} m_i(\bar\eta_i^t-\bar\eta_i^{t-1}).
    \]
    We will focus on the additional terms from $ i\in I_t $. The class-specific and global components are handled as in previous cases. The algorithm is updating other $ \bar \eta_i $ to the common level $ \bar \lambda^t$. When $ \bar \beta^t_{j_t} = \bar \beta_{j_t} $, we have $ \tilde v_t = v_t - \bar \beta_{j_t} = \bar \lambda^t $. The first-order condition (when $ x^i_t > 0 $) gives
    \[
        \exp\Bigl(\tfrac{\alpha}{m_i}x^i_t\Bigr) = \frac{\tilde v_t}{\bar\eta_i^{t-1}-\bar\beta^t_{j_t}+\bar\beta_{j_t}}.
    \]
    Using $ \bar \eta_i^t = \bar \lambda^t $, we obtain
    \begin{align*}
        \bar\eta_i^t-\bar\eta_i^{t-1}
         & = (\bar\eta_i^{t-1}-\bar\beta^t_{j_t}+\bar\beta_{j_t})\Bigl(\exp\Bigl(\tfrac{\alpha}{m_i}x^i_t\Bigr)-1\Bigr),
    \end{align*}
    For each $ i\in I_t $, the primal increment is
    \begin{align*}
        \Delta P_t^{i}
         & = \tilde v_t\,x^i_t
        = (\bar\eta_i^{t-1}-\bar\beta^t_{j_t}+\bar\beta_{j_t})\exp\Bigl(\tfrac{\alpha}{m_i}x^i_t\Bigr)\,x^i_t.
    \end{align*}
    Let $ y:=\tfrac{\alpha}{m_i}x^i_t\ge 0 $. Then $ x^i_t=(m_i/\alpha)y $, and the key inequality gives
    \begin{align*}
        \Delta P_t^{i}
         & \ge \frac{m_i}{\alpha}(\bar\eta_i^{t-1}-\bar\beta^t_{j_t}+\bar\beta_{j_t})\Bigl(\exp\Bigl(\tfrac{\alpha}{m_i}x^i_t\Bigr)-1\Bigr) \\
         & = \frac{1}{\alpha}m_i(\bar\eta_i^t-\bar\eta_i^{t-1}).
    \end{align*}
    Together with the previous components, we have
    \begin{align*}
        \Delta P_t \ge \frac{1}{\alpha}\Bigl((B-M)(\bar\lambda^t-\bar\lambda^{t-1}) + m_{j_t}(\bar\eta_{j_t}^t-\bar\eta_{j_t}^{t-1}) + \sum_{i\in I_t} m_i(\bar\eta_i^t-\bar\eta_i^{t-1})\Bigr)=\frac{1}{\alpha}\Delta D_t.
    \end{align*}

    \paragraph{Primal Feasibility.}
\end{proof}

\section{Proof of Theorem \ref{theorem-linear2linear-soft-gfq-upper-bound}}
\begin{proof}
    The proof again follows the primal-dual analysis framework. We will show dual feasibility, an incremental inequality, and primal feasibility as before.

    \paragraph{Dual Feasibility.}
    Recall the dual \eqref{equation-linear2linear-soft-quota-dual}:
    \begin{subequations}
        \begin{align}
            \min~        & (B-M)\cdot\lambda + \sum_{j\in[K]}m_j \cdot \tilde\eta_j                            \\
            \text{s.t. } & \tilde\eta_{j_t} \geq v_t, \quad \forall t\in[T]                                    \\
                         & \lambda + \bar\beta_j \geq \tilde\eta_j \geq \lambda-\beta_j, \quad \forall j\in[K]
        \end{align}
    \end{subequations}
    Based on Algorithm \ref{alg_LinearToLinear}, $ \tilde \eta_j^t $ and $ \lambda^t $ are non-decreasing in $ t $ and satisfy:
    \begin{itemize}
        \item $\tilde\eta_{j_t}^t=\max\{\tilde\eta_{j_t}^{t-1},v_t\}\ge v_t$.
        \item $\tilde\eta_i^t = \max\{\tilde\eta_i^{t-1}, \lambda^t-\beta_i\} \ge \lambda^t-\beta_i$ for all $i$.
        \item If $ i \neq j_t $ and $ \tilde\eta_i^t $ updates, then we have $ \tilde\eta_i^t = \lambda^t + \bar\beta_i $, which implies $ \tilde\eta_i^t \leq \lambda^t + \bar\beta_i $. If it does not update, then $ \tilde\eta_i^t = \tilde\eta_i^{t-1} \leq \lambda^{t-1} + \bar\beta_i \leq \lambda^t + \bar\beta_i $.
        \item For $ j_t $, if $ \tilde\eta_{j_t}^t = v_t $ and $ \lambda^t = v_t - \bar\beta_{j_t} $ (when $ \lambda $ updates), then $ \tilde\eta_{j_t}^t = v_t \leq \lambda^t + \bar\beta_{j_t} $. Otherwise $ \lambda $ does not increase and the previous inequality persists.
    \end{itemize}
    Therefore the dual variables remain feasible at each time $ t $.

    \paragraph{Incremental Inequality.} Let Let $\Delta P_t := P_t-P_{t-1}, \Delta D_t := D_t-D_{t-1}$, and define
    \[
        \beta^t_j := \beta_j\cdot \mathbf{1}_{\{z_j^{t-1}<m_j\}},\qquad
        \bar\beta^t_j := \bar\beta_j\cdot \mathbf{1}_{\{z_j^{t-1}>m_j\}},\qquad
        s_t := \beta^t_{j_t}-\bar\beta^t_{j_t},\qquad
        \tilde v_t := v_t + s_t.
    \]
    The primal increment at time $t$ is then $ \Delta P_t = \tilde v_t\,x_t $, and the dual increment is $ \Delta D_t = (B-M)(\lambda^t-\lambda^{t-1}) + \sum_{j\in[K]} m_j(\tilde\eta_j^t-\tilde\eta_j^{t-1}) $. We will show that $ \Delta P_t \geq \Delta D_t/\alpha $. Once again, we will use the key inequality $e^{y}y\ge e^{y}-1$ for all $y\ge 0$ in the analysis.

    \paragraph{Case 1.} If there are no dual updates, i.e., $ v_t < \tilde \eta_{j_t}^{t-1} $ and $ v_t - \bar \beta_{j_t} < \lambda^{t-1} $, then $ \Delta D_t = 0 $. Moreover, from Algorithm \ref{alg_LinearToLinear}, we have $ x_t = 0 $, hence $ \Delta P_t = 0 $, and the incremental inequality holds trivially.

    \paragraph{Case 2.} If only $ \tilde \eta_{j_t} $ updates, i.e., $ v_t \ge \tilde\eta_{j_t}^{t-1} $ but $ v_t-\bar\beta_{j_t} < \lambda^{t-1} $, then $\lambda^t=\lambda^{t-1}$ and $\tilde\eta_{j_t}^t=v_t$, while all other $\tilde\eta_i$ remain unchanged. We have $ \Delta D_t = m_{j_t}(\tilde\eta_{j_t}^t-\tilde\eta_{j_t}^{t-1}) $.

    By first-order condition of Algorithm \ref{alg_LinearToLinear}, if $ x^{j_t}_t>0 $, we have
    \[
        \exp\Bigl(\tfrac{\alpha}{m_{j_t}}x^{j_t}_t\Bigr) = \frac{\tilde v_t}{\tilde\eta^{t-1}_{j_t}+s_t}.
    \]
    Using $\tilde\eta_{j_t}^t=v_t$ and $\tilde v_t=v_t+s_t$, we can rewrite
    \[    \tilde\eta_{j_t}^t-\tilde\eta_{j_t}^{t-1}
        = (\tilde\eta_{j_t}^{t-1}+s_t)\Bigl(\exp\Bigl(\tfrac{\alpha}{m_{j_t}}x^{j_t}_t\Bigr)-1\Bigr),
    \]
    leading to $ \Delta D_t = m_{j_t}(\tilde\eta_{j_t}^{t-1}+s_t)\Bigl(\exp\Bigl(\tfrac{\alpha}{m_{j_t}}x^{j_t}_t\Bigr)-1\Bigr) $. Given the primal increment $ \Delta P_t = \tilde v_t\,x^{j_t}_t = (\tilde\eta_{j_t}^{t-1}+s_t)\exp\Bigl(\tfrac{\alpha}{m_{j_t}}x^{j_t}_t\Bigr)\,x^{j_t}_t $, we can conclude the incremental inequality by defining $ y:=\tfrac{\alpha}{m_{j_t}}x^{j_t}_t\ge 0 $, we have $ x^{j_t}_t=(m_{j_t}/\alpha)y $, and the key inequality implies
    \[    \Delta P_t
        = (\tilde\eta_{j_t}^{t-1}+s_t)e^{y}\cdot\frac{m_{j_t}}{\alpha}y
        \ge \frac{m_{j_t}}{\alpha}(\tilde\eta_{j_t}^{t-1}+s_t)(e^{y}-1)
        = \frac{1}{\alpha}\Delta D_t.
    \]

    \paragraph{Case 3.} If $ \lambda $ updates, but no other $\tilde\eta_i$ is raised, i.e., $ v_t-\bar\beta_{j_t} \ge \lambda^{t-1} $ but $ v_t < \tilde\eta_i^{t-1} $ for all $ i\ne j_t $, then $\lambda^t=v_t-\bar\beta_{j_t}$ and all $\tilde\eta_i$ remain unchanged. We have $ \Delta D_t = (B-M)(\lambda^t-\lambda^{t-1}) + m_{j_t}(\tilde\eta_{j_t}^t-\tilde\eta_{j_t}^{t-1}) $. We will analyze the two components separately.

    Given the first-order condition of Algorithm \ref{alg_LinearToLinear}, if $ x^G_t>0 $, we have
    \[
        \exp\Bigl(\tfrac{\alpha}{B-M}x^G_t\Bigr) = \frac{\tilde v_t}{\lambda^{t-1}+s_t+\bar\beta_{j_t}}.
    \]
    Given that $\lambda^t=v_t-\bar\beta_{j_t}$ and $\tilde v_t=v_t+s_t$, we can rewrite
    \begin{align*}
        \lambda^t-\lambda^{t-1}
         & = (v_t-\bar\beta_{j_t})-\lambda^{t-1}
        = (v_t+s_t)-( \lambda^{t-1}+s_t+\bar\beta_{j_t})                                                \\
         & = (\lambda^{t-1}+s_t+\bar\beta_{j_t})\Bigl(\exp\Bigl(\tfrac{\alpha}{B-M}x^G_t\Bigr)-1\Bigr).
    \end{align*}
    Leading to
    \[
        (B-M)(\lambda^t-\lambda^{t-1})
        =(B-M)(\lambda^{t-1}+s_t+\bar\beta_{j_t})\Bigl(\exp\Bigl(\tfrac{\alpha}{B-M}x^G_t\Bigr)-1\Bigr).
    \]
    The primal gain from $ x^G_t $ is
    \begin{align*}
        \Delta P_t^G
         & = \tilde v_t\,x^G_t
        = (\lambda^{t-1}+s_t+\bar\beta_{j_t})\exp\Bigl(\tfrac{\alpha}{B-M}x^G_t\Bigr)\,x^G_t.
    \end{align*}
    Let $ y:=\tfrac{\alpha}{B-M}x^G_t\ge 0 $. Then $ x^G_t=((B-M)/\alpha)y $, and the key inequality gives
    \begin{align*}
        \Delta P_t^G
         & \ge \frac{B-M}{\alpha}(\lambda^{t-1}+s_t+\bar\beta_{j_t})\Bigl(\exp\Bigl(\tfrac{\alpha}{B-M}x^G_t\Bigr)-1\Bigr) \\
         & = \frac{1}{\alpha}(B-M)(\lambda^t-\lambda^{t-1}).
    \end{align*}
    For the class-specific component, if $ v_t < \tilde\eta_{j_t}^{t-1} $, then $ x^{j_t}_t=0 $ and $\tilde\eta_{j_t}$ does not contribute to $\Delta D_t$. Otherwise, if $ v_t \ge \tilde\eta_{j_t}^{t-1} $, then $\tilde\eta_{j_t}$ updates and the same argument as Case 2 shows
    \[\Delta P_t^{j_t}\ge \frac{1}{\alpha}m_{j_t}(\tilde\eta_{j_t}^t-\tilde\eta_{j_t}^{t-1}).\]
    Combining the active components yields
    \begin{align*}
        \Delta P_t \ge \frac{1}{\alpha}\Bigl((B-M)(\lambda^t-\lambda^{t-1}) + m_{j_t}(\tilde\eta_{j_t}^t-\tilde\eta_{j_t}^{t-1})\Bigr)=\frac{1}{\alpha}\Delta D_t.
    \end{align*}

    \paragraph{Case 4.} If $\lambda$ updates and there are other $\tilde\eta_i$ raised to the new $\lambda^t$, i.e., $ v_t-\bar\beta_{j_t} \ge \lambda^{t-1} $ and there exists a nonempty set $ I_t\subseteq[K]\setminus\{j_t\} $ such that $ \tilde\eta_i^{t}=\lambda^{t}>\tilde\eta_i^{t-1} $ for all $ i\in I_t $, then we have
    \[
        \Delta D_t=(B-M)(\lambda^t-\lambda^{t-1}) + m_{j_t}(\tilde\eta_{j_t}^t-\tilde\eta_{j_t}^{t-1}) + \sum_{i\in I_t} m_i(\tilde\eta_i^t-\tilde\eta_i^{t-1}).
    \]
    We will focus on the additional terms from $ i\in I_t $. The class-specific and global components are handled as in previous cases. The algorithm is updating other $ \tilde \eta_i $ to the common level $ \lambda^t$. When $ \bar \beta^t_{j_t} = \bar \beta_{j_t} $, we have $ \tilde v_t = v_t + s_t = v_t - \bar \beta_{j_t} = \lambda^t $. The first-order condition (when $ x^i_t > 0 $) gives
    \[
        \exp\Bigl(\tfrac{\alpha}{m_i}x^i_t\Bigr) = \frac{\tilde v_t}{\tilde\eta_i^{ t-1}+s_t+\bar\beta_{j_t}}.
    \]
    Using $ \tilde \eta_i^t = \lambda^t $, we obtain
    \begin{align*}
        \tilde\eta_i^t-\tilde\eta_i^{t-1}
         & = (\tilde\eta_i^{t-1}+s_t+\bar\beta_{j_t})\Bigl(\exp\Bigl(\tfrac{\alpha}{m_i}x^i_t\Bigr)-1\Bigr),
    \end{align*}
    For each $ i\in I_t $, the primal increment is
    \begin{align*}
        \Delta P_t^{i}
         & = \tilde v_t\,x^i_t
        = (\tilde\eta_i^{t-1}+s_t+\bar\beta_{j_t})\exp\Bigl(\tfrac{\alpha}{m_i}x^i_t\Bigr)\,x^i_t.
    \end{align*}
    Let $ y:=\tfrac{\alpha}{m_i}x^i_t\ge 0 $. Then $ x^i_t=(m_i/\alpha)y $, and the key inequality gives
    \begin{align*}
        \Delta P_t^{i}
         & \ge \frac{m_i}{\alpha}(\tilde\eta_i^{t-1}+s_t+\bar\beta_{j_t})\Bigl(\exp\Bigl(\tfrac{\alpha}{m_i}x^i_t\Bigr)-1\Bigr) \\
         & = \frac{1}{\alpha}m_i(\tilde\eta_i^t-\tilde\eta_i^{t-1}).
    \end{align*}
    Together with the previous components, we have
    \begin{align*}
        \Delta P_t \ge \frac{1}{\alpha}\Bigl((B-M)(\lambda^t-\lambda^{t-1}) + m_{j_t}(\tilde\eta_{j_t}^t-\tilde\eta_{j_t}^{t-1}) + \sum_{i\in I_t} m_i(\tilde\eta_i^t-\tilde\eta_i^{t-1})\Bigr)=\frac{1}{\alpha}\Delta D_t.
    \end{align*}

    \paragraph{Primal Feasibility.} We again employ the future feasibility method to show that the total allocation does not exceed $B$. The argument is similar to that in the full proof of Theorem \ref{theorem-linear2zero-soft-gfq-upper-bound}. The correspond max-potential allocation expressions in this case are defined as follows:
    For each reference class $\ell\in[K]$, define
    \[
        \Gamma_{\ell}^t := Z^t + \sum_{j\in[K]} \Delta_{j,\ell}(\tilde\eta_j^t) + \bar\Delta_{\ell}(\lambda^t),
    \]
    where
    \[
        \Delta_{j,\ell}(u) := \begin{cases}
            \frac{m_j}{\alpha}\ln\Bigl(\frac{v^*_{j,\ell}+\beta_j}{u+\beta_j}\Bigr) + \frac{m_j}{\alpha}\ln\Bigl(\frac{\theta-\bar\beta_j}{v^*_{j,\ell}-\bar\beta_j}\Bigr), & \text{if } u< v^*_{j,\ell},   \\
            \frac{m_j}{\alpha}\ln\Bigl(\frac{\theta-\bar\beta_j}{u-\bar\beta_j}\Bigr),                                                                                      & \text{if } u\ge v^*_{j,\ell},
        \end{cases}
    \]
    and
    \[
        \bar\Delta_{\ell}(u) := \begin{cases}
            \frac{B-M}{\alpha}\ln\Bigl(\frac{v^*_{\ell,\ell}+\beta_{\ell}}{u+\bar\beta_{\ell}}\Bigr) + \frac{B-M}{\alpha}\ln\Bigl(\frac{\theta-\bar\beta_{\ell}}{v^*_{\ell,\ell}-\bar\beta_{\ell}}\Bigr), & \text{if } u< v^*_{\ell,\ell},   \\
            \frac{B-M}{\alpha}\ln\Bigl(\frac{\theta-\bar\beta_{\ell}}{u}\Bigr),                                                                                                                           & \text{if } u\ge v^*_{\ell,\ell}.
        \end{cases}
    \]
    First, by the constraints in Theorem \ref{theorem-linear2linear-soft-gfq-upper-bound}, for each reference class $\ell$ there exist thresholds $\{v^*_{j,\ell}\}_{j\in[K]}$ such that the base case satisfies $\Gamma_{\ell}^0 + \sum_{j\in[K]} A_j \le B$. This is ensured by the last condition in the theorem, which bounds the total allocation including reservations and future dual-driven allocations.
    The induction step follows by analyzing the update cases of Algorithm \ref{alg_LinearToLinear}: whenever $\lambda$ or some $\tilde\eta_j$ increases, the corresponding allocation components $x_t^G$, $x_t^{j_t}$, and $x_t^i$ are chosen so that the decrease in $\bar\Delta_{\ell}$ and in the relevant $\Delta_{j,\ell}$ terms offsets the increase in $Z^t$ exactly (via the same logarithmic identities used in the induction step of Theorem \ref{theorem-linear2zero-soft-gfq-upper-bound}). Therefore, $\Gamma_{\ell}^t$ is non-increasing in $t$, and hence $\Gamma_{\ell}^t\le B$ for all $t$. In particular, $Z^t\le \Gamma_{\ell}^t\le B$ for all $t$, proving primal feasibility.
\end{proof}


\chapter{Proofs of the Necessary Conditions}

\section{Proof of Theorem \ref{theorem-linear2zero-soft-quota-necessary}}
\begin{proof}
    The proof follows by considering the performance of any $\alpha$-competitive algorithm against the family of hard instances.

    \paragraph{Initial Condition \eqref{equation-linear2zero-necessary-initial-condition}} After the first stage of arrivals, the online algorithm's performance can be captured by the left-hand side of \eqref{equation-linear2zero-necessary-initial-condition}. With the second batch of arrivals for group $ j $, there must exist a critical value $v_j \in (1, \theta]$ such that:
    \begin{itemize}
        \item Up until $ v_j $, i.e., $ \forall v < v_j $, the left-hand side of \eqref{equation-linear2zero-necessary-initial-condition} strictly exceeds the right-hand side, indicating that the current allocation is sufficient to guarantee $ \alpha $ competitiveness.
        \item At $v = v_j$, equality holds in \eqref{equation-linear2zero-necessary-initial-condition}, marking the point where the algorithm must begin allocating to maintain competitiveness.
        \item After $ v_j $, the algorithm must allocate more resources to prevent the right-hand side from exceeding the left-hand side, which would then violate the $\alpha$-competitiveness requirement.
    \end{itemize}

    Meanwhile, with the initial stage and the second stage for group $ j $ with values up to $ v_j $, the offline optimal is to allocate $ m_{j} $ amount of resources to each group $ j \in [K] $, and allocate the remaining portion $ 1 - M $ to class $ j $ with value $ v_{j} $, leading to the optimal value as described on the right-hand side of \eqref{equation-linear2zero-necessary-initial-condition}.

    \paragraph{Incremental Condition (I) \eqref{equation-linear2zero-necessary-first-group-condition}} In the second batch of arrival for group $ j $, as the value of arrivals increase beyond  $ v_j $, the online algorithm must maintain $ \alpha $-competitiveness against the offline optimal, which can be expressed as followed for any $ v \in [v_j, \theta] $:
    \begin{align*}
        \OPT(I^{\SOFTQUOTA}_j) \leq (B-M+m_{j})v_{j} + \sum_{i\in [K]\setminus\{j\}} m_{i}.
    \end{align*}
    On the other hand, the performance of the online algorithm is
    \begin{align*}
        \ALG(I^{\SOFTQUOTA}_j) \geq \Psi_j(v_{j})+\sum_{i\in [K]\setminus\{j\}} \Psi_i(1),
    \end{align*}
    which implies second necessary condition.

    \paragraph{General Incremental Condition \eqref{equation-linear2zero-necessary-general-condition}}
    We now analyze the general case where batches for groups $j, 1, \ldots, i-1$ have already arrived, we are currently processing arrivals from group $i$ with maximum observed value $ v \in [1, \theta]$, and arrivals for groups $i+1, \ldots, K$ are yet to arrive.

    The offline optimal strategy in this scenario is to construct an allocation that maximizes total welfare by:
    \begin{itemize}
        \item Allocating $m_j$ units to each completed group $j$ for $j \in \{1, \ldots, i-1\}$, selecting arrivals with the maximum value $\theta$ from their respective batches.
        \item Allocating $m_i$ units to the current group $i$, selecting the arrival with value $v_i$.
        \item Allocating $m_j$ units for each group $j$ for $j \in \{i+1, \ldots, K\}$, using the arrivals with value 1 from the initial batch.
        \item Allocating the remaining capacity $(B-M+\sum_{l=1}^{i-1}m_j)$ to any arrival with value $ \theta $ in previous batches, regardless of their group.
    \end{itemize}
    This allocation strategy leads to the following upper bound on the offline optimal performance for any $ v_i \in [1, \theta] $:
    \begin{align*}
        \OPT(I^{\SOFTQUOTA}_\pi) \leq (B-M+\sum_{l=1}^{i-1}m_j)\theta + m_i v+\sum_{j=i+1}^K m_j.
    \end{align*}
    Meanwhile, the performance of the online algorithm is
    \begin{align*}
        \ALG(I^{\SOFTQUOTA}_\pi) \geq \Psi_j (\theta) + \sum_{l=1}^{i-1} \Psi_{l}(\theta) + \Psi_{i}(v) +\sum_{l=i+1}^K \Psi_{l}(1),
    \end{align*}
    which implies the third necessary condition.

    \paragraph{Boundary condition \eqref{equation-linear2zero-necessary-boundary-condition}}
    The final condition ensures that the total allocation across all groups does not exceed the available capacity. Since $\psi_i(\theta)$ represents the maximum allocation that can be made to group $i$ when the highest-value arrivals are observed, the constraint $\sum_{i \in [K]}\psi_i (\theta) \leq B$ directly enforces the overall capacity constraint.
\end{proof}