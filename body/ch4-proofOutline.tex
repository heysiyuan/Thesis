\subsection{Proof Outline for Sufficient Conditions}

The proof of the sufficient conditions in section \ref{section-main-results-soft-quota} follows the primal-dual framework. There are three main steps in the proof: dual feasibility, incremental inequality, and primal feasibility. We will outline each step below.

\emph{Dual feasibility} requires showing that the dual variables are feasible at each time step, which can be easily verified in the update rules of the Algorithms.

\emph{Incremental inequalities} involve analyzing the incremental changes in the primal and dual objectives at each time step, due to the update of the dual variables and the allocation decision made by the algorithm. We want to show that at each time step $ t $, the change in the primal objective $ \Delta P_t $ is at least $ 1/\alpha $ times the change in the dual objective $ \Delta D_t $, i.e., $ \Delta P_t \geq \Delta D_t/\alpha $. We consider different cases based on the value of the arriving agent $ v_t $ relative to the dual variables. In each case, we derive expressions for $ \Delta P_t $ and $ \Delta D_t $, and verify the incremental inequality.

\emph{Primal feasibility} is the final step, where we need to show that the allocation decisions produced by the algorithm are feasible, i.e., the total allocation does not exceed the capacity $ B $. This is the most involved part of the proof, where we use induction on time $ t $ and the idea of \emph{future feasibility}. We define a max-potential allocation function that captures the total allocation up to time $ t $ plus the maximum potential future allocation allowed by the current state of the dual variables. By showing that this max-potential allocation function remains within the capacity $ B $ for all time steps, we can conclude that the total allocation is feasible.


As an example, we provide the full proof for Theorem \ref{theorem-linear2zero-soft-gfq-upper-bound} below.
\begin{proof}
    The proof to Theorem \ref{theorem-linear2zero-soft-gfq-upper-bound} follows the primal-dual analysis framework. First, we need to show that the dual variables are feasible at each step. second, we need to show that at each time step $ t $, the incremental inequality holds for the change in the primal and dual objectives. Lastly, we need to show that the obtained primal solution is feasible by showing that the total allocation does not exceed the capacity $ B $, regardless of the arrival sequence.

    \paragraph{Dual Feasibility.} Based on the update rules in Algorithm \ref{alg_LinearToZero}, it is straightforward to verify that the dual variables $ \eta_j^t $ and $ \lambda^t $ satisfy the dual feasibility conditions at each time step $ t $.


    \paragraph{Incremental Inequality.} Let $ \Delta P_t= P_t - P_{t-1} $ and $ \Delta D_t = D_t - D_{t-1} $ denote the changes in the primal and dual objectives at time step $ t $, respectively. First, note that the primal change at each time step $ t $ can be expressed as the sum of the value obtained and the subsidy provided:
    \begin{align*}
        \Delta P_t = v_t\cdot x_t + \beta^t_{j_t} \cdot x_t = (v_t + \beta^t_{j_t}) \cdot x_t.
    \end{align*}
    Based on the arriving agent's value $ v_t $, we consider the following cases for the change in the dual objective:
    \paragraph{Case 1.} If $ v_t < \eta^{t-1}_{j_t} $ and $ v_t < \lambda^{t-1} $, then no dual variable is updated at time $ t $, and thus $ \Delta D_t = 0 $. It is also easy to see that the primal allocation $ x_t = 0 $ in this case, leading to $ \Delta P_t = 0 $. Therefore, we have $ \Delta P_t = 0 \geq \Delta D_t/\alpha $.

    \paragraph{Case 2.} If $ v_t \geq \eta^{t-1}_{j_t} $ but $ v_t < \lambda^{t-1} $, the only source of allocation is the class-specific allocation $ x^{j_t}_t $. The dual change can be expressed as:
    \begin{align*}
        \Delta D_t = m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t})
    \end{align*}
    By the design of $ x^{j_t}_t $ in Algorithm \ref{alg_LinearToZero} being the maximizer of the stated expression, we have the following from the first-order condition:
    \begin{align*}
        \exp(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t) = \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_{j_t} + \beta^t_{j_t}}.
    \end{align*}
    Rearranging the terms gives
    \begin{align*}
        v_t = (\eta^{t-1}_{j_t} + \beta^t_{j_t})\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) - \beta^t_{j_t}.
    \end{align*}
    Given the update rule $ \eta^t_{j_t} = \max\{\eta^{t-1}_{j_t}, v_t\} = v_t $ in this case, we have
    \begin{align*}
        \Delta D_t & = m_{j_t}\cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t})                                                                                                     \\
                   & = m_{j_t} \cdot \left((\eta^{t-1}_{j_t} + \beta^t_{j_t})\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) - \beta^t_{j_t} - \eta^{t-1}_{j_t}\right) \\
                   & = m_{j_t} \cdot (\eta^{t-1}_{j_t} + \beta^t_{j_t})\left(\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) - 1\right),
    \end{align*}
    and similarly,
    \begin{align*}
        \Delta P_t & = (v_t + \beta^t_{j_t}) x^{j_t}_t                                                                 \\
                   & = (\eta^{t-1}_{j_t} + \beta^t_{j_t})\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) x^{j_t}_t.
    \end{align*}
    To show $ \Delta P_t \geq \Delta D_t/\alpha $, we need to verify the following inequality:
    \begin{align*}
        (\eta^{t-1}_{j_t} + \beta^t_{j_t})\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) x^{j_t}_t \geq \frac{1}{\alpha} m_{j_t} \cdot (\eta^{t-1}_{j_t} + \beta^t_{j_t})\left(\exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) - 1\right).
    \end{align*}
    Given that $ \eta^{t-1}_{j_t} + \beta^t_{j_t} > 0 $, we can divide both sides by this positive factor. Furthermore, let $ y := \frac{\alpha}{m_{j_t}} x^{j_t}_t \geq 0 $, then $ x^{j_t}_t = (m_{j_t}/\alpha) y $, and the inequality to prove becomes
    \begin{align*}
        e^{y} \cdot \frac{m_{j_t}}{\alpha} y & \geq \frac{m_{j_t}}{\alpha} \bigl(e^{y} - 1\bigr) \\
        e^{y} y                              & \geq e^{y} - 1
    \end{align*}
    Define $ g(y) := e^{y} y - e^{y} + 1 = e^{y}(y-1) + 1 $. Given that $ g(0) = 0, g'(y) = e^{y}(y-1) + e^{y} = e^{y} y \geq 0 $ for all $ y \geq 0 $, $ g $ is non-decreasing on $ [0, \infty) $ and $ g(y) \geq g(0) = 0 $ for all $ y \geq 0 $, which is equivalent to $ e^{y} y \geq e^{y} - 1 $ for all $ y \geq 0 $. Therefore, we obtain $ \Delta P_t \geq \Delta D_t/\alpha $.

    \paragraph{Case 3.} If $ v_t \geq \eta^{t-1}_{j_t} $ and $ v_t \geq \lambda^{t-1} $, but $ v_t - \beta_i \leq \eta^{t-1}_i $ for all $ i \ne j_t $, the only sources of allocation are the global allocation $ x^G_t $ and the class-specific allocation $ x^{j_t}_t $. The dual change can be expressed as:
    \begin{align*}
        \Delta D_t = (B-M) \cdot (\lambda^t - \lambda^{t-1}) + m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t})
    \end{align*}
    By the design of $ x^G_t $ and $ x^{j_t}_t $ in Algorithm \ref{alg_LinearToZero} being the maximizers of the stated expressions, we have the following from the first-order conditions:
    \begin{align*}
        \exp\left(\tfrac{\alpha}{B-M} x^G_t\right)         & = \frac{v_t + \beta^t_{j_t}}{\lambda^{t-1} + \beta^t_{j_t}},    \\
        \exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) & = \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_{j_t} + \beta^t_{j_t}}.
    \end{align*}
    Applying the same change-of-variable and rearrangement arguments as in Case 2 to both $ x^G_t $ and $ x^{j_t}_t $, we can show that
    \begin{align*}
        \Delta P_t^G     & \geq \frac{1}{\alpha} (B-M) \cdot (\lambda^t - \lambda^{t-1}),         \\
        \Delta P_t^{j_t} & \geq \frac{1}{\alpha} m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t}),
    \end{align*}
    where $ \Delta P_t^G = (v_t + \beta^t_{j_t}) x^G_t $ and $ \Delta P_t^{j_t} = (v_t + \beta^t_{j_t}) x^{j_t}_t $. Summing these two inequalities gives
    \begin{align*}
        \Delta P_t = \Delta P_t^G + \Delta P_t^{j_t} \geq \frac{1}{\alpha} \left((B-M) \cdot (\lambda^t - \lambda^{t-1}) + m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t})\right) = \frac{1}{\alpha} \Delta D_t.
    \end{align*}

    \paragraph{Case 4.} If $ v_t \geq \eta^{t-1}_{j_t} $ and $ v_t \geq \lambda^{t-1} $, and there exists some $ i \ne j_t $ with $ v_t - \beta_i > \eta^{t-1}_i $, the sources of allocation include the global allocation $ x^G_t $, the class-specific allocation $ x^{j_t}_t $, and the cross-class allocations $ x^i_t $ for all such $ i $. Let $ I_t \subseteq [K] \setminus \{j_t\} $ denote the set of classes whose dual variables increase at time $ t $. The dual change can be expressed as:
    \begin{align*}
        \Delta D_t = (B-M) \cdot (\lambda^t - \lambda^{t-1}) + m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t}) + \sum_{i \in I_t} m_i \cdot (\eta^t_i - \eta^{t-1}_i)
    \end{align*}
    By the design of $ x^G_t $, $ x^{j_t}_t $, and $ x^i_t $ in Algorithm \ref{alg_LinearToZero} being the maximizers of the stated expressions, we have the following from the first-order conditions:
    \begin{align*}
        \exp\left(\tfrac{\alpha}{B-M} x^G_t\right)         & = \frac{v_t + \beta^t_{j_t}}{\lambda^{t-1} + \beta^t_{j_t}},                                   \\
        \exp\left(\tfrac{\alpha}{m_{j_t}} x^{j_t}_t\right) & = \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_{j_t} + \beta^t_{j_t}},                                \\
        \exp\left(\tfrac{\alpha}{m_i} x^i_t\right)         & = \frac{v_t + \beta^t_{j_t}}{\eta^{t-1}_i + \beta^t_{j_t} + \beta_i}, \quad \forall i \in I_t.
    \end{align*}
    similarly, we can show that
    \begin{align*}
        \Delta P_t^G     & \geq \frac{1}{\alpha} (B-M) \cdot (\lambda^t - \lambda^{t-1}),                      \\
        \Delta P_t^{j_t} & \geq \frac{1}{\alpha} m_{j_t} \cdot (\eta^t_{j_t} - \eta^{t-1}_{j_t}),              \\
        \Delta P_t^{i}   & \geq \frac{1}{\alpha} m_i \cdot (\eta^t_i - \eta^{t-1}_i), \quad \forall i \in I_t,
    \end{align*}
    where $ \Delta P_t^G = (v_t + \beta^t_{j_t}) x^G_t $, $ \Delta P_t^{j_t} = (v_t + \beta^t_{j_t}) x^{j_t}_t $, and $ \Delta P_t^{i} = (v_t + \beta^t_{j_t}) x^{i}_t $ for all $ i \in I_t $. Summing over all active components gives
    \begin{align*}
        \Delta P_t & = \Delta P_t^G + \Delta P_t^{j_t} + \sum_{i\in I_t}\Delta P_t^{i} \\
                   & \ge \frac{1}{\alpha}\Big((B-M)(\lambda^t-\lambda^{t-1})
        + m_{j_t}(\eta^t_{j_t}-\eta^{t-1}_{j_t})
        + \sum_{i\in I_t} m_i(\eta^t_i-\eta^{t-1}_i)\Big)                              \\
                   & = \frac{1}{\alpha}\Delta D_t.
    \end{align*}

    \paragraph{Primal Feasibility.}
    We prove that the primal solution production by Algorithm \ref{alg_LinearToZero} is feasible, i.e., $ \sum_{t=1}^T x_t \le B $, for any arrival sequence. We use induction on time $ t $ and the idea of \emph{future feasibility}. An allocation prefix is future-feasible if, even under the worst-case future sequence of arrivals, the total allocation remain within the budget $ B $.

    \textbf{Intuition.} To formalize this, we define a max-potential allocation function $ \Gamma_i^t $ that represents the total amount of resources allocated up to time $ t $, plus the maximum potential future allocation allowed by the current state of the dual variable. If we can show that $ \Gamma_i^t \leq B $ for all $ t $ and all $ i \in [K] $, then it implies that the total allocation $ Z^t = \sum_{s=1}^t x_s \leq B $ for all $ t $, establishing primal feasibility.

    \textbf{Max-potential Allocation Function.} We define the max-potential allocation function for each class $ i \in [K] $ at time $ t $ as follows:
    % \[
    %     \Gamma_i^t = Z^t + \sum_{j\in[K]} \frac{m_j}{\alpha}\ln\Biggl(\frac{\theta+\beta_{j}\cdot \mathbf{1}\{z_j^{t}<m_j\}}{\eta^{t}_j+\beta_{j}\cdot \mathbf{1}\{z_j^{t}<m_j\}}\Biggr) + \frac{B-M}{\alpha}\ln\Biggl(\frac{\theta+\beta_{i}\cdot \mathbf{1}\{z_i^{t}<m_i\}}{\lambda^{t}+\beta_{i}\cdot \mathbf{1}\{z_i^{t}<m_i\}}\Biggr).
    % \]

    \[
        \Gamma_i^t(\eta_j^t, \lambda^t) = Z_t + \sum_{j\in[K]} \Delta(\eta_j^t) + \bar{\Delta}(\lambda^t),
    \]
    where $ \Delta(\eta_j^t) $ and $ \bar{\Delta}(\lambda^t) $ are defined as follows:
    \[
        \Delta(\eta_j^t) = \begin{cases}
            \frac{m_j}{\alpha} \ln\Bigl(\frac{v^*_{j,i} + \beta_j}{\eta_j^t + \beta_j}\Bigr) + \frac{m_j}{\alpha} \ln\Bigl(\frac{\theta }{v^*_{j,i} }\Bigr) & \text{if } \eta_j^t < v^*_{j,i},    \\
            \frac{m_j}{\alpha} \ln\Bigl(\frac{\theta }{\eta_j^t}\Bigr),                                                                                     & \text{if } \eta_j^t \geq v^*_{j,i},
        \end{cases}
    \]
    \[
        \bar{\Delta}(\lambda^t) = \begin{cases}
            \frac{B-M}{\alpha} \ln\Bigl(\frac{v^*_{i,i} + \beta_i}{\lambda^t + \beta_i}\Bigr) + \frac{B-M}{\alpha} \ln\Bigl(\frac{\theta }{v^*_{i,i} }\Bigr) & \text{if } \lambda^t < v^*_{i,i},    \\
            \frac{B-M}{\alpha} \ln\Bigl(\frac{\theta }{\lambda^t}\Bigr),                                                                                     & \text{if } \lambda^t \geq v^*_{i,i}.
        \end{cases}
    \]
    Intuitively, $ \Delta(\eta_j^t) $ captures the maximum potential future allocation for each class $ j $ based on its corresponding dual variable $ \eta_j^t $, while $ \bar{\Delta}(\lambda^t) $ captures the maximum potential future allocation to class $ i $ based on the global dual variable $ \lambda^t $. Altogether, $ \Gamma_i^t(\eta_j^t, \lambda^t) $ represents the maximum potential allocation, considering when the global threshold is only helping class $ i $. For notational simplicity, we will denote $ \Gamma_i^t = \Gamma_i^t(\eta_j^t, \lambda^t) $ in the following proof.

    We will show by induction that $ \Gamma_i^t \leq B $ for all $ t $ and all $ i \in [K] $.

    \textbf{Base Case $ (t = 0) $.} At $ t = 0 $, we have $ Z^0 = Z_j^0 = 0 $ for all $ j\in [K]$, and the algorithm initializes $ \eta_j^0 = 1 $ for all $ j $ and $ \lambda^0 = 1 $. Thus, for any fixed $ i \in [K] $,
    \[
        \Gamma_i^0 = \sum_{j\in[K]} \frac{m_j}{\alpha}\ln\Bigl(\tfrac{\theta+\beta_j}{1+\beta_j}\Bigr)
        + \frac{B-M}{\alpha}\ln\Bigl(\tfrac{\theta+\beta_i}{1+\beta_i}\Bigr).
    \]
    From the design of Algorithm \ref{alg_LinearToZero}, there is a reservation $ A_j $ for each class $ j\in [K] $, therefore, to show that the prefix at time $ t = 0 $ is future-feasible, we show that $ \Gamma_i^0 + \sum_{j\in[K]} A_j \leq B $ for all $ i \in [K] $.

    By the constraints in Theorem \ref{theorem-linear2zero-soft-gfq-upper-bound}, for each "reference" class $ \ell \in [K] $ there exist thresholds $ \{v^*_{k,\ell}\}_{k\in[K]} $ such that
    \begin{align*}
        A_{\ell} + \frac{B-M+m_{\ell}}{\alpha}\ln\Bigl(\tfrac{v^*_{\ell,\ell}+\beta_{\ell}}{1+\beta_{\ell}}\Bigr)                                                                   & = m_{\ell},                                      \\
        A_{k} + \frac{m_{k}}{\alpha}\ln\Bigl(\tfrac{v^*_{k,\ell}+\beta_{k}}{1+\beta_{k}}\Bigr)                                                                                      & = m_{k}, \quad \forall k\in[K]\setminus\{\ell\}, \\
        M+\frac{B-M+m_{\ell}}{\alpha}\ln\Bigl(\tfrac{\theta}{v^*_{\ell,\ell}}\Bigr)+\sum_{k \in[K]\setminus\{\ell\}}\frac{m_{k}}{\alpha}\ln\Bigl(\tfrac{\theta}{v^*_{k,\ell}}\Bigr) & \leq B.
    \end{align*}

    Fixing any $ i \in [K] $ and taking $ \ell = i $, we rearrange the first two constraints and sum over $ k $ to obtain
    \[
        M - \sum_{j\in[K]} A_j = \frac{B-M+m_i}{\alpha}\ln\Bigl(\tfrac{v^*_{i,i}+\beta_i}{1+\beta_i}\Bigr) + \sum_{j\ne i} \frac{m_j}{\alpha}\ln\Bigl(\tfrac{v^*_{j,i}+\beta_j}{1+\beta_j}\Bigr).
    \]
    Decomposing the log terms as
    \[
        \ln\Bigl(\tfrac{\theta+\beta_j}{1+\beta_j}\Bigr) = \ln\Bigl(\tfrac{\theta}{v^*_{j,i}}\Bigr) + \ln\Bigl(\tfrac{v^*_{j,i}+\beta_j}{1+\beta_j}\Bigr) + \ln\Bigl(\tfrac{\theta+\beta_j}{\theta}\cdot\tfrac{v^*_{j,i}}{v^*_{j,i}+\beta_j}\Bigr),
    \] we note that $ \frac{\theta+\beta_j}{\theta}\cdot\frac{v^*_{j,i}}{v^*_{j,i}+\beta_j} \leq 1 $, so each such logarithm is nonpositive. Substituting these decompositions into $ \Gamma_i^0 $ and combining with the last feasibility inequality in Theorem \ref{theorem-linear2zero-soft-gfq-upper-bound}, we obtain
    \[
        \Gamma_i^0 + \sum_{j\in[K]} A_j \le B.
    \].

    \textbf{Induction Step.} Assume $ \Gamma_i^{t-1} \leq B $ for all $ i \in [K] $ at time $ t-1 $. We show that $ \Gamma_i^{t} \leq B $ for all $ i $ at time $ t $, regardless of the valuation $ v_t $ and class $ j_t $ of the arrival at time $ t $. For clarity and ease of exposition, we present the induction step for $ K = 2 $ classes first, and then argue how to extend it to general $ K \geq 2 $ classes. Without loss of generality, let $ j_t = 1 $. We then consider the following cases.

    \emph{Case 1: Quotas met $ (z_1^{t-1} \geq m_1, z_2^{t-1}\geq m_2).$} Firstly, if $ v_t \leq \lambda^{t-1} $, then $ x_t = \frac{m_1}{\alpha}\ln\Bigl(\tfrac{v_t}{\eta_1^{t-1}}\Bigr) $ and $ \eta_1^t = v_t $, therefore we have
    \begin{align*}
        \Gamma_i^t & = Z^{t-1} + x_t  + \frac{m_1}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_1^t}\Bigr) + \frac{m_2}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_2^t}\Bigr) + \frac{B-M}{\alpha} \ln \Bigl(\tfrac{\theta}{\lambda^t}\Bigr)      \\
                   & = Z^{t-1} + \frac{m_1}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_1^{t-1}}\Bigr) + \frac{m_2}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_2^{t-1}}\Bigr) + \frac{B-M}{\alpha} \ln \Bigl(\tfrac{\theta}{\lambda^{t-1}}\Bigr) \\
                   & \leq B, \forall i \in [2],
    \end{align*}
    where the inequality follows from the induction hypothesis. If instead $ v_t > \lambda^{t-1} $, the allocation $ x_t $ includes additionally a global component, meaning $ x_t = \frac{m_1}{\alpha}\ln\Bigl(\tfrac{v_t}{\eta_1^{t-1}}\Bigr) + \frac{B-M}{\alpha}\ln\Bigl(\tfrac{v_t}{\lambda^{t-1}}\Bigr) $, with $ \eta_1^t = v_t $ and $ \lambda^t = v_t $. In that case,
    \begin{align*}
        \Gamma_i^t & = Z^{t-1} + x_t  + \frac{m_1}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_1^t}\Bigr) + \frac{m_2}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_2^t}\Bigr) + \frac{B-M}{\alpha} \ln \Bigl(\tfrac{\theta}{\lambda^t}\Bigr)      \\
                   & = Z^{t-1} + \frac{m_1}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_1^{t-1}}\Bigr) + \frac{m_2}{\alpha} \ln \Bigl(\tfrac{\theta}{\eta_2^{t-1}}\Bigr) + \frac{B-M}{\alpha} \ln \Bigl(\tfrac{\theta}{\lambda^{t-1}}\Bigr) \\
                   & \leq B, \forall i \in [2],
    \end{align*}
    where the inequality again follows from the induction hypothesis.

    \emph{Case 2: Quotas not met $ (z_1^{t-1} < m_1, z_2^{t-1}< m_2).$} Assuming first that $ \lambda^{t-1} \geq v_t $, we have $ x_t = \frac{m_1}{\alpha} \ln\Bigl(\tfrac{v_t+\beta_1}{\eta_1^{t-1}+\beta_1}\Bigr) $ and $ \eta_1^t = v_t, \lambda^t = \lambda^{t-1}, \eta_2^t = \eta_2^{t-1} $.

    First, we consider the extreme case where the global allocation only affects class $ 1 $ in the future. Let $ p_1,p_2 $ be such that under some future sequence,
    \begin{align*}
        z_1^{t-1} + x_t + \frac{m_1}{\alpha}\ln\Bigl(\tfrac{p_1+\beta_1}{v_t+\beta_1}\Bigr)
        + \frac{B-M}{\alpha}\ln\Bigl(\tfrac{p_1+\beta_1}{\lambda^{t-1}+\beta_1}\Bigr)              & = m_1, \\
        z_2^{t-1} + \frac{m_2}{\alpha}\ln\Bigl(\tfrac{p_2+\beta_2}{\eta_    2^{t-1}+\beta_2}\Bigr) & = m_2.
    \end{align*}

    We then have,
    \begin{align*}
        \Gamma_1^{t} = Z^{t-1}+ x_t & + \frac{m_1}{\alpha}\ln\left(\frac{p_1 + \beta}{v_t+\beta}\right) + \frac{m_1}{\alpha}\ln\left(\frac{\theta}{p_1}\right) + \frac{m_2}{\alpha}\ln\left(\frac{p_2 + \beta}{\eta^{t-1}_2+\beta}\right) + \frac{m_2}{\alpha}\ln\left(\frac{\theta}{p_2}\right) \\&+  \frac{B-M}{\alpha}\ln\left(\frac{p_1 + \beta}{\lambda^{t-1}+\beta}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta}{p_1}\right) \leq B,
    \end{align*}
    which follows from the induction hypothesis.



    Next, we need to show that $ \Gamma_2^{t} \leq B $ by showing that we have either $ \Gamma_2^{t} \leq \Gamma_1^{t-1} \leq B $ or $ \Gamma_2^{t} \leq \Gamma_2^{t-1} \leq B $.


    we consider the case where the global allocation only affects class $ 2 $ in the future. Let $ q_1,q_2 $ be such that under some future sequence,
    \begin{align*}
        z_1^{t-1} + x_t + \frac{m_1}{\alpha}\ln\Bigl(\tfrac{q_1+\beta_1}{v_t+\beta_1}\Bigr) & = m_1, \\
        z_2^{t-1} + \frac{m_2}{\alpha}\ln\Bigl(\tfrac{q_2+\beta_2}{\eta_2^{t-1}+\beta_2}\Bigr)
        + \frac{B-M}{\alpha}\ln\Bigl(\tfrac{q_2+\beta_2}{\lambda^{t-1}+\beta_2}\Bigr)       & = m_2.
    \end{align*}
    Recall that $ x_t = \frac{m_1}{\alpha} \ln\Bigl(\tfrac{v_t+\beta_1}{\eta_1^{t-1}+\beta_1}\Bigr) $, and summing the two equations above gives
    \begin{align}
        Z^{t-1} + \frac{m_1}{\alpha}\ln\left(\frac{q_1+\beta}{\eta^{t-1}_1+\beta}\right)+\frac{m_2}{\alpha}\ln\left(\frac{q_2+\beta}{\eta^{t-1}_2+\beta}\right) + \frac{B-M}{\alpha}\ln\left(\frac{q_2+\beta}{\lambda^{t-1}+\beta}\right) = M.\label{equation-l2z-sub1}
    \end{align}

    Our goal is to show the following holds:
    \begin{align}
        \Gamma_2^{t} & = Z^{t-1}+ x_t + \frac{m_1}{\alpha}\ln\left(\frac{q_1 + \beta}{v_t+\beta}\right) + \frac{m_1}{\alpha}\ln\left(\frac{\theta}{q_1}\right) + \frac{m_2}{\alpha}\ln\left(\frac{q_2 + \beta}{\eta^{t-1}_2+\beta}\right) + \frac{m_2}{\alpha}\ln\left(\frac{\theta}{q_2}\right) \\&+  \frac{B-M}{\alpha}\ln\left(\frac{q_2 + \beta}{\lambda^{t-1}+\beta}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta}{q_2}\right) \\
                     & = M + \frac{m_1}{\alpha}\ln\left(\frac{\theta}{q_1}\right)+\frac{m_2}{\alpha}\ln\left(\frac{\theta}{q_2}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta}{q_2}\right) \leq B, \label{equation-l2z-target}
    \end{align}

    We first try to show that $ \Gamma_2^{t} \leq \Gamma_1^{t-1} \leq B $. In this case, we have the following from the induction hypothesis of $ \Gamma_1^{t-1} $:
    \begin{align}
         & Z^{t-1} + \frac{m_1}{\alpha}\ln\left(\frac{v_1+\beta}{\eta^{t-1}_1+\beta}\right)+\frac{m_2}{\alpha}\ln\left(\frac{v_2+\beta}{\eta^{t-1}_2+\beta}\right) + \frac{B-M}{\alpha}\ln\left(\frac{v_1+\beta}{\lambda^{t-1}+\beta}\right) = M \label{equation-l2z-sub2} \\
         & M + \frac{m_1}{\alpha}\ln\left(\frac{\theta}{v_1}\right)+\frac{m_2}{\alpha}\ln\left(\frac{\theta}{v_2}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta}{v_1}\right) \leq B \label{equation-l2z-target-2}
    \end{align}

    Subtracting \eqref{equation-l2z-sub2} from \eqref{equation-l2z-sub1}, we have
    \begin{align*}
        m_1\ln\Bigl(\tfrac{v_1+\beta}{q_1+\beta}\Bigr)
        + m_2\ln\Bigl(\tfrac{v_2+\beta}{q_2+\beta}\Bigr)
        + (B-M)\ln\Bigl(\tfrac{v_1+\beta}{q_2+\beta}\Bigr) & = 0
    \end{align*}

    Given that $ v_1 $ corresponds to the case where the global allocation does help class $ 1 $ to satisfy its quota, and $ q_1 $ corresponds to the case where it does not, we know that $ v_1 \leq q_1 $.

    Comparing \eqref{equation-l2z-target} and \eqref{equation-l2z-target-2}, we see that to show $ \Gamma_2^{t} \leq \Gamma_1^{t-1} $, it suffices to show that
    \begin{align*}
        m_1\ln\Bigl(\tfrac{v_1}{q_1}\Bigr)
        + m_2\ln\Bigl(\tfrac{v_2}{q_2}\Bigr)
        + (B-M)\ln\Bigl(\tfrac{v_1}{q_2}\Bigr) & \le 0,
    \end{align*}


    Let $F(x) = m_1\ln\left(\frac{v_1+x}{q_1+x}\right)+m_2\ln\left(\frac{v_2+x}{q_2+x}\right) + (B-M)\ln\left(\frac{v_1+x}{q_2+x}\right)$. If we can show that $F'(x) \geq 0$ for all $x\in[0,\beta]$, we can conclude that $F(0)\leq F(\beta)=0$. Therefore,
    \begin{align*}
        F'(x)= m_1\frac{q_1-v_1}{(q_1+x)(v_1+x)}+m_2\frac{q_2-v_2}{(q_2+x)(v_2+x)} + (B-M)\frac{q_2-v_1}{(q_2+x)(v_1+x)}.
    \end{align*}
    Since $F''(x)\leq 0$, it is only need to show that $F'(\beta)\geq 0$. Based on the well-known inequality of $1+x \geq \ln(x)$, we know that:
    \begin{align*}
        m_1\frac{q_1-v_1}{v_1+x}+m_2\frac{q_2-v_2}{v_2+x} + (B-M)\frac{q_2-v_1}{v_1+x}\geq 0.
    \end{align*}
    Since, $v_1 \leq q_1$, if $q_1 \leq q_2$, we can conclude that $F'(\beta) \geq 0$.


    However, if $q_1 \geq q_2$, we shall show $ \Gamma_2^{t} \leq \Gamma_2^{t-1} \leq B $. From the induction hypothesis of $ \Gamma_2^{t-1} $, we have \eqref{equation-l2z-sub1} and
    \begin{align}
         & Z^{t-1} + \frac{m_1}{\alpha}\ln\left(\frac{r_1+\beta}{\eta^{t-1}_1+\beta}\right)+\frac{m_2}{\alpha}\ln\left(\frac{r_2+\beta}{\eta^{t-1}_2+\beta}\right) + \frac{B-M}{\alpha}\ln\left(\frac{r_2+\beta}{\lambda^{t-1}+\beta}\right) = M\label{euqation-l2z-sub3} \\
         & M + \frac{m_1}{\alpha}\ln\left(\frac{\theta}{r_1}\right)+\frac{m_2}{\alpha}\ln\left(\frac{\theta}{r_2}\right) + \frac{B-M}{\alpha}\ln\left(\frac{\theta}{r_2}\right) \leq B \label{equation-l2z-target-4}
    \end{align}

    Subtracting \eqref{euqation-l2z-sub3} from \eqref{equation-l2z-sub1}, we have
    \begin{align*}
        m_1\ln\Bigl(\tfrac{r_1+\beta}{q_1+\beta}\Bigr)
        + m_2\ln\Bigl(\tfrac{r_2+\beta}{q_2+\beta}\Bigr)
        + (B-M)\ln\Bigl(\tfrac{r_2+\beta}{q_2+\beta}\Bigr) & = 0
    \end{align*}
    Similarly, we compare \eqref{equation-l2z-target-4} and \eqref{equation-l2z-target} to see that to show $ \Gamma_2^{t} \leq \Gamma_2^{t-1} $, it suffices to show that
    \begin{align*}
        m_1\ln\Bigl(\tfrac{r_1}{q_1}\Bigr)
        + m_2\ln\Bigl(\tfrac{r_2}{q_2}\Bigr)
        + (B-M)\ln\Bigl(\tfrac{r_2}{q_2}\Bigr) & \le 0,
    \end{align*},
    by defining $G(x) = m_1\ln\left(\frac{r_1+x}{q_1+x}\right)+m_2\ln\left(\frac{r_2+x}{q_2+x}\right) + (B-M)\ln\left(\frac{r_2+x}{q_2+x}\right)$, and following the same argument as before, we have $G'(\beta) \geq 0$.

    As a result, at least one of $F(0)$ or $G(0)$ is less than 0 which finishes the proof.

    \textbf{Generalizing to $ K \geq 2 $ classes.} Given $ \Gamma_i^{t-1} \leq B $ for all $ i \in [K] $ at time $ t-1 $, we need to show that $ \Gamma_i^{t} \leq B $ for all $ i $ at time $ t $. Given $ j_t = j $, it is sufficient to show that we have $ \Gamma_i^{t} \leq \Gamma_i^{t-1} $ or $ \Gamma_i^{t} \leq \Gamma_j^{t-1} $ for each $ i \in [K] $. In this case, for any fixed $ i \in [K] $, the transition values from classes $ k \in [K]\setminus\{i, j\} $ are fixed, the proof then follows similarly as the $ K = 2 $ case by considering only classes $ i $ and $ j $.

    Given that $ v_1 $ corresponds to the case where the global allocation does help class $ 1 $ to satisfy its quota, and $ q_1 $ corresponds to the case where it does not, we know that $ v_1 \leq q_1 $.


\end{proof}

\subsection{Proof Outline for Necessary Conditions}

The proof of necessary conditions in section \ref{section-main-results-soft-quota} follows directly by analyzing the performance of any online algorithm's utilization against the family of hard instances $ \{I^{\SOFTQUOTA}\}_{\forall j \in [K]} $ as defined in Definition \ref{definition-hard-instance-soft-quota}. The key is that these hard instances are indistinguishable to the online algorithm in the first stage of arrivals, the algorithm needs to provide $ \alpha $-competitiveness guarantee against all possible $ j \in [K] $ in the second stage of arrivals.



As an example, we provide the full proof for Theorem \ref{theorem-linear2zero-soft-quota-necessary} below.
\begin{proof}
    The proof follows by considering the performance of any $\alpha$-competitive algorithm against the family of hard instances.

    \paragraph{Initial Condition \eqref{equation-linear2zero-necessary-initial-condition}} After the first stage of arrivals, the online algorithm's performance can be captured by the left-hand side of \eqref{equation-linear2zero-necessary-initial-condition}. With the second batch of arrivals for group $ j $, there must exist a critical value $v_j \in (1, \theta]$ such that:
    \begin{itemize}
        \item Up until $ v_j $, i.e., $ \forall v < v_j $, the left-hand side of \eqref{equation-linear2zero-necessary-initial-condition} strictly exceeds the right-hand side, indicating that the current allocation is sufficient to guarantee $ \alpha $ competitiveness.
        \item At $v = v_j$, equality holds in \eqref{equation-linear2zero-necessary-initial-condition}, marking the point where the algorithm must begin allocating to maintain competitiveness.
        \item After $ v_j $, the algorithm must allocate more resources to prevent the right-hand side from exceeding the left-hand side, which would then violate the $\alpha$-competitiveness requirement.
    \end{itemize}

    Meanwhile, with the initial stage and the second stage for group $ j $ with values up to $ v_j $, the offline optimal is to allocate $ m_{j} $ amount of resources to each group $ j \in [K] $, and allocate the remaining portion $ 1 - M $ to class $ j $ with value $ v_{j} $, leading to the optimal value as described on the right-hand side of \eqref{equation-linear2zero-necessary-initial-condition}.

    \paragraph{Incremental Condition (I) \eqref{equation-linear2zero-necessary-first-group-condition}} In the second batch of arrival for group $ j $, as the value of arrivals increase beyond  $ v_j $, the online algorithm must maintain $ \alpha $-competitiveness against the offline optimal, which can be expressed as followed for any $ v \in [v_j, \theta] $:
    \begin{align*}
        \OPT(I^{\SOFTQUOTA}_j) \leq (B-M+m_{j})v_{j} + \sum_{i\in [K]\setminus\{j\}} m_{i}.
    \end{align*}
    On the other hand, the performance of the online algorithm is
    \begin{align*}
        \ALG(I^{\SOFTQUOTA}_j) \geq \Psi_j(v_{j})+\sum_{i\in [K]\setminus\{j\}} \Psi_i(1),
    \end{align*}
    which implies second necessary condition.

    \paragraph{General Incremental Condition \eqref{equation-linear2zero-necessary-general-condition}}
    We now analyze the general case where batches for groups $j, 1, \ldots, i-1$ have already arrived, we are currently processing arrivals from group $i$ with maximum observed value $ v \in [1, \theta]$, and arrivals for groups $i+1, \ldots, K$ are yet to arrive.

    The offline optimal strategy in this scenario is to construct an allocation that maximizes total welfare by:
    \begin{itemize}
        \item Allocating $m_j$ units to each completed group $j$ for $j \in \{1, \ldots, i-1\}$, selecting arrivals with the maximum value $\theta$ from their respective batches.
        \item Allocating $m_i$ units to the current group $i$, selecting the arrival with value $v_i$.
        \item Allocating $m_j$ units for each group $j$ for $j \in \{i+1, \ldots, K\}$, using the arrivals with value 1 from the initial batch.
        \item Allocating the remaining capacity $(B-M+\sum_{l=1}^{i-1}m_j)$ to any arrival with value $ \theta $ in previous batches, regardless of their group.
    \end{itemize}
    This allocation strategy leads to the following upper bound on the offline optimal performance for any $ v_i \in [1, \theta] $:
    \begin{align*}
        \OPT(I^{\SOFTQUOTA}_\pi) \leq (B-M+\sum_{l=1}^{i-1}m_j)\theta + m_i v+\sum_{j=i+1}^K m_j.
    \end{align*}
    Meanwhile, the performance of the online algorithm is
    \begin{align*}
        \ALG(I^{\SOFTQUOTA}_\pi) \geq \Psi_j (\theta) + \sum_{l=1}^{i-1} \Psi_{l}(\theta) + \Psi_{i}(v) +\sum_{l=i+1}^K \Psi_{l}(1),
    \end{align*}
    which implies the third necessary condition.

    \paragraph{Boundary condition \eqref{equation-linear2zero-necessary-boundary-condition}}
    The final condition ensures that the total allocation across all groups does not exceed the available capacity. Since $\psi_i(\theta)$ represents the maximum allocation that can be made to group $i$ when the highest-value arrivals are observed, the constraint $\sum_{i \in [K]}\psi_i (\theta) \leq B$ directly enforces the overall capacity constraint.
\end{proof}